Processing  7 subjects



Now processing model 1 of 224 using max all lm 1 
Linear Regression 

1430 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1287, 1288, 1286, 1287, 1288, 1286, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.52505  0.7436521  9.530745

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 2 of 224 using same all lm 1 
Linear Regression 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 183, 183, 184, 183, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  13.26924  0.7127775  9.718124

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 3 of 224 using max du lm 1 
Linear Regression 

1430 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1287, 1287, 1288, 1287, 1287, 1287, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  12.9843  0.7251575  10.11482

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 4 of 224 using same du lm 1 
Linear Regression 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 184, 185, 183, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  13.65546  0.7010174  10.51302

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 5 of 224 using max rms lm 1 
Linear Regression 

1430 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1287, 1288, 1288, 1286, 1287, 1287, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  15.22247  0.6266454  11.38751

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 6 of 224 using same rms lm 1 
Linear Regression 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 185, 184, 183, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  14.07772  0.6792267  10.51648

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 7 of 224 using max hudgins lm 1 
Linear Regression 

1430 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1286, 1287, 1288, 1287, 1286, 1287, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  14.54817  0.6632089  10.89017

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 8 of 224 using same hudgins lm 1 
Linear Regression 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 184, 184, 182, 182, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  13.96981  0.6854256  10.77885

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 9 of 224 using max all knn 1 
k-Nearest Neighbors 

1430 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1287, 1287, 1286, 1287, 1287, 1288, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE    
  5  10.73939  0.8108140  5.24753
  7  10.86297  0.8065058  5.55989
  9  10.87847  0.8058261  5.75726

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 10 of 224 using same all knn 1 
k-Nearest Neighbors 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 183, 183, 183, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  13.87542  0.6784112  8.531971
  7  14.06356  0.6732870  9.143440
  9  14.31756  0.6627574  9.638431

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 11 of 224 using max du knn 1 
k-Nearest Neighbors 

1430 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1288, 1286, 1287, 1288, 1288, 1287, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.67910  0.8122833  5.236484
  7  10.85479  0.8060764  5.569847
  9  10.85011  0.8063832  5.765011

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 12 of 224 using same du knn 1 
k-Nearest Neighbors 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 185, 183, 183, 183, 185, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  14.13413  0.6660356  8.709362
  7  14.25748  0.6634071  9.248903
  9  14.43450  0.6584332  9.701138

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 13 of 224 using max rms knn 1 
k-Nearest Neighbors 

1430 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1285, 1287, 1288, 1287, 1287, 1288, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.717387  0.8433285  4.423637
  7  9.793313  0.8409057  4.649697
  9  9.864720  0.8388845  4.798804

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 14 of 224 using same rms knn 1 
k-Nearest Neighbors 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 183, 183, 185, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  11.75274  0.7696698  6.646272
  7  11.63972  0.7744002  6.891537
  9  11.75983  0.7706667  7.352287

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 15 of 224 using max hudgins knn 1 
k-Nearest Neighbors 

1430 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1287, 1288, 1288, 1286, 1288, 1286, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  16.37268  0.5660730  10.84203
  7  16.30234  0.5673171  11.06598
  9  16.25942  0.5686801  11.16861

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 16 of 224 using same hudgins knn 1 
k-Nearest Neighbors 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 183, 184, 184, 185, 185, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.42970  0.5152721  12.28207
  7  17.27632  0.5235796  12.39536
  9  17.06934  0.5335424  12.53336

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 17 of 224 using max all svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1430 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1288, 1287, 1288, 1287, 1285, 1287, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  17.73475  0.5190389  13.897166
  1       0.001  0.50  16.72662  0.5615018  12.785648
  1       0.001  1.00  16.08556  0.5886683  12.080983
  1       0.010  0.25  15.42419  0.6205638  11.394427
  1       0.010  0.50  14.82587  0.6475388  10.932826
  1       0.010  1.00  14.14930  0.6764402  10.436033
  1       0.100  0.25  13.28635  0.7117129   9.932126
  1       0.100  0.50  12.85849  0.7297027   9.640235
  1       0.100  1.00  12.64080  0.7388794   9.464240
  2       0.001  0.25  16.61190  0.5677734  12.716539
  2       0.001  0.50  15.82271  0.6008063  11.933114
  2       0.001  1.00  15.09261  0.6334520  11.261821
  2       0.010  0.25  13.52155  0.7102302   9.395912
  2       0.010  0.50  14.03261  0.7041640   9.247386
  2       0.010  1.00  14.41664  0.7034521   9.122841
  2       0.100  0.25  17.75781  0.7078515   8.851398
  2       0.100  0.50  20.56075  0.7098090   8.938059
  2       0.100  1.00  21.28139  0.7155734   8.854037
  3       0.001  0.25  15.91862  0.5976500  12.101380
  3       0.001  0.50  15.00796  0.6373847  11.309411
  3       0.001  1.00  13.95283  0.6836843  10.518598
  3       0.010  0.25  14.64195  0.6988774   9.116331
  3       0.010  0.50  14.45814  0.7073627   8.889456
  3       0.010  1.00  14.54256  0.7136172   8.707387
  3       0.100  0.25  26.00771  0.7185750   8.724805
  3       0.100  0.50  32.02272  0.7121548   9.312162
  3       0.100  1.00  40.75690  0.6992852  10.226551

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 1, scale = 0.1 and C = 1.


Now processing model 18 of 224 using same all svmPoly 1 
Support Vector Machines with Polynomial Kernel 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 185, 184, 182, 185, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  24.18456  0.4377589  19.732818
  1       0.001  0.50  20.89596  0.4599167  17.377641
  1       0.001  1.00  18.41248  0.5251179  14.916946
  1       0.010  0.25  16.06791  0.6090899  12.586510
  1       0.010  0.50  15.28957  0.6294414  11.731744
  1       0.010  1.00  14.97471  0.6431566  11.402614
  1       0.100  0.25  14.51779  0.6631737  10.988605
  1       0.100  0.50  14.05171  0.6836735  10.582812
  1       0.100  1.00  13.65391  0.7007178  10.278699
  2       0.001  0.25  20.89149  0.4605141  17.371644
  2       0.001  0.50  18.39596  0.5266064  14.903334
  2       0.001  1.00  16.48451  0.5988426  13.025427
  2       0.010  0.25  14.47585  0.6692943  11.062478
  2       0.010  0.50  13.39303  0.7100812   9.959952
  2       0.010  1.00  12.45108  0.7464128   8.983715
  2       0.100  0.25  12.33783  0.7512313   8.753730
  2       0.100  0.50  12.65210  0.7420432   9.074057
  2       0.100  1.00  13.01648  0.7311646   9.388455
  3       0.001  0.25  19.46097  0.4908008  15.983876
  3       0.001  0.50  17.12729  0.5771434  13.649701
  3       0.001  1.00  15.70853  0.6225788  12.222099
  3       0.010  0.25  12.95537  0.7296860   9.524678
  3       0.010  0.50  12.38359  0.7483200   8.823450
  3       0.010  1.00  12.20286  0.7548349   8.589932
  3       0.100  0.25  15.87842  0.6500136  10.377246
  3       0.100  0.50  17.63980  0.6044735  11.309217
  3       0.100  1.00  19.68801  0.5526833  12.679542

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 19 of 224 using max du svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1430 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1287, 1287, 1287, 1286, 1287, 1288, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  19.12204  0.4405492  15.270783
  1       0.001  0.50  17.99305  0.4968097  13.832367
  1       0.001  1.00  17.25825  0.5305962  12.933819
  1       0.010  0.25  16.66733  0.5635965  12.265836
  1       0.010  0.50  16.14419  0.5904999  11.813868
  1       0.010  1.00  15.57372  0.6172634  11.420747
  1       0.100  0.25  14.67464  0.6559063  10.866144
  1       0.100  0.50  13.93849  0.6850403  10.506698
  1       0.100  1.00  13.31851  0.7106327  10.248703
  2       0.001  0.25  17.93824  0.5001041  13.795670
  2       0.001  0.50  17.12909  0.5371901  12.860653
  2       0.001  1.00  16.49576  0.5686047  12.242847
  2       0.010  0.25  13.65207  0.6986009  10.014856
  2       0.010  0.50  13.92303  0.6963701   9.635779
  2       0.010  1.00  14.61909  0.6910759   9.437353
  2       0.100  0.25  18.51421  0.6880898   9.234228
  2       0.100  0.50  20.14269  0.6908189   9.279122
  2       0.100  1.00  21.95430  0.6931119   9.358336
  3       0.001  0.25  17.34010  0.5285962  13.170388
  3       0.001  0.50  16.53761  0.5656201  12.367529
  3       0.001  1.00  15.67505  0.6068801  11.676297
  3       0.010  0.25  15.16720  0.6783018   9.608971
  3       0.010  0.50  15.37637  0.6842850   9.355921
  3       0.010  1.00  15.40063  0.6915915   9.119767
  3       0.100  0.25  17.99807  0.6960625   8.696988
  3       0.100  0.50  23.68808  0.6853764   9.147112
  3       0.100  1.00  40.05689  0.6769450  10.587287

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 1, scale = 0.1 and C = 1.


Now processing model 20 of 224 using same du svmPoly 1 
Support Vector Machines with Polynomial Kernel 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 183, 184, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  25.48111  0.3950540  20.613237
  1       0.001  0.50  22.95935  0.4157174  18.913495
  1       0.001  1.00  20.17049  0.4531568  16.630717
  1       0.010  0.25  17.59087  0.5461261  13.951437
  1       0.010  0.50  16.54815  0.5807518  12.783804
  1       0.010  1.00  16.02104  0.5995093  12.172863
  1       0.100  0.25  15.64839  0.6177107  11.843403
  1       0.100  0.50  15.23531  0.6366751  11.511734
  1       0.100  1.00  14.95086  0.6504820  11.263354
  2       0.001  0.25  22.95205  0.4157725  18.905563
  2       0.001  0.50  20.16640  0.4536122  16.626533
  2       0.001  1.00  18.01045  0.5287956  14.415578
  2       0.010  0.25  16.11504  0.6041900  12.461554
  2       0.010  0.50  15.09321  0.6427458  11.439764
  2       0.010  1.00  13.88879  0.6900295  10.266419
  2       0.100  0.25  12.59258  0.7399461   8.811807
  2       0.100  0.50  12.76071  0.7348740   8.994913
  2       0.100  1.00  13.21681  0.7206459   9.457833
  3       0.001  0.25  21.09040  0.4326770  17.457676
  3       0.001  0.50  18.80180  0.4951463  15.199613
  3       0.001  1.00  17.14109  0.5630107  13.481294
  3       0.010  0.25  14.63027  0.6653254  11.062317
  3       0.010  0.50  13.36563  0.7116348   9.798336
  3       0.010  1.00  12.76386  0.7318063   9.061705
  3       0.100  0.25  14.86555  0.6721651   9.799507
  3       0.100  0.50  16.90441  0.6272427  10.649289
  3       0.100  1.00  19.22777  0.5773168  11.925255

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 21 of 224 using max rms svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1430 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1286, 1287, 1288, 1286, 1287, 1288, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  23.13298  0.4547832  19.603084
  1       0.001  0.50  19.61403  0.4847062  16.753926
  1       0.001  1.00  17.30279  0.5647447  14.172943
  1       0.010  0.25  15.51952  0.6251759  12.076895
  1       0.010  0.50  15.16221  0.6298609  11.452394
  1       0.010  1.00  15.15117  0.6296327  11.255250
  1       0.100  0.25  15.17924  0.6294145  11.189748
  1       0.100  0.50  15.20537  0.6291329  11.183522
  1       0.100  1.00  15.22264  0.6289641  11.182449
  2       0.001  0.25  19.61286  0.4847186  16.751579
  2       0.001  0.50  17.29964  0.5649840  14.169957
  2       0.001  1.00  15.80243  0.6186929  12.452579
  2       0.010  0.25  14.98643  0.6383736  11.344751
  2       0.010  0.50  14.76819  0.6464563  11.043991
  2       0.010  1.00  14.43333  0.6609285  10.770543
  2       0.100  0.25  14.41495  0.6891708   9.655441
  2       0.100  0.50  14.80453  0.6857830   9.623274
  2       0.100  1.00  14.97743  0.6845980   9.612909
  3       0.001  0.25  18.17541  0.5278626  15.144759
  3       0.001  0.50  16.31836  0.6049054  13.053219
  3       0.001  1.00  15.36678  0.6291848  11.857812
  3       0.010  0.25  14.56608  0.6558322  10.980745
  3       0.010  0.50  14.07308  0.6771913  10.591376
  3       0.010  1.00  13.53600  0.7009734  10.165091
  3       0.100  0.25  17.15309  0.6863804   9.225867
  3       0.100  0.50  17.66407  0.6888900   9.086811
  3       0.100  1.00  18.38424  0.6895259   8.987906

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 22 of 224 using same rms svmPoly 1 
Support Vector Machines with Polynomial Kernel 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 185, 185, 182, 182, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.89225  0.4716305  22.083479
  1       0.001  0.50  26.38522  0.4718584  21.502568
  1       0.001  1.00  25.12118  0.4858868  20.391928
  1       0.010  0.25  21.24763  0.5215281  17.702762
  1       0.010  0.50  18.12598  0.5841364  15.014435
  1       0.010  1.00  15.48229  0.6650963  12.468954
  1       0.100  0.25  14.15085  0.6865781  10.774874
  1       0.100  0.50  13.98745  0.6873922  10.482180
  1       0.100  1.00  13.92018  0.6880368  10.389636
  2       0.001  0.25  26.38476  0.4719270  21.502098
  2       0.001  0.50  25.12147  0.4860481  20.391378
  2       0.001  1.00  22.37511  0.5175462  18.585853
  2       0.010  0.25  18.11841  0.5851365  14.989459
  2       0.010  0.50  15.45866  0.6668916  12.439523
  2       0.010  1.00  14.21166  0.6876894  10.938606
  2       0.100  0.25  13.09733  0.7225096   9.759192
  2       0.100  0.50  12.68596  0.7353530   9.292814
  2       0.100  1.00  12.42770  0.7429675   8.927114
  3       0.001  0.25  25.83668  0.4748357  20.919713
  3       0.001  0.50  23.67157  0.5060335  19.449803
  3       0.001  1.00  20.30439  0.5302210  16.879134
  3       0.010  0.25  16.28690  0.6425120  13.259159
  3       0.010  0.50  14.50901  0.6879761  11.373146
  3       0.010  1.00  13.91966  0.6944908  10.536704
  3       0.100  0.25  12.25717  0.7498872   8.760010
  3       0.100  0.50  12.00769  0.7572712   8.394006
  3       0.100  1.00  11.93857  0.7597881   8.296002

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 23 of 224 using max hudgins svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1430 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1287, 1287, 1286, 1288, 1287, 1286, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  20.26659  0.3916825  16.721652
  1       0.001  0.50  18.72526  0.4582540  14.886684
  1       0.001  1.00  17.67325  0.5082507  13.587942
  1       0.010  0.25  16.60269  0.5585667  12.548542
  1       0.010  0.50  15.91709  0.5945166  11.940624
  1       0.010  1.00  15.27750  0.6258990  11.404804
  1       0.100  0.25  14.70000  0.6548296  10.879525
  1       0.100  0.50  14.51491  0.6643698  10.689753
  1       0.100  1.00  14.44612  0.6675551  10.644756
  2       0.001  0.25  18.71268  0.4592043  14.876037
  2       0.001  0.50  17.63855  0.5104648  13.561144
  2       0.001  1.00  16.76538  0.5509945  12.706887
  2       0.010  0.25  14.48639  0.6628172  10.838785
  2       0.010  0.50  13.50785  0.7037696   9.927109
  2       0.010  1.00  12.99577  0.7241295   9.497363
  2       0.100  0.25  12.98835  0.7300691   8.878641
  2       0.100  0.50  13.09017  0.7292593   8.823274
  2       0.100  1.00  13.04794  0.7315487   8.757117
  3       0.001  0.25  18.06534  0.4924833  14.034360
  3       0.001  0.50  17.04816  0.5378487  12.995869
  3       0.001  1.00  16.19261  0.5804117  12.229319
  3       0.010  0.25  13.34168  0.7124058   9.805509
  3       0.010  0.50  12.82062  0.7319685   9.330537
  3       0.010  1.00  12.59968  0.7401440   9.071463
  3       0.100  0.25  14.91131  0.7025656   8.576955
  3       0.100  0.50  15.90006  0.6993326   8.528570
  3       0.100  1.00  16.56078  0.6990445   8.538603

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 24 of 224 using same hudgins svmPoly 1 
Support Vector Machines with Polynomial Kernel 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 183, 183, 183, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.25362  0.3775290  21.474124
  1       0.001  0.50  25.14791  0.3953332  20.395641
  1       0.001  1.00  22.23614  0.4068412  18.448530
  1       0.010  0.25  18.90177  0.4778294  15.344526
  1       0.010  0.50  17.20363  0.5495912  13.522815
  1       0.010  1.00  16.21727  0.5885693  12.426824
  1       0.100  0.25  15.53996  0.6193566  11.788061
  1       0.100  0.50  15.05037  0.6436677  11.415777
  1       0.100  1.00  14.49559  0.6706634  10.926194
  2       0.001  0.25  25.14661  0.3954948  20.393946
  2       0.001  0.50  22.23332  0.4070540  18.444210
  2       0.001  1.00  19.71811  0.4531741  16.220499
  2       0.010  0.25  17.04579  0.5598966  13.408222
  2       0.010  0.50  15.89870  0.6063564  12.196489
  2       0.010  1.00  14.96578  0.6476910  11.313987
  2       0.100  0.25  12.91378  0.7345980   9.241582
  2       0.100  0.50  12.92061  0.7355502   9.204448
  2       0.100  1.00  13.10234  0.7311735   9.351619
  3       0.001  0.25  23.66269  0.4037450  19.424495
  3       0.001  0.50  20.61157  0.4325239  17.098166
  3       0.001  1.00  18.39104  0.4985592  14.784186
  3       0.010  0.25  16.03086  0.6062423  12.394085
  3       0.010  0.50  14.69050  0.6626125  11.102422
  3       0.010  1.00  13.39463  0.7153230   9.828823
  3       0.100  0.25  13.64349  0.7146619   9.480402
  3       0.100  0.50  14.28836  0.7001205   9.642150
  3       0.100  1.00  16.05817  0.6619788  10.415371

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 25 of 224 using max all ranger 1 
Random Forest 

1430 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1288, 1287, 1287, 1286, 1287, 1286, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.601440  0.8496720  5.252437
   2    extratrees  10.031557  0.8425465  6.172811
  13    variance     9.574423  0.8477262  4.672042
  13    extratrees   9.301354  0.8582732  4.934558
  24    variance     9.843369  0.8391371  4.770810
  24    extratrees   9.239933  0.8597217  4.813677

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 26 of 224 using same all ranger 1 
Random Forest 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 183, 185, 184, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    12.01297  0.7650551  7.752023
   2    extratrees  12.54142  0.7622270  8.931866
  13    variance    12.09520  0.7522882  7.216575
  13    extratrees  11.76277  0.7684247  7.262489
  24    variance    12.26341  0.7460038  7.391609
  24    extratrees  11.80971  0.7652143  7.208313

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 27 of 224 using max du ranger 1 
Random Forest 

1430 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1286, 1288, 1287, 1287, 1287, 1287, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance    10.004967  0.8381403  5.687420
   2    extratrees  10.729508  0.8212429  6.874903
  10    variance     9.618282  0.8473117  4.799129
  10    extratrees   9.795557  0.8441070  5.457749
  18    variance     9.882510  0.8385862  4.770674
  18    extratrees   9.667358  0.8475383  5.238681

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 28 of 224 using same du ranger 1 
Random Forest 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 183, 182, 184, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    12.15885  0.7607420  8.043907
   2    extratrees  12.90912  0.7489519  9.435971
  10    variance    11.89916  0.7583502  7.264042
  10    extratrees  11.97781  0.7611447  7.651592
  18    variance    12.09565  0.7502197  7.363721
  18    extratrees  11.90543  0.7607350  7.433236

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 29 of 224 using max rms ranger 1 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1430 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1286, 1287, 1286, 1288, 1287, 1288, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
  2     variance     9.959482  0.8361078  4.937308
  2     extratrees   9.633898  0.8492873  5.260735
  3     variance    10.047972  0.8327993  4.797863
  3     extratrees   9.599886  0.8489096  5.018171

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 30 of 224 using same rms ranger 1 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 184, 184, 184, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    12.18886  0.7475505  7.320122
  2     extratrees  11.78562  0.7783047  7.722261
  3     variance    12.31593  0.7416620  7.101401
  3     extratrees  11.55578  0.7803920  7.274488

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 31 of 224 using max hudgins ranger 1 
Random Forest 

1430 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1287, 1286, 1287, 1288, 1287, 1286, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance    10.430028  0.8268538  6.305964
   2    extratrees  11.413791  0.8014530  7.729307
   7    variance     9.589407  0.8483548  4.976220
   7    extratrees  10.040098  0.8383289  5.901413
  12    variance     9.874225  0.8384940  4.806722
  12    extratrees   9.730518  0.8466626  5.469597

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 32 of 224 using same hudgins ranger 1 
Random Forest 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 182, 184, 185, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE      
   2    variance    12.36287  0.7637564   8.481618
   2    extratrees  13.45913  0.7339437  10.131577
   7    variance    11.72851  0.7707335   7.190876
   7    extratrees  12.20634  0.7602833   8.123570
  12    variance    11.82034  0.7653364   7.153446
  12    extratrees  11.96357  0.7659237   7.675152

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 33 of 224 using max all lm 2 
Linear Regression 

613 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 551, 553, 551, 551, 552, 551, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.79857  0.7726614  8.892232

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 34 of 224 using same all lm 2 
Linear Regression 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 184, 183, 184, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.99273  0.7683252  9.275957

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 35 of 224 using max du lm 2 
Linear Regression 

613 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 553, 552, 551, 552, 551, 551, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.13107  0.7610521  9.398909

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 36 of 224 using same du lm 2 
Linear Regression 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 185, 184, 185, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.09834  0.7636169  9.540838

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 37 of 224 using max rms lm 2 
Linear Regression 

613 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 551, 551, 552, 551, 552, 551, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  13.67673  0.6961109  10.43656

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 38 of 224 using same rms lm 2 
Linear Regression 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 185, 183, 185, 183, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  13.3125  0.7180214  10.28678

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 39 of 224 using max hudgins lm 2 
Linear Regression 

613 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 552, 551, 551, 552, 550, 551, ... 
Resampling results:

  RMSE    Rsquared   MAE     
  12.537  0.7444853  9.847715

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 40 of 224 using same hudgins lm 2 
Linear Regression 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 184, 183, 183, 185, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.35586  0.7564115  9.741795

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 41 of 224 using max all knn 2 
k-Nearest Neighbors 

613 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 551, 552, 552, 552, 550, 554, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.870360  0.8318635  4.733519
  7  9.988458  0.8284665  5.137073
  9  9.982297  0.8296992  5.371336

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 42 of 224 using same all knn 2 
k-Nearest Neighbors 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 184, 183, 184, 182, 185, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.24193  0.8124012  6.135449
  7  10.81398  0.7930383  6.764339
  9  11.54029  0.7686326  7.578535

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 43 of 224 using max du knn 2 
k-Nearest Neighbors 

613 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 551, 553, 551, 551, 551, 553, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.891101  0.8332612  4.702963
  7  10.018478  0.8307797  5.112061
  9   9.975697  0.8326060  5.296510

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 44 of 224 using same du knn 2 
k-Nearest Neighbors 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 184, 184, 184, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.32048  0.8111488  6.202709
  7  10.91798  0.7921661  6.836073
  9  11.62134  0.7695132  7.596557

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 45 of 224 using max rms knn 2 
k-Nearest Neighbors 

613 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 554, 551, 551, 551, 552, 552, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.584136  0.8433854  4.413441
  7  9.626303  0.8417324  4.572444
  9  9.451874  0.8476088  4.672675

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 46 of 224 using same rms knn 2 
k-Nearest Neighbors 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 185, 184, 184, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.314372  0.8365585  5.340083
  7  9.256841  0.8420284  5.523732
  9  9.727134  0.8310647  6.040777

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 47 of 224 using max hudgins knn 2 
k-Nearest Neighbors 

613 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 553, 552, 551, 551, 551, 552, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE      
  5  14.83479  0.6422339   9.522350
  7  14.73223  0.6451362   9.727093
  9  14.89735  0.6364464  10.062366

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 48 of 224 using same hudgins knn 2 
k-Nearest Neighbors 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 184, 185, 184, 184, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE      
  5  14.55958  0.6521094   9.896481
  7  15.08052  0.6264254  10.564464
  9  15.64285  0.5995745  11.218772

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 49 of 224 using max all svmPoly 2 
Support Vector Machines with Polynomial Kernel 

613 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 553, 551, 551, 552, 553, 551, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.79171  0.4951714  15.232450
  1       0.001  0.50  16.85937  0.5780302  13.324907
  1       0.001  1.00  15.51325  0.6287421  12.077269
  1       0.010  0.25  14.47827  0.6632124  11.191594
  1       0.010  0.50  13.93215  0.6854586  10.759135
  1       0.010  1.00  13.35202  0.7106012  10.314655
  1       0.100  0.25  12.66683  0.7397307   9.748366
  1       0.100  0.50  12.22026  0.7571158   9.306136
  1       0.100  1.00  11.99264  0.7651556   9.010609
  2       0.001  0.25  16.81990  0.5807354  13.291389
  2       0.001  0.50  15.43518  0.6332699  12.017297
  2       0.001  1.00  14.50414  0.6647601  11.213888
  2       0.010  0.25  12.08432  0.7657196   8.931893
  2       0.010  0.50  11.66990  0.7779368   8.453765
  2       0.010  1.00  11.48386  0.7835793   8.219016
  2       0.100  0.25  11.07322  0.7981310   7.730935
  2       0.100  0.50  10.95531  0.8023381   7.611927
  2       0.100  1.00  10.83065  0.8070806   7.530431
  3       0.001  0.25  15.87076  0.6187331  12.426479
  3       0.001  0.50  14.73744  0.6587027  11.413375
  3       0.001  1.00  13.82260  0.6937402  10.667007
  3       0.010  0.25  11.63388  0.7790680   8.332032
  3       0.010  0.50  11.41191  0.7861642   8.065484
  3       0.010  1.00  11.23330  0.7922028   7.854907
  3       0.100  0.25  10.94929  0.8028120   7.209096
  3       0.100  0.50  11.14980  0.7974780   7.394333
  3       0.100  1.00  11.80889  0.7782916   7.838851

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 50 of 224 using same all svmPoly 2 
Support Vector Machines with Polynomial Kernel 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 185, 183, 183, 183, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  23.40147  0.4425824  19.220066
  1       0.001  0.50  19.76120  0.4645615  16.407971
  1       0.001  1.00  17.76679  0.5431938  14.321205
  1       0.010  0.25  15.51006  0.6370072  12.405640
  1       0.010  0.50  14.51070  0.6682666  11.461496
  1       0.010  1.00  13.91577  0.6922347  10.905232
  1       0.100  0.25  13.19215  0.7215056  10.277429
  1       0.100  0.50  12.71447  0.7406066   9.885003
  1       0.100  1.00  12.27153  0.7577219   9.548532
  2       0.001  0.25  19.76733  0.4651590  16.398449
  2       0.001  0.50  17.74826  0.5449440  14.303572
  2       0.001  1.00  15.88261  0.6239954  12.724781
  2       0.010  0.25  13.37683  0.7241540  10.524492
  2       0.010  0.50  12.04879  0.7704634   9.304249
  2       0.010  1.00  11.36539  0.7936545   8.555840
  2       0.100  0.25  10.89981  0.8071540   8.037141
  2       0.100  0.50  10.96522  0.8054097   8.034472
  2       0.100  1.00  11.13602  0.8014021   8.125397
  3       0.001  0.25  18.61367  0.5108897  15.150748
  3       0.001  0.50  16.63177  0.5943810  13.377052
  3       0.001  1.00  14.99344  0.6588977  11.936514
  3       0.010  0.25  11.69731  0.7848509   8.913039
  3       0.010  0.50  11.17229  0.7999138   8.337309
  3       0.010  1.00  10.94003  0.8058247   8.092220
  3       0.100  0.25  12.15005  0.7725592   7.952242
  3       0.100  0.50  14.14051  0.7287353   8.784788
  3       0.100  1.00  16.70182  0.6834989   9.862088

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 51 of 224 using max du svmPoly 2 
Support Vector Machines with Polynomial Kernel 

613 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 550, 552, 552, 552, 552, 553, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  20.22688  0.4275245  16.658194
  1       0.001  0.50  18.39848  0.5007730  14.590106
  1       0.001  1.00  16.85066  0.5636114  13.055701
  1       0.010  0.25  15.63163  0.6099341  11.927970
  1       0.010  0.50  14.96592  0.6376283  11.421946
  1       0.010  1.00  14.40360  0.6627138  11.045567
  1       0.100  0.25  13.64852  0.6968064  10.573782
  1       0.100  0.50  13.12500  0.7199781  10.203092
  1       0.100  1.00  12.65657  0.7400174   9.814924
  2       0.001  0.25  18.38109  0.5021970  14.575996
  2       0.001  0.50  16.80644  0.5665456  13.020994
  2       0.001  1.00  15.77744  0.6059121  12.069477
  2       0.010  0.25  13.18930  0.7251541   9.912322
  2       0.010  0.50  12.35709  0.7546506   9.119343
  2       0.010  1.00  11.90502  0.7692359   8.660737
  2       0.100  0.25  11.40912  0.7861087   7.996576
  2       0.100  0.50  11.34208  0.7885964   7.923826
  2       0.100  1.00  11.31288  0.7898449   7.883919
  3       0.001  0.25  17.30617  0.5483059  13.529173
  3       0.001  0.50  16.10349  0.5965035  12.369776
  3       0.001  1.00  15.16792  0.6320541  11.569761
  3       0.010  0.25  12.25461  0.7585761   8.948945
  3       0.010  0.50  11.84332  0.7710380   8.496569
  3       0.010  1.00  11.58565  0.7799778   8.213265
  3       0.100  0.25  11.35172  0.7878892   7.593076
  3       0.100  0.50  11.70433  0.7760613   7.857733
  3       0.100  1.00  12.19556  0.7608637   8.163982

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 52 of 224 using same du svmPoly 2 
Support Vector Machines with Polynomial Kernel 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 183, 184, 184, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  24.83222  0.4116719  20.180533
  1       0.001  0.50  21.96532  0.4315130  18.139597
  1       0.001  1.00  19.15063  0.4791578  15.592073
  1       0.010  0.25  16.88285  0.5756340  13.481562
  1       0.010  0.50  15.62406  0.6239407  12.268280
  1       0.010  1.00  14.96447  0.6483506  11.659234
  1       0.100  0.25  14.23176  0.6797298  11.055998
  1       0.100  0.50  13.78840  0.6992848  10.695985
  1       0.100  1.00  13.30895  0.7190296  10.357485
  2       0.001  0.25  21.95670  0.4306978  18.126204
  2       0.001  0.50  19.14961  0.4795578  15.586130
  2       0.001  1.00  17.41317  0.5525656  13.903790
  2       0.010  0.25  15.01546  0.6569351  11.777172
  2       0.010  0.50  13.77356  0.7051322  10.753062
  2       0.010  1.00  12.42149  0.7559969   9.629643
  2       0.100  0.25  11.50418  0.7854429   8.409153
  2       0.100  0.50  11.81503  0.7765374   8.599278
  2       0.100  1.00  12.09610  0.7684407   8.780882
  3       0.001  0.25  19.99652  0.4534614  16.443831
  3       0.001  0.50  18.07607  0.5243707  14.489953
  3       0.001  1.00  16.46684  0.5954076  13.098119
  3       0.010  0.25  13.40816  0.7254204  10.445926
  3       0.010  0.50  11.98817  0.7728998   9.161205
  3       0.010  1.00  11.52540  0.7860847   8.693048
  3       0.100  0.25  11.80884  0.7835368   8.129499
  3       0.100  0.50  12.76469  0.7605841   8.636468
  3       0.100  1.00  15.17743  0.7021342   9.593209

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 53 of 224 using max rms svmPoly 2 
Support Vector Machines with Polynomial Kernel 

613 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 551, 552, 552, 553, 551, 552, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  25.70556  0.4779484  20.938552
  1       0.001  0.50  23.14320  0.5074454  19.283684
  1       0.001  1.00  19.22989  0.5252386  16.260380
  1       0.010  0.25  16.12149  0.6375785  13.170635
  1       0.010  0.50  14.42707  0.6870598  11.538102
  1       0.010  1.00  13.81273  0.6950554  10.749812
  1       0.100  0.25  13.69372  0.6962261  10.467900
  1       0.100  0.50  13.70058  0.6962167  10.425321
  1       0.100  1.00  13.71157  0.6960265  10.414883
  2       0.001  0.25  23.14255  0.5074489  19.282369
  2       0.001  0.50  19.22794  0.5252229  16.257208
  2       0.001  1.00  16.80847  0.6090700  13.821400
  2       0.010  0.25  14.36543  0.6902171  11.473294
  2       0.010  0.50  13.68936  0.7011515  10.652002
  2       0.010  1.00  13.45274  0.7068291  10.313832
  2       0.100  0.25  11.93798  0.7683857   8.714830
  2       0.100  0.50  11.89602  0.7692234   8.550768
  2       0.100  1.00  11.90969  0.7687086   8.482601
  3       0.001  0.25  20.95043  0.5177951  17.714764
  3       0.001  0.50  17.75205  0.5732631  14.761330
  3       0.001  1.00  15.55406  0.6575249  12.656650
  3       0.010  0.25  13.77189  0.7034615  10.813498
  3       0.010  0.50  13.36925  0.7115408  10.280980
  3       0.010  1.00  13.06422  0.7231919   9.950836
  3       0.100  0.25  11.43646  0.7868135   8.079993
  3       0.100  0.50  11.24326  0.7936598   7.854718
  3       0.100  1.00  11.07510  0.7995202   7.651346

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 54 of 224 using same rms svmPoly 2 
Support Vector Machines with Polynomial Kernel 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 182, 183, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.93175  0.4901816  22.089536
  1       0.001  0.50  26.36897  0.4916205  21.439579
  1       0.001  1.00  24.80953  0.5078986  20.224887
  1       0.010  0.25  20.43952  0.5423537  17.342170
  1       0.010  0.50  17.35737  0.6000967  14.516355
  1       0.010  1.00  15.24885  0.6817505  12.679498
  1       0.100  0.25  13.51579  0.7168415  10.695177
  1       0.100  0.50  13.37091  0.7175718  10.366540
  1       0.100  1.00  13.35450  0.7181442  10.279976
  2       0.001  0.25  26.36853  0.4916467  21.439040
  2       0.001  0.50  24.80964  0.5079322  20.223685
  2       0.001  1.00  21.71374  0.5398244  18.283410
  2       0.010  0.25  17.34839  0.6018668  14.489899
  2       0.010  0.50  15.18314  0.6850933  12.609456
  2       0.010  1.00  13.60121  0.7212581  10.921447
  2       0.100  0.25  12.07730  0.7661271   9.223788
  2       0.100  0.50  11.64111  0.7777484   8.753588
  2       0.100  1.00  11.47561  0.7812449   8.480204
  3       0.001  0.25  25.60129  0.4978633  20.795494
  3       0.001  0.50  23.07535  0.5269731  19.221420
  3       0.001  1.00  19.38988  0.5471756  16.453083
  3       0.010  0.25  15.95425  0.6570409  13.240968
  3       0.010  0.50  13.90997  0.7191956  11.325992
  3       0.010  1.00  13.07575  0.7334448  10.264330
  3       0.100  0.25  11.31265  0.7894390   8.350457
  3       0.100  0.50  10.94502  0.8000436   7.927402
  3       0.100  1.00  10.61018  0.8111248   7.600462

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 55 of 224 using max hudgins svmPoly 2 
Support Vector Machines with Polynomial Kernel 

613 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 550, 552, 552, 551, 552, 554, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.67779  0.3950601  18.759537
  1       0.001  0.50  19.71943  0.4414450  16.130133
  1       0.001  1.00  18.06785  0.5083718  14.232479
  1       0.010  0.25  16.34856  0.5813716  12.534115
  1       0.010  0.50  15.50538  0.6159767  11.852408
  1       0.010  1.00  14.72930  0.6509210  11.302683
  1       0.100  0.25  13.71911  0.6946297  10.678774
  1       0.100  0.50  13.09011  0.7215488  10.257181
  1       0.100  1.00  12.74772  0.7356197   9.994112
  2       0.001  0.25  19.71886  0.4417463  16.128957
  2       0.001  0.50  18.05468  0.5092478  14.220776
  2       0.001  1.00  16.68287  0.5680042  12.826521
  2       0.010  0.25  14.76884  0.6573894  11.283736
  2       0.010  0.50  13.34130  0.7191330  10.136759
  2       0.010  1.00  12.24058  0.7598021   9.158643
  2       0.100  0.25  11.36844  0.7878363   8.040994
  2       0.100  0.50  11.29073  0.7906577   7.972518
  2       0.100  1.00  11.22095  0.7929989   7.894762
  3       0.001  0.25  18.71411  0.4806008  14.990400
  3       0.001  0.50  17.18116  0.5483222  13.302826
  3       0.001  1.00  16.01615  0.5964905  12.263523
  3       0.010  0.25  13.15305  0.7298754   9.890881
  3       0.010  0.50  12.19073  0.7627478   9.006875
  3       0.010  1.00  11.73775  0.7762146   8.536373
  3       0.100  0.25  11.27815  0.7905760   7.690920
  3       0.100  0.50  11.43777  0.7850021   7.715873
  3       0.100  1.00  11.92408  0.7686391   7.909701

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 56 of 224 using same hudgins svmPoly 2 
Support Vector Machines with Polynomial Kernel 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 185, 183, 185, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.10473  0.4005646  21.226751
  1       0.001  0.50  24.24089  0.4111327  19.818097
  1       0.001  1.00  21.01359  0.4261530  17.387687
  1       0.010  0.25  18.34261  0.5044068  14.686839
  1       0.010  0.50  16.71027  0.5740243  13.251254
  1       0.010  1.00  15.58733  0.6183303  12.168067
  1       0.100  0.25  14.76367  0.6579586  11.414988
  1       0.100  0.50  14.05801  0.6898293  10.913329
  1       0.100  1.00  13.34585  0.7208739  10.411248
  2       0.001  0.25  24.23735  0.4107000  19.814196
  2       0.001  0.50  21.00984  0.4257098  17.378052
  2       0.001  1.00  18.77385  0.4876781  15.138406
  2       0.010  0.25  16.54533  0.5849627  13.122711
  2       0.010  0.50  15.21306  0.6383202  11.885919
  2       0.010  1.00  14.09749  0.6902492  10.959700
  2       0.100  0.25  11.38219  0.7917350   8.536329
  2       0.100  0.50  11.36931  0.7908567   8.471466
  2       0.100  1.00  11.71325  0.7800330   8.646635
  3       0.001  0.25  22.56836  0.4219321  18.600402
  3       0.001  0.50  19.60724  0.4623391  16.012051
  3       0.001  1.00  17.86374  0.5259638  14.243841
  3       0.010  0.25  15.40786  0.6378130  12.121396
  3       0.010  0.50  13.92348  0.7021021  10.893061
  3       0.010  1.00  12.34752  0.7628894   9.572790
  3       0.100  0.25  12.22143  0.7734987   8.606498
  3       0.100  0.50  12.12086  0.7796116   8.533856
  3       0.100  1.00  12.24320  0.7773062   8.581921

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 57 of 224 using max all ranger 2 
Random Forest 

613 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 552, 551, 551, 551, 551, 552, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    9.014823  0.8630365  4.828713
   2    extratrees  9.545393  0.8559206  5.878349
  13    variance    9.086299  0.8567171  4.431610
  13    extratrees  8.918717  0.8648811  4.613935
  24    variance    9.285335  0.8508310  4.560052
  24    extratrees  8.850973  0.8659037  4.489339

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 58 of 224 using same all ranger 2 
Random Forest 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 182, 183, 184, 183, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    8.839014  0.8703753  5.579663
   2    extratrees  9.849487  0.8568371  6.994382
  13    variance    8.914921  0.8582060  5.298938
  13    extratrees  8.775881  0.8644546  5.261969
  24    variance    9.203751  0.8504196  5.503716
  24    extratrees  8.757036  0.8637644  5.161317

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 59 of 224 using max du ranger 2 
Random Forest 

613 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 551, 553, 552, 551, 552, 551, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.351834  0.8555385  5.150124
   2    extratrees  10.094822  0.8414655  6.387377
  10    variance     9.092890  0.8590226  4.496527
  10    extratrees   9.328105  0.8550987  5.013966
  18    variance     9.252256  0.8542255  4.546358
  18    extratrees   9.205981  0.8579630  4.827576

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 60 of 224 using same du ranger 2 
Random Forest 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 183, 183, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.039559  0.8640377  5.799997
   2    extratrees  10.272397  0.8444764  7.412250
  10    variance     9.119070  0.8514988  5.394417
  10    extratrees   8.968524  0.8590711  5.573814
  18    variance     9.412363  0.8432345  5.581107
  18    extratrees   8.835337  0.8599940  5.336283

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 61 of 224 using max rms ranger 2 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

613 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 553, 553, 551, 553, 551, 552, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    8.991150  0.8607662  4.366927
  2     extratrees  9.079039  0.8635313  5.068264
  3     variance    9.104479  0.8570700  4.291908
  3     extratrees  8.918619  0.8657109  4.718003

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 62 of 224 using same rms ranger 2 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 183, 185, 183, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    8.827431  0.8602139  4.974146
  2     extratrees  9.189588  0.8621710  5.973516
  3     variance    8.988067  0.8538107  4.901972
  3     extratrees  8.945007  0.8626290  5.452570

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 63 of 224 using max hudgins ranger 2 
Random Forest 

613 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 552, 551, 552, 552, 552, 551, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.895508  0.8416024  5.758620
   2    extratrees  10.787259  0.8219680  7.186419
   7    variance     9.329749  0.8535965  4.754011
   7    extratrees   9.757240  0.8438642  5.544148
  12    variance     9.387966  0.8512577  4.615915
  12    extratrees   9.548009  0.8489410  5.184318

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 64 of 224 using same hudgins ranger 2 
Random Forest 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 183, 183, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.506454  0.8551012  6.408105
   2    extratrees  10.947396  0.8263315  8.147841
   7    variance     8.960484  0.8566335  5.503813
   7    extratrees   9.466903  0.8494245  6.283307
  12    variance     9.193767  0.8488985  5.604524
  12    extratrees   9.188713  0.8536733  5.866365

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 65 of 224 using max all lm 3 
Linear Regression 

409 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 369, 369, 367, 368, 368, 368, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.47965  0.7889871  8.726533

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 66 of 224 using same all lm 3 
Linear Regression 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 184, 184, 183, 184, 184, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  11.8257  0.7780643  8.836456

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 67 of 224 using max du lm 3 
Linear Regression 

409 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 370, 367, 368, 368, 368, 368, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.61853  0.7818347  9.150208

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 68 of 224 using same du lm 3 
Linear Regression 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 183, 184, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.56514  0.7920345  9.008962

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 69 of 224 using max rms lm 3 
Linear Regression 

409 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 368, 368, 369, 368, 367, 368, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  13.06154  0.727742  10.10532

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 70 of 224 using same rms lm 3 
Linear Regression 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 184, 183, 184, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  12.73447  0.750561  9.791537

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 71 of 224 using max hudgins lm 3 
Linear Regression 

409 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 367, 369, 369, 367, 368, 369, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.06487  0.7669745  9.592333

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 72 of 224 using same hudgins lm 3 
Linear Regression 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 185, 182, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.85903  0.7797334  9.317578

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 73 of 224 using max all knn 3 
k-Nearest Neighbors 

409 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 369, 367, 367, 368, 368, 369, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  8.166933  0.8793283  3.949141
  7  8.549693  0.8706672  4.452294
  9  8.767450  0.8659756  4.858510

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 74 of 224 using same all knn 3 
k-Nearest Neighbors 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 184, 183, 183, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.14000  0.8189072  5.838888
  7  10.48401  0.8082245  6.491137
  9  10.89890  0.7975569  6.953860

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 75 of 224 using max du knn 3 
k-Nearest Neighbors 

409 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 368, 368, 369, 367, 368, 368, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  8.111494  0.8825005  3.876856
  7  8.604026  0.8707673  4.470229
  9  8.814131  0.8669505  4.857310

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 76 of 224 using same du knn 3 
k-Nearest Neighbors 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 182, 185, 183, 184, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.13803  0.8228826  5.777354
  7  10.66036  0.8089986  6.492117
  9  11.05219  0.7964432  6.968190

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 77 of 224 using max rms knn 3 
k-Nearest Neighbors 

409 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 368, 367, 367, 368, 368, 369, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  8.300150  0.8800088  3.707930
  7  8.293144  0.8806129  3.914829
  9  8.521701  0.8751073  4.283854

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 78 of 224 using same rms knn 3 
k-Nearest Neighbors 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 182, 182, 184, 183, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.665025  0.8272414  5.385883
  7  9.328970  0.8388407  5.444325
  9  9.166508  0.8448995  5.593050

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 79 of 224 using max hudgins knn 3 
k-Nearest Neighbors 

409 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 369, 369, 368, 367, 368, 370, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  14.19355  0.6726025  9.255431
  7  14.23505  0.6703517  9.536321
  9  14.43680  0.6609996  9.793729

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 80 of 224 using same hudgins knn 3 
k-Nearest Neighbors 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 184, 184, 183, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  15.10671  0.6350448  10.39581
  7  15.27050  0.6262962  10.72485
  9  15.69652  0.6071988  11.17979

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 81 of 224 using max all svmPoly 3 
Support Vector Machines with Polynomial Kernel 

409 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 369, 368, 368, 367, 368, 369, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  19.60092  0.4808002  16.379796
  1       0.001  0.50  17.58982  0.5605356  14.164239
  1       0.001  1.00  15.76207  0.6343038  12.535804
  1       0.010  0.25  14.33046  0.6806949  11.297802
  1       0.010  0.50  13.72138  0.7030966  10.808830
  1       0.010  1.00  13.13384  0.7257721  10.350518
  1       0.100  0.25  12.35722  0.7558261   9.681993
  1       0.100  0.50  11.89412  0.7732767   9.222648
  1       0.100  1.00  11.61605  0.7825722   8.870205
  2       0.001  0.25  17.56271  0.5626046  14.141660
  2       0.001  0.50  15.70670  0.6374493  12.488659
  2       0.001  1.00  14.49005  0.6790895  11.454550
  2       0.010  0.25  11.73163  0.7863538   8.971745
  2       0.010  0.50  11.19719  0.8000559   8.318189
  2       0.010  1.00  10.94930  0.8057196   7.984233
  2       0.100  0.25  10.58137  0.8170588   7.368560
  2       0.100  0.50  10.47690  0.8207887   7.298527
  2       0.100  1.00  10.34791  0.8253549   7.205480
  3       0.001  0.25  16.37992  0.6149153  13.052899
  3       0.001  0.50  14.84929  0.6696634  11.766159
  3       0.001  1.00  13.78864  0.7044522  10.864499
  3       0.010  0.25  11.09823  0.8033120   8.159818
  3       0.010  0.50  10.84871  0.8090948   7.780747
  3       0.010  1.00  10.65299  0.8148501   7.511420
  3       0.100  0.25  10.97643  0.8061591   7.121328
  3       0.100  0.50  12.08919  0.7748577   7.689911
  3       0.100  1.00  13.44174  0.7377354   8.368878

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 82 of 224 using same all svmPoly 3 
Support Vector Machines with Polynomial Kernel 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 182, 183, 184, 184, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  23.35938  0.4745346  19.567140
  1       0.001  0.50  19.54495  0.5009147  16.561367
  1       0.001  1.00  17.29566  0.5811790  14.056205
  1       0.010  0.25  14.97978  0.6696577  11.944324
  1       0.010  0.50  14.06703  0.6960786  11.021818
  1       0.010  1.00  13.56071  0.7148684  10.570787
  1       0.100  0.25  12.80624  0.7450570   9.946551
  1       0.100  0.50  12.21555  0.7665701   9.432921
  1       0.100  1.00  11.73180  0.7838013   8.982340
  2       0.001  0.25  19.54522  0.5008342  16.555836
  2       0.001  0.50  17.27153  0.5831299  14.034484
  2       0.001  1.00  15.36644  0.6579576  12.296410
  2       0.010  0.25  12.75430  0.7569558   9.953160
  2       0.010  0.50  11.46633  0.7979079   8.774675
  2       0.010  1.00  11.00191  0.8104612   8.245444
  2       0.100  0.25  10.75568  0.8171749   7.818306
  2       0.100  0.50  10.77081  0.8171347   7.785309
  2       0.100  1.00  10.80788  0.8169780   7.736291
  3       0.001  0.25  18.14669  0.5484943  15.010151
  3       0.001  0.50  16.04651  0.6357545  12.890496
  3       0.001  1.00  14.51744  0.6881727  11.521914
  3       0.010  0.25  11.33710  0.8031836   8.586732
  3       0.010  0.50  10.92167  0.8126624   8.128267
  3       0.010  1.00  10.88187  0.8123774   8.008560
  3       0.100  0.25  11.82546  0.7856524   7.943883
  3       0.100  0.50  12.00386  0.7800992   8.098560
  3       0.100  1.00  12.78067  0.7588355   8.626251

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 83 of 224 using max du svmPoly 3 
Support Vector Machines with Polynomial Kernel 

409 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 368, 368, 367, 368, 369, 370, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  21.57283  0.4200283  18.020978
  1       0.001  0.50  19.00779  0.4816128  15.526814
  1       0.001  1.00  17.29293  0.5580320  13.623464
  1       0.010  0.25  15.61677  0.6253410  12.177410
  1       0.010  0.50  14.82642  0.6549106  11.516655
  1       0.010  1.00  14.24029  0.6799163  11.124055
  1       0.100  0.25  13.43908  0.7146657  10.600401
  1       0.100  0.50  12.89599  0.7362610  10.207224
  1       0.100  1.00  12.46104  0.7538332   9.853094
  2       0.001  0.25  19.00098  0.4824435  15.520086
  2       0.001  0.50  17.26700  0.5598596  13.601500
  2       0.001  1.00  15.85189  0.6172877  12.381316
  2       0.010  0.25  13.38847  0.7295021  10.360874
  2       0.010  0.50  12.04325  0.7749793   9.191611
  2       0.010  1.00  11.43579  0.7932325   8.534312
  2       0.100  0.25  10.94437  0.8066734   7.663383
  2       0.100  0.50  10.86803  0.8091651   7.574028
  2       0.100  1.00  10.82244  0.8109238   7.547750
  3       0.001  0.25  17.95425  0.5289655  14.301247
  3       0.001  0.50  16.29994  0.5996410  12.740807
  3       0.001  1.00  15.17253  0.6461950  11.809413
  3       0.010  0.25  11.88888  0.7817165   8.970197
  3       0.010  0.50  11.32978  0.7963667   8.347436
  3       0.010  1.00  11.04531  0.8040417   7.915026
  3       0.100  0.25  11.64359  0.7863342   7.697904
  3       0.100  0.50  12.51930  0.7600095   8.027311
  3       0.100  1.00  14.35235  0.7095608   8.805075

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 84 of 224 using same du svmPoly 3 
Support Vector Machines with Polynomial Kernel 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 183, 184, 184, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  24.78053  0.4239833  20.738822
  1       0.001  0.50  21.86444  0.4442970  18.439044
  1       0.001  1.00  18.90673  0.4945822  15.611317
  1       0.010  0.25  16.57751  0.5973128  13.073802
  1       0.010  0.50  15.42746  0.6423652  12.005084
  1       0.010  1.00  14.74444  0.6677663  11.385163
  1       0.100  0.25  13.97312  0.6999371  10.816474
  1       0.100  0.50  13.40410  0.7235114  10.472698
  1       0.100  1.00  12.77191  0.7475510  10.018402
  2       0.001  0.25  21.86029  0.4438419  18.433035
  2       0.001  0.50  18.89831  0.4956318  15.601209
  2       0.001  1.00  17.19128  0.5718107  13.635262
  2       0.010  0.25  14.63053  0.6847360  11.328598
  2       0.010  0.50  13.17771  0.7393508  10.159444
  2       0.010  1.00  11.85671  0.7843659   9.074889
  2       0.100  0.25  10.89744  0.8090815   7.990078
  2       0.100  0.50  10.90651  0.8088156   7.913287
  2       0.100  1.00  10.95420  0.8078421   7.937804
  3       0.001  0.25  19.84668  0.4607421  16.567266
  3       0.001  0.50  17.82411  0.5433560  14.315718
  3       0.001  1.00  16.14954  0.6164875  12.674874
  3       0.010  0.25  12.81370  0.7586001   9.809958
  3       0.010  0.50  11.66449  0.7917062   8.825563
  3       0.010  1.00  11.17263  0.8029156   8.350761
  3       0.100  0.25  11.95361  0.7775503   8.280590
  3       0.100  0.50  12.61912  0.7558549   8.596862
  3       0.100  1.00  13.60527  0.7279988   9.128678

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 85 of 224 using max rms svmPoly 3 
Support Vector Machines with Polynomial Kernel 

409 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 368, 368, 369, 367, 367, 368, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.25889  0.4960813  21.749148
  1       0.001  0.50  24.56752  0.5030576  20.363814
  1       0.001  1.00  21.19785  0.5354148  18.100682
  1       0.010  0.25  17.00451  0.6134567  14.152084
  1       0.010  0.50  14.77330  0.6997328  12.156098
  1       0.010  1.00  13.38240  0.7275469  10.754114
  1       0.100  0.25  13.03903  0.7284169  10.154803
  1       0.100  0.50  13.05326  0.7280039  10.095336
  1       0.100  1.00  13.07506  0.7274873  10.069189
  2       0.001  0.25  24.56667  0.5030630  20.362939
  2       0.001  0.50  21.19645  0.5354090  18.098700
  2       0.001  1.00  17.76658  0.5816352  14.924350
  2       0.010  0.25  14.73564  0.7016653  12.109149
  2       0.010  0.50  13.30670  0.7312245  10.682727
  2       0.010  1.00  12.90935  0.7360182  10.088435
  2       0.100  0.25  11.47813  0.7890393   8.572136
  2       0.100  0.50  11.29347  0.7948670   8.218515
  2       0.100  1.00  11.33070  0.7936914   8.152500
  3       0.001  0.25  22.73774  0.5259773  19.225299
  3       0.001  0.50  18.89908  0.5480707  16.057523
  3       0.001  1.00  16.39192  0.6408887  13.598818
  3       0.010  0.25  13.67065  0.7270444  11.082974
  3       0.010  0.50  12.89910  0.7398766  10.170443
  3       0.010  1.00  12.58810  0.7473116   9.731243
  3       0.100  0.25  10.88777  0.8097866   7.764519
  3       0.100  0.50  10.68479  0.8159574   7.529878
  3       0.100  1.00  10.45918  0.8230061   7.283550

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 86 of 224 using same rms svmPoly 3 
Support Vector Machines with Polynomial Kernel 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 182, 185, 182, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  27.03641  0.5205913  22.562483
  1       0.001  0.50  26.19459  0.5206434  21.870642
  1       0.001  1.00  24.60363  0.5325437  20.556534
  1       0.010  0.25  20.03641  0.5794035  17.314759
  1       0.010  0.50  16.77852  0.6398064  14.145797
  1       0.010  1.00  14.30862  0.7235949  11.832308
  1       0.100  0.25  12.92946  0.7453692  10.162971
  1       0.100  0.50  12.74352  0.7453630   9.899329
  1       0.100  1.00  12.74581  0.7446565   9.848457
  2       0.001  0.25  26.19403  0.5206769  21.870213
  2       0.001  0.50  24.60271  0.5325791  20.555818
  2       0.001  1.00  21.28182  0.5690371  18.319522
  2       0.010  0.25  16.76418  0.6410517  14.120806
  2       0.010  0.50  14.26688  0.7259261  11.787280
  2       0.010  1.00  13.02891  0.7474201  10.357467
  2       0.100  0.25  11.60468  0.7881457   8.955660
  2       0.100  0.50  11.14970  0.8027306   8.450813
  2       0.100  1.00  10.92162  0.8095195   8.134014
  3       0.001  0.25  25.54587  0.5300039  21.234031
  3       0.001  0.50  22.91159  0.5551339  19.388448
  3       0.001  1.00  18.88224  0.5833794  16.287518
  3       0.010  0.25  15.19129  0.7008199  12.636101
  3       0.010  0.50  13.22935  0.7479908  10.686206
  3       0.010  1.00  12.62250  0.7542591   9.856396
  3       0.100  0.25  10.79945  0.8148372   8.021316
  3       0.100  0.50  10.59147  0.8205722   7.789561
  3       0.100  1.00  10.49414  0.8234288   7.707674

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 87 of 224 using max hudgins svmPoly 3 
Support Vector Machines with Polynomial Kernel 

409 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 368, 368, 368, 369, 367, 368, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  24.17864  0.4022100  20.001280
  1       0.001  0.50  20.89691  0.4267895  17.413772
  1       0.001  1.00  18.60527  0.4939340  15.059722
  1       0.010  0.25  16.58906  0.5824658  12.899576
  1       0.010  0.50  15.51812  0.6287123  12.053870
  1       0.010  1.00  14.64754  0.6629312  11.406356
  1       0.100  0.25  13.63001  0.7062045  10.760850
  1       0.100  0.50  12.95442  0.7333647  10.320548
  1       0.100  1.00  12.43553  0.7534216   9.913828
  2       0.001  0.25  20.89546  0.4265800  17.410049
  2       0.001  0.50  18.59833  0.4944758  15.054365
  2       0.001  1.00  17.06313  0.5624871  13.326165
  2       0.010  0.25  14.97333  0.6598439  11.642507
  2       0.010  0.50  13.50723  0.7222486  10.513772
  2       0.010  1.00  12.12631  0.7715779   9.368577
  2       0.100  0.25  10.91743  0.8072951   7.788027
  2       0.100  0.50  10.86997  0.8091516   7.693838
  2       0.100  1.00  10.82895  0.8104732   7.653880
  3       0.001  0.25  19.36755  0.4630570  15.903641
  3       0.001  0.50  17.66533  0.5379024  13.959819
  3       0.001  1.00  16.18018  0.6019064  12.549134
  3       0.010  0.25  13.35473  0.7337709  10.351509
  3       0.010  0.50  11.94774  0.7798235   9.129906
  3       0.010  1.00  11.27130  0.7983604   8.420471
  3       0.100  0.25  11.25369  0.7973520   7.765128
  3       0.100  0.50  11.83092  0.7791881   7.942960
  3       0.100  1.00  12.62925  0.7543831   8.168619

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 88 of 224 using same hudgins svmPoly 3 
Support Vector Machines with Polynomial Kernel 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 184, 183, 182, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  25.94774  0.4261847  21.773048
  1       0.001  0.50  24.24711  0.4311530  20.323694
  1       0.001  1.00  20.96138  0.4507357  17.664573
  1       0.010  0.25  18.01208  0.5346834  14.376618
  1       0.010  0.50  16.46227  0.6072964  12.852747
  1       0.010  1.00  15.40979  0.6514694  11.990853
  1       0.100  0.25  14.36910  0.6906657  11.200747
  1       0.100  0.50  13.49594  0.7249632  10.587307
  1       0.100  1.00  12.67588  0.7535681   9.924791
  2       0.001  0.25  24.24661  0.4308534  20.321658
  2       0.001  0.50  20.96148  0.4504096  17.661426
  2       0.001  1.00  18.43268  0.5142118  15.026392
  2       0.010  0.25  16.25528  0.6198195  12.658279
  2       0.010  0.50  14.83428  0.6796211  11.515476
  2       0.010  1.00  13.40985  0.7349506  10.403082
  2       0.100  0.25  10.99123  0.8090969   8.142034
  2       0.100  0.50  10.75413  0.8149356   7.900896
  2       0.100  1.00  10.61533  0.8194059   7.805603
  3       0.001  0.25  22.61655  0.4452830  18.983694
  3       0.001  0.50  19.28761  0.4805011  15.946761
  3       0.001  1.00  17.57794  0.5527587  13.865939
  3       0.010  0.25  14.85686  0.6842792  11.448049
  3       0.010  0.50  13.18337  0.7463691  10.132894
  3       0.010  1.00  11.87602  0.7879335   9.049288
  3       0.100  0.25  11.47361  0.7917919   8.177333
  3       0.100  0.50  11.70596  0.7868712   8.288289
  3       0.100  1.00  12.89812  0.7535912   8.834333

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 89 of 224 using max all ranger 3 
Random Forest 

409 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 368, 368, 368, 369, 368, 368, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    8.166916  0.8878478  4.522183
   2    extratrees  8.724620  0.8846118  5.601345
  13    variance    8.278831  0.8784976  4.023097
  13    extratrees  7.821583  0.8944400  4.116365
  24    variance    8.494879  0.8722106  4.128558
  24    extratrees  7.703987  0.8961555  3.975688

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 90 of 224 using same all ranger 3 
Random Forest 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 183, 183, 183, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    9.262489  0.8588645  5.615719
   2    extratrees  9.918545  0.8584064  6.983036
  13    variance    9.332052  0.8472450  5.240692
  13    extratrees  8.829712  0.8664691  5.173479
  24    variance    9.589196  0.8398509  5.453363
  24    extratrees  8.782869  0.8664174  5.045608

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 91 of 224 using max du ranger 3 
Random Forest 

409 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 367, 368, 369, 369, 368, 368, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    8.311759  0.8860541  4.741624
   2    extratrees  9.105126  0.8765628  6.003145
  10    variance    8.229002  0.8809713  4.054605
  10    extratrees  8.086552  0.8899793  4.415533
  18    variance    8.461238  0.8732945  4.114466
  18    extratrees  7.954785  0.8918287  4.209466

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 92 of 224 using same du ranger 3 
Random Forest 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 183, 183, 185, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.331805  0.8589613  5.805175
   2    extratrees  10.322069  0.8480952  7.464345
  10    variance     9.177641  0.8534149  5.188247
  10    extratrees   9.068068  0.8610397  5.543802
  18    variance     9.480353  0.8448828  5.379962
  18    extratrees   9.013604  0.8597251  5.332874

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 93 of 224 using max rms ranger 3 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

409 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 369, 367, 369, 367, 368, 368, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    8.651229  0.8701733  4.178753
  2     extratrees  8.310159  0.8881611  4.745713
  3     variance    9.037386  0.8580304  4.102038
  3     extratrees  8.132166  0.8894178  4.326530

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 94 of 224 using same rms ranger 3 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 183, 183, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    9.613338  0.8343687  5.167867
  2     extratrees  9.609438  0.8502977  6.005233
  3     variance    9.653576  0.8299475  5.067007
  3     extratrees  9.298982  0.8532674  5.487673

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 95 of 224 using max hudgins ranger 3 
Random Forest 

409 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 368, 369, 369, 367, 368, 367, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    8.875839  0.8753116  5.356069
   2    extratrees  9.853430  0.8607162  6.823019
   7    variance    8.049964  0.8887351  4.193950
   7    extratrees  8.607531  0.8807106  5.060680
  12    variance    7.973742  0.8881040  4.006922
  12    extratrees  8.372807  0.8844773  4.670736

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 96 of 224 using same hudgins ranger 3 
Random Forest 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 185, 184, 184, 183, 185, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance    10.173192  0.8436474  6.614012
   2    extratrees  11.185487  0.8293073  8.335508
   7    variance     9.593790  0.8477744  5.538980
   7    extratrees   9.721974  0.8516845  6.316077
  12    variance     9.992467  0.8343378  5.670415
  12    extratrees   9.483370  0.8546717  5.860850

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 97 of 224 using max all lm 4 
Linear Regression 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 182, 184, 184, 182, 183, ... 
Resampling results:

  RMSE     Rsquared  MAE     
  11.6872  0.788152  8.997579

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 98 of 224 using same all lm 4 
Linear Regression 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 182, 184, 185, 183, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  11.03143  0.808708  8.303136

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 99 of 224 using max du lm 4 
Linear Regression 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 182, 183, 184, 185, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.54539  0.7898084  9.099336

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 100 of 224 using same du lm 4 
Linear Regression 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 183, 184, 183, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.18232  0.8071252  8.715346

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 101 of 224 using max rms lm 4 
Linear Regression 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 183, 185, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  13.03464  0.7427346  10.12585

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 102 of 224 using same rms lm 4 
Linear Regression 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 184, 183, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.29396  0.7681859  9.409472

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 103 of 224 using max hudgins lm 4 
Linear Regression 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 184, 182, 184, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  12.0312  0.7717632  9.557201

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 104 of 224 using same hudgins lm 4 
Linear Regression 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 183, 184, 184, 182, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  11.66954  0.788251  9.063013

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 105 of 224 using max all knn 4 
k-Nearest Neighbors 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 185, 183, 184, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.66565  0.8012682  6.024534
  7  11.43455  0.7773872  6.880869
  9  11.79576  0.7690901  7.474094

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 106 of 224 using same all knn 4 
k-Nearest Neighbors 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 183, 185, 184, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.62069  0.7993963  5.758158
  7  11.05921  0.7856306  6.453713
  9  11.35972  0.7807215  7.069541

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 107 of 224 using max du knn 4 
k-Nearest Neighbors 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 183, 185, 183, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.63714  0.8030035  5.981321
  7  11.30855  0.7816325  6.805708
  9  11.67008  0.7718717  7.446148

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 108 of 224 using same du knn 4 
k-Nearest Neighbors 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 183, 184, 186, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.78559  0.7973283  5.803941
  7  11.22714  0.7833173  6.486197
  9  11.45557  0.7799655  7.062149

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 109 of 224 using max rms knn 4 
k-Nearest Neighbors 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 185, 184, 183, 184, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.499347  0.8359279  4.836736
  7  9.258787  0.8459672  5.131196
  9  9.573776  0.8390685  5.610481

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 110 of 224 using same rms knn 4 
k-Nearest Neighbors 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 182, 184, 183, 185, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.452851  0.8379133  4.810095
  7  9.251810  0.8455174  5.013333
  9  9.233091  0.8472109  5.288102

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 111 of 224 using max hudgins knn 4 
k-Nearest Neighbors 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 185, 182, 184, 183, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  15.58588  0.6107427  10.42444
  7  14.96023  0.6388236  10.42177
  9  15.36461  0.6235103  11.06626

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 112 of 224 using same hudgins knn 4 
k-Nearest Neighbors 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 183, 184, 184, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE      
  5  14.37998  0.6641483   9.692737
  7  14.41036  0.6619647  10.012059
  9  14.77704  0.6499834  10.483097

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 113 of 224 using max all svmPoly 4 
Support Vector Machines with Polynomial Kernel 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 184, 184, 183, 183, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.84100  0.4724914  18.891478
  1       0.001  0.50  19.33843  0.5053436  15.969516
  1       0.001  1.00  17.28572  0.5904712  13.782531
  1       0.010  0.25  15.22710  0.6628392  12.056477
  1       0.010  0.50  14.27067  0.6920325  11.215506
  1       0.010  1.00  13.69446  0.7114021  10.749740
  1       0.100  0.25  12.87357  0.7439728  10.158827
  1       0.100  0.50  12.31141  0.7651213   9.666037
  1       0.100  1.00  11.90969  0.7790720   9.282999
  2       0.001  0.25  19.33898  0.5049877  15.963703
  2       0.001  0.50  17.27203  0.5920311  13.766248
  2       0.001  1.00  15.53412  0.6556365  12.310318
  2       0.010  0.25  13.20289  0.7423936  10.245304
  2       0.010  0.50  12.08323  0.7783407   9.269680
  2       0.010  1.00  11.28243  0.8010651   8.428980
  2       0.100  0.25  10.83248  0.8095246   7.657972
  2       0.100  0.50  10.84102  0.8095705   7.625351
  2       0.100  1.00  11.03109  0.8048236   7.706831
  3       0.001  0.25  18.14051  0.5535644  14.559463
  3       0.001  0.50  16.18083  0.6386721  12.834385
  3       0.001  1.00  14.72077  0.6810860  11.612424
  3       0.010  0.25  11.82885  0.7874762   8.993076
  3       0.010  0.50  11.07894  0.8061837   8.143125
  3       0.010  1.00  10.86641  0.8093631   7.757522
  3       0.100  0.25  12.42414  0.7624972   8.139834
  3       0.100  0.50  13.80170  0.7227185   8.804658
  3       0.100  1.00  15.39669  0.6793680   9.680226

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 114 of 224 using same all svmPoly 4 
Support Vector Machines with Polynomial Kernel 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 183, 183, 183, 184, 182, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE       Rsquared   MAE      
  1       0.001  0.25  22.826420  0.4907263  18.879712
  1       0.001  0.50  19.163609  0.5281868  15.787645
  1       0.001  1.00  16.999684  0.6134221  13.476725
  1       0.010  0.25  14.588321  0.6893755  11.359073
  1       0.010  0.50  13.663454  0.7176568  10.508370
  1       0.010  1.00  13.144457  0.7367830  10.136677
  1       0.100  0.25  12.468672  0.7623350   9.713191
  1       0.100  0.50  11.932514  0.7825067   9.308045
  1       0.100  1.00  11.457771  0.7968998   8.875303
  2       0.001  0.25  19.168716  0.5277872  15.784844
  2       0.001  0.50  16.979791  0.6154579  13.455854
  2       0.001  1.00  14.958420  0.6815089  11.725102
  2       0.010  0.25  12.624589  0.7686840   9.674257
  2       0.010  0.50  11.489152  0.8041915   8.799214
  2       0.010  1.00  10.556124  0.8292144   7.863345
  2       0.100  0.25   9.964731  0.8416755   7.151067
  2       0.100  0.50   9.961900  0.8409412   7.130676
  2       0.100  1.00  10.200375  0.8330953   7.268126
  3       0.001  0.25  17.782340  0.5784534  14.258888
  3       0.001  0.50  15.584718  0.6660547  12.274013
  3       0.001  1.00  14.187930  0.7057481  11.003528
  3       0.010  0.25  11.091830  0.8188381   8.384552
  3       0.010  0.50  10.460909  0.8318569   7.666862
  3       0.010  1.00  10.176812  0.8367414   7.334710
  3       0.100  0.25  11.387626  0.8005651   7.527561
  3       0.100  0.50  12.696376  0.7646321   8.313226
  3       0.100  1.00  14.770621  0.7085238   9.452256

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 115 of 224 using max du svmPoly 4 
Support Vector Machines with Polynomial Kernel 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 184, 183, 182, 185, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  24.56593  0.4374301  20.030988
  1       0.001  0.50  21.23259  0.4529313  17.633591
  1       0.001  1.00  18.72356  0.5166788  15.025806
  1       0.010  0.25  16.46581  0.6084246  12.883781
  1       0.010  0.50  15.37292  0.6415875  11.949337
  1       0.010  1.00  14.76107  0.6651141  11.422566
  1       0.100  0.25  14.01593  0.6953008  10.868710
  1       0.100  0.50  13.41713  0.7204569  10.416135
  1       0.100  1.00  12.98867  0.7388759  10.171504
  2       0.001  0.25  21.22832  0.4519110  17.623064
  2       0.001  0.50  18.72261  0.5170560  15.025488
  2       0.001  1.00  16.97197  0.5929964  13.304190
  2       0.010  0.25  14.77578  0.6746034  11.432416
  2       0.010  0.50  13.62345  0.7203220  10.448563
  2       0.010  1.00  12.53659  0.7608651   9.570775
  2       0.100  0.25  11.16328  0.8020114   7.865407
  2       0.100  0.50  11.26510  0.7984744   7.896148
  2       0.100  1.00  11.30424  0.7968651   7.889180
  3       0.001  0.25  19.47388  0.4834230  15.997850
  3       0.001  0.50  17.64347  0.5648050  13.899109
  3       0.001  1.00  16.08696  0.6224881  12.573667
  3       0.010  0.25  13.35452  0.7360727  10.196089
  3       0.010  0.50  12.19619  0.7744401   9.279396
  3       0.010  1.00  11.39716  0.7975293   8.372811
  3       0.100  0.25  12.40481  0.7578308   8.341001
  3       0.100  0.50  13.93239  0.7080932   9.154646
  3       0.100  1.00  16.25609  0.6429614  10.305493

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 116 of 224 using same du svmPoly 4 
Support Vector Machines with Polynomial Kernel 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 186, 183, 183, 182, 183, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  24.70971  0.4479547  20.110030
  1       0.001  0.50  21.22430  0.4632960  17.621327
  1       0.001  1.00  18.42014  0.5312491  14.851988
  1       0.010  0.25  16.07932  0.6239586  12.489048
  1       0.010  0.50  14.96870  0.6587637  11.455953
  1       0.010  1.00  14.33926  0.6838952  10.890274
  1       0.100  0.25  13.66443  0.7135623  10.446404
  1       0.100  0.50  13.15325  0.7355670  10.143141
  1       0.100  1.00  12.71153  0.7534382   9.880481
  2       0.001  0.25  21.21853  0.4624236  17.610572
  2       0.001  0.50  18.41777  0.5318534  14.851165
  2       0.001  1.00  16.48391  0.6116078  12.861339
  2       0.010  0.25  14.32240  0.6921326  10.913855
  2       0.010  0.50  13.13256  0.7402875   9.942074
  2       0.010  1.00  11.98937  0.7829855   9.147830
  2       0.100  0.25  10.45106  0.8246232   7.624703
  2       0.100  0.50  10.45868  0.8240715   7.601033
  2       0.100  1.00  10.54334  0.8212259   7.622815
  3       0.001  0.25  19.35068  0.4961489  15.852847
  3       0.001  0.50  17.34281  0.5820328  13.618256
  3       0.001  1.00  15.63884  0.6378426  12.094867
  3       0.010  0.25  12.77191  0.7598519   9.661512
  3       0.010  0.50  11.48865  0.8012492   8.725568
  3       0.010  1.00  10.78104  0.8179617   7.971754
  3       0.100  0.25  11.55948  0.7894336   7.620292
  3       0.100  0.50  12.39721  0.7653559   8.126896
  3       0.100  1.00  14.28388  0.7113124   9.193840

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 117 of 224 using max rms svmPoly 4 
Support Vector Machines with Polynomial Kernel 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 184, 183, 184, 184, 182, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  27.05583  0.5215460  22.158080
  1       0.001  0.50  26.38621  0.5233719  21.385000
  1       0.001  1.00  24.53294  0.5451951  20.066725
  1       0.010  0.25  19.79390  0.5744245  16.752480
  1       0.010  0.50  16.76616  0.6393395  13.896862
  1       0.010  1.00  14.57754  0.7144404  12.020637
  1       0.100  0.25  13.19873  0.7350969  10.546399
  1       0.100  0.50  13.02529  0.7374532  10.257985
  1       0.100  1.00  12.96427  0.7378805  10.114182
  2       0.001  0.25  26.38584  0.5233618  21.384551
  2       0.001  0.50  24.53244  0.5451527  20.065694
  2       0.001  1.00  21.14849  0.5695731  17.800096
  2       0.010  0.25  16.76262  0.6414647  13.867589
  2       0.010  0.50  14.52811  0.7179155  11.942198
  2       0.010  1.00  13.24607  0.7398633  10.644054
  2       0.100  0.25  11.80361  0.7815538   9.002012
  2       0.100  0.50  11.50372  0.7891429   8.546970
  2       0.100  1.00  11.35770  0.7916318   8.175157
  3       0.001  0.25  25.45773  0.5339616  20.671020
  3       0.001  0.50  22.71862  0.5576690  18.879968
  3       0.001  1.00  18.73290  0.5798998  15.796944
  3       0.010  0.25  15.37639  0.6949831  12.663849
  3       0.010  0.50  13.48258  0.7382845  10.900225
  3       0.010  1.00  12.86631  0.7478360  10.160550
  3       0.100  0.25  11.04235  0.8042304   7.975062
  3       0.100  0.50  10.72779  0.8121895   7.529505
  3       0.100  1.00  10.41002  0.8214475   7.205985

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 118 of 224 using same rms svmPoly 4 
Support Vector Machines with Polynomial Kernel 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 182, 184, 184, 184, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  27.36331  0.5343900  22.305897
  1       0.001  0.50  26.58728  0.5385906  21.471351
  1       0.001  1.00  24.53702  0.5625844  20.097513
  1       0.010  0.25  19.53046  0.5914831  16.541219
  1       0.010  0.50  16.34658  0.6607913  13.466394
  1       0.010  1.00  13.87070  0.7408627  11.314648
  1       0.100  0.25  12.50610  0.7621078   9.775729
  1       0.100  0.50  12.32924  0.7648447   9.497852
  1       0.100  1.00  12.28233  0.7659739   9.392124
  2       0.001  0.25  26.58704  0.5385845  21.470804
  2       0.001  0.50  24.53619  0.5625404  20.096358
  2       0.001  1.00  20.99415  0.5873562  17.680689
  2       0.010  0.25  16.33226  0.6632602  13.439507
  2       0.010  0.50  13.83106  0.7437079  11.255229
  2       0.010  1.00  12.54640  0.7653125   9.846741
  2       0.100  0.25  11.14714  0.8089542   8.440762
  2       0.100  0.50  10.66636  0.8207024   7.849099
  2       0.100  1.00  10.45608  0.8256921   7.470055
  3       0.001  0.25  25.53300  0.5501150  20.741547
  3       0.001  0.50  22.63330  0.5750269  18.852730
  3       0.001  1.00  18.35490  0.5949026  15.451601
  3       0.010  0.25  14.76344  0.7213544  12.080991
  3       0.010  0.50  12.90444  0.7621065  10.265184
  3       0.010  1.00  12.20778  0.7736596   9.454513
  3       0.100  0.25  10.28280  0.8337498   7.356349
  3       0.100  0.50  10.00369  0.8398892   6.985102
  3       0.100  1.00   9.74375  0.8467887   6.695014

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 119 of 224 using max hudgins svmPoly 4 
Support Vector Machines with Polynomial Kernel 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 183, 183, 185, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.18841  0.4283884  21.214158
  1       0.001  0.50  23.94406  0.4395906  19.656899
  1       0.001  1.00  20.50968  0.4614625  17.005925
  1       0.010  0.25  17.84746  0.5507964  14.066103
  1       0.010  0.50  16.42642  0.6084240  12.808578
  1       0.010  1.00  15.29648  0.6472049  11.797220
  1       0.100  0.25  14.38950  0.6824242  11.152641
  1       0.100  0.50  13.64173  0.7123736  10.668740
  1       0.100  1.00  12.86739  0.7426410  10.140868
  2       0.001  0.25  23.94121  0.4390671  19.653598
  2       0.001  0.50  20.51776  0.4613661  17.008562
  2       0.001  1.00  18.43768  0.5304640  14.687876
  2       0.010  0.25  16.19681  0.6209845  12.622486
  2       0.010  0.50  14.84385  0.6698038  11.445544
  2       0.010  1.00  13.70474  0.7151235  10.569159
  2       0.100  0.25  11.20573  0.8004670   8.108243
  2       0.100  0.50  11.13109  0.8022267   8.013111
  2       0.100  1.00  11.11666  0.8031900   7.967071
  3       0.001  0.25  21.98023  0.4460742  18.217060
  3       0.001  0.50  19.05337  0.5023346  15.514020
  3       0.001  1.00  17.40971  0.5691184  13.652514
  3       0.010  0.25  15.02747  0.6656296  11.603711
  3       0.010  0.50  13.55493  0.7255931  10.366311
  3       0.010  1.00  12.34210  0.7671667   9.431047
  3       0.100  0.25  12.12071  0.7697873   8.212468
  3       0.100  0.50  12.17017  0.7683651   8.213469
  3       0.100  1.00  13.24189  0.7356587   8.878025

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 120 of 224 using same hudgins svmPoly 4 
Support Vector Machines with Polynomial Kernel 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 186, 184, 182, 183, 183, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.46063  0.4309938  21.258590
  1       0.001  0.50  24.05637  0.4465050  19.686525
  1       0.001  1.00  20.37785  0.4687709  16.864288
  1       0.010  0.25  17.40982  0.5695666  13.641627
  1       0.010  0.50  15.85624  0.6209638  12.131335
  1       0.010  1.00  14.86705  0.6577901  11.260941
  1       0.100  0.25  13.86756  0.6993898  10.565253
  1       0.100  0.50  13.17983  0.7291636  10.166243
  1       0.100  1.00  12.51843  0.7555000   9.763203
  2       0.001  0.25  24.05454  0.4460746  19.683934
  2       0.001  0.50  20.38319  0.4687508  16.864640
  2       0.001  1.00  18.03707  0.5460467  14.382994
  2       0.010  0.25  15.63266  0.6326727  11.947070
  2       0.010  0.50  14.41135  0.6814292  10.910178
  2       0.010  1.00  13.11400  0.7339765   9.944145
  2       0.100  0.25  10.41033  0.8284002   7.605916
  2       0.100  0.50  10.21699  0.8328268   7.451311
  2       0.100  1.00  10.17438  0.8332898   7.417317
  3       0.001  0.25  21.96359  0.4519596  18.178782
  3       0.001  0.50  18.77252  0.5146629  15.235239
  3       0.001  1.00  16.94061  0.5858227  13.174794
  3       0.010  0.25  14.55124  0.6796622  11.001792
  3       0.010  0.50  12.95600  0.7445573   9.771535
  3       0.010  1.00  11.68350  0.7916174   8.885677
  3       0.100  0.25  12.35906  0.7673033   8.186689
  3       0.100  0.50  12.65757  0.7599983   8.212683
  3       0.100  1.00  13.62377  0.7359044   8.820970

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 121 of 224 using max all ranger 4 
Random Forest 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 184, 183, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.148157  0.8577788  5.353015
   2    extratrees  10.071297  0.8451062  6.759878
  13    variance     9.370041  0.8441792  4.901462
  13    extratrees   9.198904  0.8516264  5.109695
  24    variance     9.425058  0.8439593  5.008032
  24    extratrees   9.154731  0.8515649  4.962507

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 122 of 224 using same all ranger 4 
Random Forest 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 183, 185, 182, 185, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    9.129709  0.8575269  5.331181
   2    extratrees  9.844674  0.8496446  6.558369
  13    variance    9.375047  0.8416179  4.998158
  13    extratrees  9.072218  0.8527943  4.989367
  24    variance    9.449618  0.8405402  5.164776
  24    extratrees  9.074731  0.8518000  4.864097

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 123 of 224 using max du ranger 4 
Random Forest 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 182, 185, 184, 183, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.532156  0.8525535  5.698000
   2    extratrees  10.571341  0.8360693  7.230847
  10    variance     9.457729  0.8453090  5.049322
  10    extratrees   9.522634  0.8461829  5.515926
  18    variance     9.538180  0.8433644  5.089272
  18    extratrees   9.433741  0.8463261  5.284182

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 124 of 224 using same du ranger 4 
Random Forest 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 183, 184, 185, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.322866  0.8556730  5.525050
   2    extratrees  10.168287  0.8445906  6.925228
  10    variance     9.335673  0.8477785  4.978996
  10    extratrees   9.295760  0.8500931  5.285497
  18    variance     9.380491  0.8470102  5.080981
  18    extratrees   9.305147  0.8481219  5.133814

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = extratrees
 and min.node.size = 5.


Now processing model 125 of 224 using max rms ranger 4 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 186, 183, 185, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    9.361659  0.8456934  4.971469
  2     extratrees  9.700729  0.8482126  5.851561
  3     variance    9.770165  0.8288759  4.810650
  3     extratrees  9.460225  0.8496709  5.421974

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 126 of 224 using same rms ranger 4 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 182, 183, 184, 184, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    8.928603  0.8515470  4.887762
  2     extratrees  9.309797  0.8549631  5.651708
  3     variance    9.208956  0.8361895  4.683286
  3     extratrees  9.040240  0.8564720  5.182249

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 127 of 224 using max hudgins ranger 4 
Random Forest 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 182, 183, 182, 183, 185, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.718134  0.8477031  6.118094
   2    extratrees  11.096632  0.8223377  7.972657
   7    variance     9.050604  0.8537903  5.078507
   7    extratrees   9.732130  0.8436990  6.100345
  12    variance     9.166109  0.8495082  5.001599
  12    extratrees   9.544328  0.8448213  5.699015

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 128 of 224 using same hudgins ranger 4 
Random Forest 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 185, 184, 182, 184, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.555321  0.8527789  5.977619
   2    extratrees  10.746816  0.8322528  7.674617
   7    variance     9.077420  0.8556676  5.077268
   7    extratrees   9.587262  0.8480880  5.979555
  12    variance     9.153014  0.8535249  5.047244
  12    extratrees   9.458632  0.8481011  5.624293

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 129 of 224 using max all lm 5 
Linear Regression 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 183, 184, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE    
  11.14538  0.8003612  8.47418

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 130 of 224 using same all lm 5 
Linear Regression 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 183, 185, 184, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.00783  0.8049138  8.469055

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 131 of 224 using max du lm 5 
Linear Regression 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 184, 183, 184, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE    
  11.16313  0.8018654  8.66962

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 132 of 224 using same du lm 5 
Linear Regression 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 184, 184, 185, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.11186  0.7988765  8.702697

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 133 of 224 using max rms lm 5 
Linear Regression 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 184, 182, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE    
  12.75426  0.7505338  9.81809

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 134 of 224 using same rms lm 5 
Linear Regression 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 184, 184, 183, 184, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  12.4964  0.7545001  9.691823

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 135 of 224 using max hudgins lm 5 
Linear Regression 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 182, 184, 184, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.79545  0.7823176  9.269315

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 136 of 224 using same hudgins lm 5 
Linear Regression 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 183, 182, 184, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.67869  0.7816286  9.211222

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 137 of 224 using max all knn 5 
k-Nearest Neighbors 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 184, 183, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.06844  0.8220515  5.709155
  7  10.59934  0.8053394  6.550534
  9  10.93049  0.7983455  7.073577

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 138 of 224 using same all knn 5 
k-Nearest Neighbors 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 184, 184, 182, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.60386  0.8061324  5.920879
  7  11.01247  0.7939283  6.742738
  9  11.24694  0.7888874  7.305704

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 139 of 224 using max du knn 5 
k-Nearest Neighbors 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 183, 184, 184, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.14254  0.8163433  5.750532
  7  10.72773  0.8011136  6.615812
  9  10.98848  0.7955581  7.088634

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 140 of 224 using same du knn 5 
k-Nearest Neighbors 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 185, 185, 183, 182, 182, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.51570  0.8045652  5.874478
  7  11.02832  0.7906484  6.768979
  9  11.32125  0.7822942  7.330510

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 141 of 224 using max rms knn 5 
k-Nearest Neighbors 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 182, 184, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.078631  0.8479221  4.728973
  7  9.289489  0.8451231  5.074340
  9  9.472040  0.8418343  5.474074

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 142 of 224 using same rms knn 5 
k-Nearest Neighbors 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 184, 183, 184, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  8.397274  0.8667379  4.395271
  7  8.680751  0.8622957  4.862891
  9  8.952943  0.8577476  5.364380

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 143 of 224 using max hudgins knn 5 
k-Nearest Neighbors 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 184, 184, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE      
  5  14.37924  0.6591855   9.690784
  7  14.44810  0.6578753  10.012680
  9  14.64249  0.6517893  10.459981

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 144 of 224 using same hudgins knn 5 
k-Nearest Neighbors 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 185, 182, 183, 185, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  14.83983  0.6370717  10.07187
  7  14.90728  0.6329805  10.51397
  9  14.93800  0.6331431  10.79144

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 145 of 224 using max all svmPoly 5 
Support Vector Machines with Polynomial Kernel 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 184, 183, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.98855  0.4918671  18.901097
  1       0.001  0.50  19.33214  0.5235666  15.944877
  1       0.001  1.00  17.19953  0.6042533  13.730013
  1       0.010  0.25  14.93974  0.6708615  11.763481
  1       0.010  0.50  14.11104  0.6956466  11.051303
  1       0.010  1.00  13.62407  0.7140596  10.679985
  1       0.100  0.25  12.81811  0.7431949  10.119389
  1       0.100  0.50  12.18255  0.7670050   9.636665
  1       0.100  1.00  11.62891  0.7865494   9.028474
  2       0.001  0.25  19.33744  0.5243804  15.940665
  2       0.001  0.50  17.17449  0.6058644  13.707953
  2       0.001  1.00  15.29206  0.6629730  12.115988
  2       0.010  0.25  12.64588  0.7595153   9.844212
  2       0.010  0.50  11.40801  0.7994937   8.701472
  2       0.010  1.00  10.75666  0.8162091   7.901617
  2       0.100  0.25  10.20602  0.8301118   7.094396
  2       0.100  0.50  10.19245  0.8295721   7.105365
  2       0.100  1.00  10.21342  0.8286810   7.144621
  3       0.001  0.25  18.04540  0.5679934  14.490978
  3       0.001  0.50  15.85868  0.6526106  12.591488
  3       0.001  1.00  14.47833  0.6902847  11.374786
  3       0.010  0.25  11.14213  0.8085144   8.357753
  3       0.010  0.50  10.55299  0.8223527   7.583912
  3       0.010  1.00  10.39011  0.8257573   7.354621
  3       0.100  0.25  11.37857  0.7929791   7.620428
  3       0.100  0.50  11.64946  0.7855951   7.944348
  3       0.100  1.00  12.47892  0.7612347   8.575588

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 146 of 224 using same all svmPoly 5 
Support Vector Machines with Polynomial Kernel 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 183, 185, 183, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE       Rsquared   MAE      
  1       0.001  0.25  22.618948  0.4870622  18.701062
  1       0.001  0.50  18.967607  0.5152932  15.661568
  1       0.001  1.00  16.866381  0.5985925  13.503458
  1       0.010  0.25  14.624381  0.6752345  11.554200
  1       0.010  0.50  13.763667  0.7026244  10.773446
  1       0.010  1.00  13.242861  0.7210322  10.341173
  1       0.100  0.25  12.562440  0.7457921   9.875077
  1       0.100  0.50  12.000436  0.7672313   9.481789
  1       0.100  1.00  11.424040  0.7879705   8.917632
  2       0.001  0.25  18.977732  0.5156614  15.664229
  2       0.001  0.50  16.846501  0.6002792  13.483349
  2       0.001  1.00  14.943221  0.6681861  11.860989
  2       0.010  0.25  12.522512  0.7585138   9.762306
  2       0.010  0.50  11.271484  0.7996422   8.671077
  2       0.010  1.00  10.554292  0.8191291   7.891578
  2       0.100  0.25   9.835253  0.8369909   6.995995
  2       0.100  0.50   9.666205  0.8420864   6.852993
  2       0.100  1.00   9.579697  0.8447493   6.767999
  3       0.001  0.25  17.761509  0.5627126  14.270263
  3       0.001  0.50  15.640495  0.6512562  12.455905
  3       0.001  1.00  14.165253  0.6932968  11.140428
  3       0.010  0.25  10.980062  0.8107829   8.344671
  3       0.010  0.50  10.295349  0.8267831   7.545655
  3       0.010  1.00  10.031362  0.8316640   7.230378
  3       0.100  0.25  11.184067  0.7988877   7.586737
  3       0.100  0.50  12.129522  0.7714668   8.200270
  3       0.100  1.00  13.538363  0.7282008   8.960167

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 147 of 224 using max du svmPoly 5 
Support Vector Machines with Polynomial Kernel 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 182, 184, 184, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  24.66599  0.4498324  20.013206
  1       0.001  0.50  21.54211  0.4748366  17.784049
  1       0.001  1.00  18.74413  0.5271720  15.110695
  1       0.010  0.25  16.34708  0.6159558  12.805226
  1       0.010  0.50  15.30312  0.6476139  11.836395
  1       0.010  1.00  14.68891  0.6714825  11.369463
  1       0.100  0.25  14.12021  0.6948667  10.970758
  1       0.100  0.50  13.50509  0.7173906  10.590616
  1       0.100  1.00  12.86005  0.7422127  10.197059
  2       0.001  0.25  21.53481  0.4741583  17.773756
  2       0.001  0.50  18.73703  0.5277822  15.104969
  2       0.001  1.00  16.80407  0.6031117  13.191511
  2       0.010  0.25  14.48023  0.6892951  11.200462
  2       0.010  0.50  13.11719  0.7397264  10.088703
  2       0.010  1.00  11.76196  0.7864058   8.961104
  2       0.100  0.25  10.41114  0.8251667   7.265834
  2       0.100  0.50  10.42215  0.8243982   7.250734
  2       0.100  1.00  10.40733  0.8247649   7.229127
  3       0.001  0.25  19.52563  0.4996820  16.001090
  3       0.001  0.50  17.59272  0.5752833  13.915704
  3       0.001  1.00  15.87821  0.6294282  12.390253
  3       0.010  0.25  12.66407  0.7618873   9.688330
  3       0.010  0.50  11.34267  0.8013388   8.480028
  3       0.010  1.00  10.72999  0.8166820   7.745811
  3       0.100  0.25  11.64289  0.7889487   7.846482
  3       0.100  0.50  12.32701  0.7669736   8.401178
  3       0.100  1.00  13.43148  0.7299224   9.207508

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 148 of 224 using same du svmPoly 5 
Support Vector Machines with Polynomial Kernel 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 185, 184, 184, 182, 182, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE       Rsquared   MAE      
  1       0.001  0.25  24.262321  0.4548246  19.786297
  1       0.001  0.50  21.037815  0.4735695  17.471619
  1       0.001  1.00  18.376817  0.5314676  14.753697
  1       0.010  0.25  16.006366  0.6273877  12.552068
  1       0.010  0.50  14.852354  0.6616901  11.521698
  1       0.010  1.00  14.229577  0.6843181  11.000873
  1       0.100  0.25  13.651679  0.7063856  10.590712
  1       0.100  0.50  13.220212  0.7232662  10.370570
  1       0.100  1.00  12.712093  0.7422780  10.091844
  2       0.001  0.25  21.029697  0.4728373  17.460130
  2       0.001  0.50  18.372963  0.5322516  14.750353
  2       0.001  1.00  16.473188  0.6118261  12.953091
  2       0.010  0.25  14.193973  0.6963655  11.010987
  2       0.010  0.50  12.895951  0.7438758   9.927428
  2       0.010  1.00  11.584455  0.7872419   8.874345
  2       0.100  0.25  10.184334  0.8283149   7.298318
  2       0.100  0.50  10.113510  0.8300123   7.163757
  2       0.100  1.00   9.930663  0.8353652   6.973808
  3       0.001  0.25  19.119370  0.4999559  15.692594
  3       0.001  0.50  17.196743  0.5798501  13.595297
  3       0.001  1.00  15.514006  0.6410820  12.126824
  3       0.010  0.25  12.506515  0.7629678   9.625608
  3       0.010  0.50  11.270261  0.8007014   8.547247
  3       0.010  1.00  10.587416  0.8177757   7.800508
  3       0.100  0.25  11.203310  0.7957489   7.561253
  3       0.100  0.50  12.434002  0.7588995   8.322536
  3       0.100  1.00  14.429646  0.7000000   9.572323

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 149 of 224 using max rms svmPoly 5 
Support Vector Machines with Polynomial Kernel 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 184, 184, 182, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.92944  0.5423870  22.059408
  1       0.001  0.50  26.27987  0.5426158  21.338636
  1       0.001  1.00  24.59439  0.5635592  20.012830
  1       0.010  0.25  19.77263  0.5931679  16.662436
  1       0.010  0.50  16.47333  0.6564191  13.597586
  1       0.010  1.00  14.22121  0.7284862  11.600204
  1       0.100  0.25  12.99389  0.7423276  10.250053
  1       0.100  0.50  12.79378  0.7452901   9.964158
  1       0.100  1.00  12.76115  0.7450503   9.868036
  2       0.001  0.25  26.27935  0.5426452  21.338124
  2       0.001  0.50  24.59413  0.5635860  20.012007
  2       0.001  1.00  21.24423  0.5907527  17.746531
  2       0.010  0.25  16.45062  0.6589666  13.566009
  2       0.010  0.50  14.17792  0.7306308  11.555824
  2       0.010  1.00  13.00371  0.7473870  10.333046
  2       0.100  0.25  11.46316  0.7938170   8.717593
  2       0.100  0.50  11.12008  0.8030819   8.175729
  2       0.100  1.00  11.00113  0.8058492   7.846659
  3       0.001  0.25  25.46573  0.5482536  20.604987
  3       0.001  0.50  22.73434  0.5753968  18.805292
  3       0.001  1.00  18.65429  0.5993187  15.663566
  3       0.010  0.25  15.13261  0.7127885  12.406048
  3       0.010  0.50  13.30938  0.7461508  10.682568
  3       0.010  1.00  12.58513  0.7570900   9.880591
  3       0.100  0.25  10.69460  0.8167746   7.599968
  3       0.100  0.50  10.43194  0.8234821   7.228914
  3       0.100  1.00  10.23552  0.8289231   6.985930

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 150 of 224 using same rms svmPoly 5 
Support Vector Machines with Polynomial Kernel 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 184, 183, 185, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.90422  0.5302375  21.918028
  1       0.001  0.50  26.17365  0.5338336  21.135716
  1       0.001  1.00  24.26235  0.5569829  19.835632
  1       0.010  0.25  19.48387  0.5820515  16.527072
  1       0.010  0.50  16.38952  0.6465535  13.630023
  1       0.010  1.00  14.14597  0.7232198  11.688853
  1       0.100  0.25  12.81148  0.7491514  10.199058
  1       0.100  0.50  12.53700  0.7536772   9.774149
  1       0.100  1.00  12.53375  0.7536175   9.685928
  2       0.001  0.25  26.17341  0.5338749  21.135139
  2       0.001  0.50  24.26223  0.5570126  19.834911
  2       0.001  1.00  20.92600  0.5799519  17.601833
  2       0.010  0.25  16.37969  0.6483942  13.608049
  2       0.010  0.50  14.07787  0.7269131  11.604022
  2       0.010  1.00  12.87530  0.7520804  10.325792
  2       0.100  0.25  11.46059  0.7942496   8.815974
  2       0.100  0.50  11.06185  0.8049615   8.248291
  2       0.100  1.00  10.85980  0.8095318   7.861699
  3       0.001  0.25  25.13580  0.5425804  20.433945
  3       0.001  0.50  22.41362  0.5693323  18.677065
  3       0.001  1.00  18.35661  0.5882989  15.528481
  3       0.010  0.25  15.03746  0.7025981  12.446650
  3       0.010  0.50  13.13293  0.7499834  10.625124
  3       0.010  1.00  12.44106  0.7622898   9.787566
  3       0.100  0.25  10.53862  0.8221789   7.604314
  3       0.100  0.50  10.21078  0.8311316   7.163156
  3       0.100  1.00  10.01065  0.8372836   6.927622

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 151 of 224 using max hudgins svmPoly 5 
Support Vector Machines with Polynomial Kernel 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 183, 184, 184, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.05658  0.4367085  21.193887
  1       0.001  0.50  24.03087  0.4476993  19.641809
  1       0.001  1.00  20.57774  0.4695284  17.001674
  1       0.010  0.25  17.67852  0.5568609  13.976945
  1       0.010  0.50  16.16088  0.6112083  12.526382
  1       0.010  1.00  15.22588  0.6453557  11.698035
  1       0.100  0.25  14.29394  0.6853618  11.051179
  1       0.100  0.50  13.64774  0.7120876  10.676686
  1       0.100  1.00  12.90901  0.7411874  10.232907
  2       0.001  0.25  24.02702  0.4473110  19.638384
  2       0.001  0.50  20.57570  0.4693135  16.995753
  2       0.001  1.00  18.28095  0.5355215  14.599600
  2       0.010  0.25  15.84236  0.6278093  12.267294
  2       0.010  0.50  14.59173  0.6779390  11.186902
  2       0.010  1.00  13.33799  0.7284257  10.232335
  2       0.100  0.25  10.61141  0.8196500   7.518303
  2       0.100  0.50  10.50223  0.8221848   7.372696
  2       0.100  1.00  10.45968  0.8233213   7.284105
  3       0.001  0.25  22.15901  0.4614958  18.291304
  3       0.001  0.50  19.03983  0.5052646  15.494151
  3       0.001  1.00  17.17695  0.5766371  13.499566
  3       0.010  0.25  14.69506  0.6778297  11.263185
  3       0.010  0.50  12.96243  0.7468181   9.891036
  3       0.010  1.00  11.58637  0.7936940   8.757198
  3       0.100  0.25  11.12104  0.8014989   7.564353
  3       0.100  0.50  11.37244  0.7940465   7.636878
  3       0.100  1.00  12.16576  0.7688151   8.228452

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 152 of 224 using same hudgins svmPoly 5 
Support Vector Machines with Polynomial Kernel 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 183, 184, 183, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  25.91648  0.4415549  20.900652
  1       0.001  0.50  23.58571  0.4569813  19.369984
  1       0.001  1.00  20.07453  0.4755908  16.642729
  1       0.010  0.25  17.28658  0.5709179  13.655883
  1       0.010  0.50  15.68902  0.6281763  12.205033
  1       0.010  1.00  14.67132  0.6654552  11.314672
  1       0.100  0.25  13.73679  0.7020554  10.623576
  1       0.100  0.50  13.20510  0.7238650  10.275653
  1       0.100  1.00  12.59040  0.7480807   9.935166
  2       0.001  0.25  23.58286  0.4565992  19.366452
  2       0.001  0.50  20.07309  0.4754653  16.637112
  2       0.001  1.00  17.74826  0.5489140  14.145268
  2       0.010  0.25  15.43965  0.6430844  12.004298
  2       0.010  0.50  14.20258  0.6921038  10.937748
  2       0.010  1.00  12.99137  0.7387875   9.974819
  2       0.100  0.25  10.36077  0.8258730   7.592655
  2       0.100  0.50  10.23178  0.8281533   7.436465
  2       0.100  1.00  10.18767  0.8288198   7.337186
  3       0.001  0.25  21.58528  0.4646529  17.935449
  3       0.001  0.50  18.62521  0.5158265  15.120746
  3       0.001  1.00  16.81257  0.5879809  13.200928
  3       0.010  0.25  14.31959  0.6925512  11.035591
  3       0.010  0.50  12.75997  0.7528493   9.764428
  3       0.010  1.00  11.42542  0.7974482   8.707118
  3       0.100  0.25  10.44719  0.8206692   7.217047
  3       0.100  0.50  10.88528  0.8070855   7.398278
  3       0.100  1.00  12.05014  0.7702586   8.073515

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 153 of 224 using max all ranger 5 
Random Forest 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 184, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    9.219122  0.8557566  5.235827
   2    extratrees  9.835079  0.8536972  6.512520
  13    variance    9.424364  0.8418887  4.910334
  13    extratrees  9.009669  0.8581466  4.948034
  24    variance    9.516915  0.8400151  5.042855
  24    extratrees  9.027376  0.8559395  4.848140

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 154 of 224 using same all ranger 5 
Random Forest 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 182, 184, 184, 184, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    8.474901  0.8772181  5.004246
   2    extratrees  9.260491  0.8699651  6.259076
  13    variance    8.470813  0.8670318  4.685607
  13    extratrees  8.141680  0.8804682  4.579768
  24    variance    8.609779  0.8657112  4.866014
  24    extratrees  8.076639  0.8808853  4.481175

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 155 of 224 using max du ranger 5 
Random Forest 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 183, 184, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.443212  0.8522011  5.507429
   2    extratrees  10.283438  0.8433129  6.965803
  10    variance     9.393120  0.8430634  4.999040
  10    extratrees   9.392340  0.8490354  5.374906
  18    variance     9.519544  0.8397286  5.076526
  18    extratrees   9.367500  0.8476758  5.216244

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 156 of 224 using same du ranger 5 
Random Forest 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 185, 183, 184, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    8.644766  0.8749114  5.218417
   2    extratrees  9.659986  0.8594374  6.598805
  10    variance    8.325604  0.8740049  4.679669
  10    extratrees  8.525070  0.8719375  4.933260
  18    variance    8.465422  0.8717250  4.775449
  18    extratrees  8.394975  0.8739087  4.718719

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 157 of 224 using max rms ranger 5 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 184, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    9.368052  0.8440611  4.926303
  2     extratrees  9.152977  0.8628761  5.523914
  3     variance    9.943071  0.8217043  4.922148
  3     extratrees  8.941561  0.8631411  5.094495

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 158 of 224 using same rms ranger 5 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 183, 183, 185, 182, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    8.832947  0.8565600  4.782786
  2     extratrees  8.567610  0.8785917  5.303467
  3     variance    9.468683  0.8325466  4.777320
  3     extratrees  8.230784  0.8819480  4.822284

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 159 of 224 using max hudgins ranger 5 
Random Forest 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 182, 184, 183, 184, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.620465  0.8528640  5.920537
   2    extratrees  10.751840  0.8327327  7.619499
   7    variance     9.107955  0.8580555  4.968721
   7    extratrees   9.601626  0.8483165  5.885064
  12    variance     9.153457  0.8560306  4.840656
  12    extratrees   9.478723  0.8491454  5.519638

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 160 of 224 using same hudgins ranger 5 
Random Forest 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 185, 183, 182, 185, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.155621  0.8648407  5.781258
   2    extratrees  10.378977  0.8423395  7.359481
   7    variance     8.259916  0.8773041  4.776020
   7    extratrees   9.052131  0.8623303  5.654951
  12    variance     8.393464  0.8714758  4.805825
  12    extratrees   8.744825  0.8672339  5.226155

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 161 of 224 using max all lm 6 
Linear Regression 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 183, 184, 182, 184, 184, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  11.3503  0.7951609  8.740169

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 162 of 224 using same all lm 6 
Linear Regression 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 184, 184, 184, 184, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.19032  0.7983939  8.751483

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 163 of 224 using max du lm 6 
Linear Regression 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 185, 184, 182, 185, 185, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.47715  0.7903086  9.013233

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 164 of 224 using same du lm 6 
Linear Regression 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 185, 184, 183, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  11.1369  0.8013329  8.783621

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 165 of 224 using max rms lm 6 
Linear Regression 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 182, 184, 184, 184, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.91399  0.7388369  9.948338

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 166 of 224 using same rms lm 6 
Linear Regression 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 185, 182, 183, 184, 183, ... 
Resampling results:

  RMSE    Rsquared   MAE     
  12.419  0.7556332  9.533906

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 167 of 224 using max hudgins lm 6 
Linear Regression 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 184, 184, 184, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.98334  0.7734399  9.484266

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 168 of 224 using same hudgins lm 6 
Linear Regression 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 185, 183, 184, 184, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.59411  0.7816101  9.195386

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 169 of 224 using max all knn 6 
k-Nearest Neighbors 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 183, 184, 184, 184, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.30576  0.8092954  5.737208
  7  10.91711  0.7946003  6.645438
  9  11.32066  0.7817133  7.161108

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 170 of 224 using same all knn 6 
k-Nearest Neighbors 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 185, 185, 183, 183, 184, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.971357  0.8178843  5.361622
  7  10.181218  0.8139869  5.953772
  9  10.753756  0.7960967  6.701379

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 171 of 224 using max du knn 6 
k-Nearest Neighbors 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 185, 183, 184, 184, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.24120  0.8094020  5.674701
  7  10.74311  0.7969813  6.528751
  9  11.28991  0.7823308  7.087615

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 172 of 224 using same du knn 6 
k-Nearest Neighbors 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 185, 183, 183, 185, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.17262  0.8127727  5.418112
  7  10.36633  0.8094838  6.031013
  9  10.95996  0.7920819  6.776295

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 173 of 224 using max rms knn 6 
k-Nearest Neighbors 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 185, 185, 183, 183, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.230128  0.8431797  4.859882
  7  9.402213  0.8381626  5.210010
  9  9.513615  0.8386177  5.614842

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 174 of 224 using same rms knn 6 
k-Nearest Neighbors 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 185, 183, 183, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  8.829061  0.8512741  4.429432
  7  8.855556  0.8498530  4.716767
  9  8.699123  0.8585941  5.042391

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 175 of 224 using max hudgins knn 6 
k-Nearest Neighbors 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 182, 184, 183, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE      
  5  14.25827  0.6674244   9.180753
  7  14.18329  0.6717478   9.460845
  9  14.45862  0.6597749  10.038074

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 176 of 224 using same hudgins knn 6 
k-Nearest Neighbors 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 184, 184, 184, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE      
  5  14.58731  0.6523501   9.354027
  7  14.09335  0.6709065   9.493314
  9  14.51586  0.6503317  10.215502

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 177 of 224 using max all svmPoly 6 
Support Vector Machines with Polynomial Kernel 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 185, 184, 185, 185, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  23.06971  0.4591687  18.928563
  1       0.001  0.50  19.57353  0.4813483  15.903815
  1       0.001  1.00  17.33642  0.5750868  13.721060
  1       0.010  0.25  14.97905  0.6670727  11.797140
  1       0.010  0.50  14.02441  0.7002514  10.956098
  1       0.010  1.00  13.56153  0.7157919  10.597486
  1       0.100  0.25  12.94216  0.7381679  10.132807
  1       0.100  0.50  12.48275  0.7552238   9.810023
  1       0.100  1.00  11.99107  0.7724774   9.345398
  2       0.001  0.25  19.57693  0.4819594  15.892943
  2       0.001  0.50  17.31912  0.5776737  13.696509
  2       0.001  1.00  15.41810  0.6544842  12.197824
  2       0.010  0.25  12.93349  0.7490704   9.989623
  2       0.010  0.50  11.79556  0.7847545   8.954608
  2       0.010  1.00  11.10595  0.8029269   8.214776
  2       0.100  0.25  10.54848  0.8179654   7.426668
  2       0.100  0.50  10.42339  0.8218123   7.280289
  2       0.100  1.00  10.40779  0.8223696   7.241247
  3       0.001  0.25  18.26980  0.5367112  14.513769
  3       0.001  0.50  16.21298  0.6274559  12.808756
  3       0.001  1.00  14.54020  0.6865284  11.400264
  3       0.010  0.25  11.63771  0.7909300   8.754592
  3       0.010  0.50  10.96246  0.8068272   8.036849
  3       0.010  1.00  10.66542  0.8141017   7.675556
  3       0.100  0.25  11.43941  0.7903942   7.731696
  3       0.100  0.50  12.33242  0.7651885   8.312011
  3       0.100  1.00  13.31218  0.7417023   8.962331

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 178 of 224 using same all svmPoly 6 
Support Vector Machines with Polynomial Kernel 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 185, 184, 183, 183, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE       Rsquared   MAE      
  1       0.001  0.25  22.923268  0.4739961  18.666831
  1       0.001  0.50  19.439106  0.4976822  15.802503
  1       0.001  1.00  17.048025  0.5883652  13.518197
  1       0.010  0.25  14.572720  0.6850698  11.517842
  1       0.010  0.50  13.577427  0.7172955  10.680693
  1       0.010  1.00  13.157788  0.7297688  10.332176
  1       0.100  0.25  12.542893  0.7509745   9.856665
  1       0.100  0.50  12.087486  0.7675122   9.545405
  1       0.100  1.00  11.607878  0.7849345   9.107588
  2       0.001  0.25  19.434474  0.4976856  15.783976
  2       0.001  0.50  17.027968  0.5903524  13.495101
  2       0.001  1.00  15.084303  0.6702935  11.949996
  2       0.010  0.25  12.511962  0.7632104   9.747711
  2       0.010  0.50  11.377806  0.7971905   8.696919
  2       0.010  1.00  10.584771  0.8168842   7.828704
  2       0.100  0.25  10.086129  0.8304390   7.042455
  2       0.100  0.50  10.038213  0.8325825   6.940763
  2       0.100  1.00   9.980479  0.8350720   6.875960
  3       0.001  0.25  18.035160  0.5483268  14.329002
  3       0.001  0.50  15.915295  0.6411617  12.590409
  3       0.001  1.00  14.106924  0.7031192  11.101303
  3       0.010  0.25  11.156353  0.8045132   8.451088
  3       0.010  0.50  10.406279  0.8222272   7.592099
  3       0.010  1.00  10.206118  0.8268534   7.297548
  3       0.100  0.25  10.880432  0.8066417   7.229202
  3       0.100  0.50  11.391222  0.7911574   7.678570
  3       0.100  1.00  12.389180  0.7626606   8.424779

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 179 of 224 using max du svmPoly 6 
Support Vector Machines with Polynomial Kernel 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 184, 184, 183, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  24.68501  0.4242839  19.973158
  1       0.001  0.50  21.55204  0.4432438  17.743231
  1       0.001  1.00  18.82474  0.5009371  14.893923
  1       0.010  0.25  16.37452  0.6021673  12.798997
  1       0.010  0.50  15.12414  0.6490933  11.675526
  1       0.010  1.00  14.42836  0.6733410  11.108039
  1       0.100  0.25  13.74478  0.6998509  10.612182
  1       0.100  0.50  13.44493  0.7116320  10.461146
  1       0.100  1.00  13.00309  0.7292321  10.218064
  2       0.001  0.25  21.54304  0.4423546  17.727685
  2       0.001  0.50  18.82403  0.5018986  14.889258
  2       0.001  1.00  16.92773  0.5847862  13.215493
  2       0.010  0.25  14.41100  0.6876893  11.059811
  2       0.010  0.50  13.17761  0.7321474  10.048599
  2       0.010  1.00  11.96355  0.7754186   9.037139
  2       0.100  0.25  10.66077  0.8123825   7.594168
  2       0.100  0.50  10.59464  0.8145427   7.512832
  2       0.100  1.00  10.60925  0.8147992   7.497782
  3       0.001  0.25  19.68143  0.4676334  15.871301
  3       0.001  0.50  17.55184  0.5560591  13.727042
  3       0.001  1.00  15.84013  0.6247050  12.338420
  3       0.010  0.25  12.81712  0.7509503   9.735721
  3       0.010  0.50  11.80533  0.7827279   8.906680
  3       0.010  1.00  11.03151  0.8032448   8.166282
  3       0.100  0.25  11.33755  0.7938198   7.771605
  3       0.100  0.50  11.99061  0.7739196   8.179602
  3       0.100  1.00  12.96687  0.7462960   8.855041

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 180 of 224 using same du svmPoly 6 
Support Vector Machines with Polynomial Kernel 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 186, 183, 183, 184, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  24.55634  0.4371555  19.688248
  1       0.001  0.50  21.42410  0.4561876  17.580250
  1       0.001  1.00  18.60738  0.5090099  14.742674
  1       0.010  0.25  16.08477  0.6145770  12.513394
  1       0.010  0.50  14.80775  0.6628488  11.446604
  1       0.010  1.00  14.09023  0.6897965  10.893059
  1       0.100  0.25  13.50653  0.7130265  10.449485
  1       0.100  0.50  13.19630  0.7263648  10.286859
  1       0.100  1.00  12.83441  0.7408528  10.118050
  2       0.001  0.25  21.41592  0.4552399  17.567238
  2       0.001  0.50  18.60758  0.5096724  14.737835
  2       0.001  1.00  16.70736  0.5931541  13.009752
  2       0.010  0.25  14.20124  0.6971202  10.961532
  2       0.010  0.50  12.88922  0.7457341   9.886020
  2       0.010  1.00  11.68507  0.7874578   8.853686
  2       0.100  0.25  10.28208  0.8251938   7.151397
  2       0.100  0.50  10.19345  0.8279499   7.033658
  2       0.100  1.00  10.21601  0.8282013   7.004796
  3       0.001  0.25  19.49649  0.4779402  15.747794
  3       0.001  0.50  17.31801  0.5646483  13.543746
  3       0.001  1.00  15.62129  0.6352568  12.135877
  3       0.010  0.25  12.58000  0.7618328   9.597548
  3       0.010  0.50  11.48620  0.7951238   8.673605
  3       0.010  1.00  10.63796  0.8168709   7.800638
  3       0.100  0.25  10.89419  0.8064750   7.328398
  3       0.100  0.50  11.27669  0.7935436   7.631515
  3       0.100  1.00  12.08269  0.7714053   8.236548

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 181 of 224 using max rms svmPoly 6 
Support Vector Machines with Polynomial Kernel 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 183, 184, 183, 185, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  27.55412  0.4860965  21.972684
  1       0.001  0.50  26.58974  0.4974104  21.243575
  1       0.001  1.00  24.66260  0.5254262  20.036390
  1       0.010  0.25  20.29562  0.5529125  16.912672
  1       0.010  0.50  16.95990  0.6198059  13.941264
  1       0.010  1.00  14.69529  0.7064193  12.059535
  1       0.100  0.25  13.14115  0.7388204  10.385234
  1       0.100  0.50  12.89515  0.7405271  10.027723
  1       0.100  1.00  12.85062  0.7410873   9.928012
  2       0.001  0.25  26.58970  0.4974062  21.242976
  2       0.001  0.50  24.66269  0.5254578  20.035522
  2       0.001  1.00  21.46461  0.5456693  17.881791
  2       0.010  0.25  16.95848  0.6205498  13.913519
  2       0.010  0.50  14.65775  0.7087370  12.005631
  2       0.010  1.00  13.24390  0.7411307  10.557214
  2       0.100  0.25  11.81359  0.7816869   9.012833
  2       0.100  0.50  11.38882  0.7927976   8.469924
  2       0.100  1.00  11.28152  0.7933488   8.182095
  3       0.001  0.25  25.59298  0.5102525  20.611761
  3       0.001  0.50  22.97259  0.5379977  18.934296
  3       0.001  1.00  19.13255  0.5611013  15.996710
  3       0.010  0.25  15.66826  0.6778947  12.815708
  3       0.010  0.50  13.54942  0.7372602  10.924345
  3       0.010  1.00  12.75133  0.7512674   9.993937
  3       0.100  0.25  11.05183  0.8038223   7.990425
  3       0.100  0.50  10.69746  0.8129395   7.543275
  3       0.100  1.00  10.46681  0.8198130   7.274014

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 182 of 224 using same rms svmPoly 6 
Support Vector Machines with Polynomial Kernel 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 182, 183, 184, 184, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE       Rsquared   MAE      
  1       0.001  0.25  27.247259  0.4993818  21.611243
  1       0.001  0.50  26.313909  0.5106516  20.894352
  1       0.001  1.00  24.433788  0.5401419  19.714220
  1       0.010  0.25  20.176186  0.5731863  16.755940
  1       0.010  0.50  16.710297  0.6358311  13.770742
  1       0.010  1.00  14.287004  0.7230125  11.774151
  1       0.100  0.25  12.540089  0.7566367   9.940525
  1       0.100  0.50  12.354582  0.7566408   9.640352
  1       0.100  1.00  12.360373  0.7562101   9.547474
  2       0.001  0.25  26.313826  0.5106636  20.893800
  2       0.001  0.50  24.434385  0.5401828  19.713772
  2       0.001  1.00  21.360484  0.5652100  17.679300
  2       0.010  0.25  16.705843  0.6369971  13.749186
  2       0.010  0.50  14.250478  0.7250694  11.730733
  2       0.010  1.00  12.736244  0.7567490  10.204386
  2       0.100  0.25  11.390476  0.7924097   8.724694
  2       0.100  0.50  10.935981  0.8039734   8.166198
  2       0.100  1.00  10.739826  0.8081752   7.770110
  3       0.001  0.25  25.399317  0.5277425  20.293957
  3       0.001  0.50  22.770839  0.5538370  18.660242
  3       0.001  1.00  19.005449  0.5802325  15.885628
  3       0.010  0.25  15.324094  0.6940035  12.588564
  3       0.010  0.50  13.029626  0.7548211  10.563430
  3       0.010  1.00  12.247203  0.7658079   9.638061
  3       0.100  0.25  10.492771  0.8184144   7.565667
  3       0.100  0.50  10.168366  0.8262149   7.158856
  3       0.100  1.00   9.941042  0.8328081   6.932121

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 183 of 224 using max hudgins svmPoly 6 
Support Vector Machines with Polynomial Kernel 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 184, 184, 184, 185, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.26925  0.4047750  20.964029
  1       0.001  0.50  24.01883  0.4280942  19.563386
  1       0.001  1.00  20.59749  0.4450100  16.935628
  1       0.010  0.25  17.67776  0.5434216  13.818692
  1       0.010  0.50  16.02628  0.6139963  12.433806
  1       0.010  1.00  15.07065  0.6543532  11.614226
  1       0.100  0.25  14.09504  0.6910036  10.866499
  1       0.100  0.50  13.51021  0.7146344  10.502831
  1       0.100  1.00  12.99100  0.7344881  10.180802
  2       0.001  0.25  24.01416  0.4275477  19.558474
  2       0.001  0.50  20.59228  0.4445910  16.926431
  2       0.001  1.00  18.25614  0.5201610  14.367353
  2       0.010  0.25  15.75647  0.6302329  12.195475
  2       0.010  0.50  14.48650  0.6838160  11.115664
  2       0.010  1.00  13.24671  0.7307233  10.106376
  2       0.100  0.25  10.81741  0.8126770   7.827914
  2       0.100  0.50  10.64822  0.8165234   7.579293
  2       0.100  1.00  10.66936  0.8156717   7.528610
  3       0.001  0.25  22.01090  0.4315481  18.160254
  3       0.001  0.50  19.10224  0.4876628  15.259624
  3       0.001  1.00  17.20798  0.5664097  13.417178
  3       0.010  0.25  14.52478  0.6859912  11.107788
  3       0.010  0.50  13.16060  0.7367926  10.017885
  3       0.010  1.00  11.94101  0.7787663   9.025054
  3       0.100  0.25  11.09860  0.8021391   7.710469
  3       0.100  0.50  11.21456  0.7985852   7.744834
  3       0.100  1.00  11.90884  0.7778842   8.244311

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 184 of 224 using same hudgins svmPoly 6 
Support Vector Machines with Polynomial Kernel 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 184, 183, 183, 185, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.06722  0.4236872  20.622312
  1       0.001  0.50  23.91458  0.4463756  19.289293
  1       0.001  1.00  20.48612  0.4659037  16.833194
  1       0.010  0.25  17.43863  0.5582646  13.650468
  1       0.010  0.50  15.79594  0.6271231  12.264327
  1       0.010  1.00  14.67902  0.6707380  11.353318
  1       0.100  0.25  13.70019  0.7072324  10.659536
  1       0.100  0.50  13.13656  0.7288135  10.301382
  1       0.100  1.00  12.64836  0.7475903  10.008396
  2       0.001  0.25  23.91152  0.4459469  19.286024
  2       0.001  0.50  20.48101  0.4653309  16.824387
  2       0.001  1.00  17.98647  0.5354977  14.172796
  2       0.010  0.25  15.57733  0.6402549  12.072388
  2       0.010  0.50  14.23134  0.6949154  10.978588
  2       0.010  1.00  12.94062  0.7408567   9.976543
  2       0.100  0.25  10.36746  0.8240687   7.433340
  2       0.100  0.50  10.22054  0.8276341   7.197689
  2       0.100  1.00  10.16491  0.8297110   7.095716
  3       0.001  0.25  21.97815  0.4534477  17.997324
  3       0.001  0.50  18.87689  0.5024791  15.102300
  3       0.001  1.00  16.97439  0.5796882  13.224781
  3       0.010  0.25  14.28823  0.6970106  10.956295
  3       0.010  0.50  12.79458  0.7499053   9.773696
  3       0.010  1.00  11.45646  0.7944803   8.681314
  3       0.100  0.25  10.67916  0.8152772   7.401681
  3       0.100  0.50  10.80861  0.8101392   7.418120
  3       0.100  1.00  11.34846  0.7936276   7.816552

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 185 of 224 using max all ranger 6 
Random Forest 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 182, 185, 185, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    9.055920  0.8614340  5.309157
   2    extratrees  9.761539  0.8568280  6.540323
  13    variance    9.096039  0.8506988  4.924170
  13    extratrees  8.821757  0.8634815  4.993777
  24    variance    9.222637  0.8484888  5.069029
  24    extratrees  8.781055  0.8629830  4.897117

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 186 of 224 using same all ranger 6 
Random Forest 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 185, 184, 183, 183, 182, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    8.785346  0.8662863  5.072495
   2    extratrees  9.370776  0.8650214  6.277482
  13    variance    9.033794  0.8493529  4.730140
  13    extratrees  8.495926  0.8684188  4.648489
  24    variance    9.182742  0.8457773  4.875494
  24    extratrees  8.362832  0.8715850  4.508637

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 187 of 224 using max du ranger 6 
Random Forest 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 182, 185, 184, 184, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.228695  0.8585741  5.596612
   2    extratrees  10.193031  0.8446237  7.009768
  10    variance     9.093480  0.8504873  5.046192
  10    extratrees   9.127075  0.8564552  5.386961
  18    variance     9.213370  0.8484344  5.130349
  18    extratrees   9.022071  0.8576095  5.194667

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 188 of 224 using same du ranger 6 
Random Forest 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 183, 183, 183, 184, 182, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    8.923351  0.8632976  5.268302
   2    extratrees  9.728673  0.8552403  6.651691
  10    variance    8.923570  0.8531242  4.755503
  10    extratrees  8.768257  0.8623827  4.942479
  18    variance    9.065598  0.8485623  4.818800
  18    extratrees  8.703519  0.8625164  4.761599

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 189 of 224 using max rms ranger 6 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 185, 183, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    9.296168  0.8400040  5.114285
  2     extratrees  9.221561  0.8578386  5.701029
  3     variance    9.597251  0.8235953  5.078032
  3     extratrees  8.984672  0.8581588  5.250429

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 190 of 224 using same rms ranger 6 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 184, 184, 184, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    9.199217  0.8448315  4.884793
  2     extratrees  8.956639  0.8637540  5.425212
  3     variance    9.488141  0.8318310  4.833049
  3     extratrees  8.711231  0.8648009  4.947632

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 191 of 224 using max hudgins ranger 6 
Random Forest 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 182, 183, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.922476  0.8442300  6.191128
   2    extratrees  10.945335  0.8260756  7.788050
   7    variance     9.410208  0.8471069  5.252257
   7    extratrees   9.781633  0.8434296  6.141074
  12    variance     9.502757  0.8434595  5.215238
  12    extratrees   9.618913  0.8444432  5.786837

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 192 of 224 using same hudgins ranger 6 
Random Forest 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 185, 183, 183, 184, 185, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.349603  0.8585999  5.757979
   2    extratrees  10.479903  0.8402624  7.419727
   7    variance     8.764623  0.8611056  4.726315
   7    extratrees   9.170819  0.8582958  5.583172
  12    variance     9.000671  0.8516151  4.751481
  12    extratrees   8.972690  0.8603391  5.214665

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 193 of 224 using max all lm 7 
Linear Regression 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 186, 184, 183, 182, ... 
Resampling results:

  RMSE     Rsquared  MAE     
  11.3381  0.801819  8.842173

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 194 of 224 using same all lm 7 
Linear Regression 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 185, 182, 184, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.32922  0.7940942  8.746152

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 195 of 224 using max du lm 7 
Linear Regression 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 183, 184, 185, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.22316  0.8034231  8.749936

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 196 of 224 using same du lm 7 
Linear Regression 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 184, 184, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.33528  0.7935537  8.713058

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 197 of 224 using max rms lm 7 
Linear Regression 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 184, 183, 183, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.81864  0.7449776  9.875421

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 198 of 224 using same rms lm 7 
Linear Regression 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 183, 184, 183, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.72225  0.7425138  9.717372

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 199 of 224 using max hudgins lm 7 
Linear Regression 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 184, 182, 183, 185, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.18559  0.7697609  9.499042

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 200 of 224 using same hudgins lm 7 
Linear Regression 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 183, 182, 184, 184, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.17733  0.7609661  9.370472

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 201 of 224 using max all knn 7 
k-Nearest Neighbors 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 186, 184, 184, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.164571  0.8493383  5.126221
  7  9.633831  0.8388265  5.763584
  9  9.982994  0.8314941  6.217810

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 202 of 224 using same all knn 7 
k-Nearest Neighbors 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 185, 182, 184, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.665206  0.8279905  5.313745
  7  10.164511  0.8148163  5.986861
  9  10.662547  0.8015738  6.679656

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 203 of 224 using max du knn 7 
k-Nearest Neighbors 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 183, 185, 184, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.025634  0.8527778  4.996524
  7  9.409512  0.8447008  5.613402
  9  9.859398  0.8334301  6.151373

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 204 of 224 using same du knn 7 
k-Nearest Neighbors 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 183, 183, 183, 184, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.363180  0.8387019  5.176096
  7   9.920894  0.8221223  5.901477
  9  10.527770  0.8063548  6.654510

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 205 of 224 using max rms knn 7 
k-Nearest Neighbors 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 182, 184, 185, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  7.615319  0.8897653  4.021137
  7  7.741028  0.8895392  4.415607
  9  8.057204  0.8823017  4.814289

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 206 of 224 using same rms knn 7 
k-Nearest Neighbors 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 182, 185, 184, 184, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.135489  0.8477117  4.702888
  7  9.018631  0.8535723  4.953449
  9  9.302272  0.8466147  5.380854

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 207 of 224 using max hudgins knn 7 
k-Nearest Neighbors 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 182, 186, 182, 184, 183, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  14.63789  0.6561726  9.304416
  7  14.40796  0.6654204  9.558529
  9  14.25097  0.6715083  9.827635

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 208 of 224 using same hudgins knn 7 
k-Nearest Neighbors 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 184, 185, 183, 183, 184, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE      
  5  14.33622  0.6555648   9.090719
  7  14.27996  0.6594689   9.685146
  9  14.29764  0.6592408  10.085840

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 209 of 224 using max all svmPoly 7 
Support Vector Machines with Polynomial Kernel 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 184, 183, 183, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE       Rsquared   MAE      
  1       0.001  0.25  23.455568  0.4851471  19.521101
  1       0.001  0.50  19.825912  0.5114681  16.614180
  1       0.001  1.00  17.470174  0.5934453  14.210142
  1       0.010  0.25  15.101405  0.6728394  12.148110
  1       0.010  0.50  14.229494  0.6942238  11.321901
  1       0.010  1.00  13.787585  0.7081708  10.910233
  1       0.100  0.25  13.247786  0.7280130  10.471646
  1       0.100  0.50  12.720353  0.7473350  10.022555
  1       0.100  1.00  12.128069  0.7677049   9.400101
  2       0.001  0.25  19.809319  0.5115360  16.592301
  2       0.001  0.50  17.450545  0.5948207  14.189132
  2       0.001  1.00  15.492829  0.6667089  12.523948
  2       0.010  0.25  12.882638  0.7574416  10.152737
  2       0.010  0.50  11.614470  0.7932769   8.805415
  2       0.010  1.00  10.941133  0.8091251   7.962435
  2       0.100  0.25  10.277148  0.8283999   7.225995
  2       0.100  0.50  10.036227  0.8357167   7.101933
  2       0.100  1.00   9.851355  0.8417245   6.979422
  3       0.001  0.25  18.388460  0.5581126  15.007857
  3       0.001  0.50  16.251659  0.6464255  13.152872
  3       0.001  1.00  14.647256  0.6891841  11.745898
  3       0.010  0.25  11.436761  0.8002000   8.587658
  3       0.010  0.50  10.926119  0.8097851   7.867283
  3       0.010  1.00  10.666223  0.8152276   7.485396
  3       0.100  0.25  12.046390  0.7818591   8.092193
  3       0.100  0.50  13.209773  0.7533827   8.728288
  3       0.100  1.00  15.232460  0.7068224   9.889363

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 210 of 224 using same all svmPoly 7 
Support Vector Machines with Polynomial Kernel 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 182, 185, 183, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  23.77727  0.4460158  19.476253
  1       0.001  0.50  20.46099  0.4704173  16.909771
  1       0.001  1.00  17.69574  0.5604920  14.239374
  1       0.010  0.25  14.99954  0.6759649  11.991203
  1       0.010  0.50  14.10002  0.6985721  11.169448
  1       0.010  1.00  13.57563  0.7147905  10.583067
  1       0.100  0.25  13.09620  0.7326971  10.045553
  1       0.100  0.50  12.64866  0.7497044   9.581968
  1       0.100  1.00  12.22327  0.7641761   9.146675
  2       0.001  0.25  20.44403  0.4702256  16.884317
  2       0.001  0.50  17.67344  0.5632101  14.213118
  2       0.001  1.00  15.54158  0.6601869  12.460518
  2       0.010  0.25  12.85400  0.7533603  10.056366
  2       0.010  0.50  11.68684  0.7847518   8.736080
  2       0.010  1.00  11.13753  0.7989013   8.061791
  2       0.100  0.25  10.61296  0.8140310   7.400121
  2       0.100  0.50  10.43519  0.8208860   7.291230
  2       0.100  1.00  10.21062  0.8289273   7.146275
  3       0.001  0.25  18.73981  0.5181415  15.158450
  3       0.001  0.50  16.40698  0.6272082  13.161660
  3       0.001  1.00  14.51799  0.6944066  11.590921
  3       0.010  0.25  11.54610  0.7901363   8.634163
  3       0.010  0.50  11.00911  0.8014962   7.916964
  3       0.010  1.00  10.76836  0.8073013   7.506499
  3       0.100  0.25  12.18864  0.7695450   8.142436
  3       0.100  0.50  13.46464  0.7393727   8.823121
  3       0.100  1.00  15.20891  0.7025392   9.891970

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 211 of 224 using max du svmPoly 7 
Support Vector Machines with Polynomial Kernel 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 183, 184, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  25.22300  0.4555829  20.607595
  1       0.001  0.50  22.01083  0.4737199  18.437073
  1       0.001  1.00  19.09806  0.5202439  15.548005
  1       0.010  0.25  16.60140  0.6168868  13.259489
  1       0.010  0.50  15.38201  0.6492161  12.150390
  1       0.010  1.00  14.78944  0.6665392  11.639079
  1       0.100  0.25  14.24614  0.6885669  11.171163
  1       0.100  0.50  13.86445  0.7051266  10.893417
  1       0.100  1.00  13.47138  0.7203386  10.635655
  2       0.001  0.25  22.00126  0.4735231  18.424718
  2       0.001  0.50  19.09133  0.5211752  15.539468
  2       0.001  1.00  17.15373  0.5992415  13.735351
  2       0.010  0.25  14.53935  0.6944808  11.444555
  2       0.010  0.50  13.21987  0.7409850  10.310305
  2       0.010  1.00  11.95312  0.7816893   9.054576
  2       0.100  0.25  10.74693  0.8163853   7.524109
  2       0.100  0.50  10.62848  0.8207156   7.479726
  2       0.100  1.00  10.36424  0.8295079   7.341889
  3       0.001  0.25  19.93241  0.4927087  16.573340
  3       0.001  0.50  17.82533  0.5668892  14.304110
  3       0.001  1.00  16.06278  0.6361736  12.784250
  3       0.010  0.25  12.72585  0.7640355   9.818290
  3       0.010  0.50  11.71039  0.7904597   8.769256
  3       0.010  1.00  11.34373  0.7976623   8.215008
  3       0.100  0.25  12.14495  0.7779628   8.292024
  3       0.100  0.50  13.62000  0.7386915   8.982127
  3       0.100  1.00  16.50011  0.6747495  10.332058

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 212 of 224 using same du svmPoly 7 
Support Vector Machines with Polynomial Kernel 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 182, 184, 185, 183, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  25.31085  0.4046422  20.412678
  1       0.001  0.50  22.47757  0.4270715  18.553462
  1       0.001  1.00  19.42949  0.4722048  15.685387
  1       0.010  0.25  16.68505  0.5981502  13.207261
  1       0.010  0.50  15.26488  0.6519013  12.020074
  1       0.010  1.00  14.57866  0.6725050  11.382271
  1       0.100  0.25  13.89471  0.6991334  10.687719
  1       0.100  0.50  13.67820  0.7089363  10.461022
  1       0.100  1.00  13.35354  0.7242688  10.170319
  2       0.001  0.25  22.46644  0.4265707  18.540594
  2       0.001  0.50  19.42497  0.4734436  15.673469
  2       0.001  1.00  17.35174  0.5703794  13.758439
  2       0.010  0.25  14.56039  0.6912403  11.420082
  2       0.010  0.50  13.15066  0.7386717  10.146079
  2       0.010  1.00  12.05400  0.7739728   8.972786
  2       0.100  0.25  11.01325  0.8064287   7.663924
  2       0.100  0.50  10.90826  0.8103105   7.561928
  2       0.100  1.00  10.72177  0.8179489   7.458349
  3       0.001  0.25  20.48152  0.4415584  16.838330
  3       0.001  0.50  18.03911  0.5345927  14.336762
  3       0.001  1.00  16.15196  0.6229363  12.747298
  3       0.010  0.25  12.83695  0.7572681   9.825239
  3       0.010  0.50  11.93205  0.7793347   8.890027
  3       0.010  1.00  11.44989  0.7915605   8.304634
  3       0.100  0.25  11.96451  0.7783577   8.209241
  3       0.100  0.50  13.39058  0.7411707   8.931761
  3       0.100  1.00  15.33099  0.6949722   9.924451

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 213 of 224 using max rms svmPoly 7 
Support Vector Machines with Polynomial Kernel 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 185, 183, 183, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  27.19161  0.5338370  22.437369
  1       0.001  0.50  26.62947  0.5344400  21.790687
  1       0.001  1.00  24.94292  0.5518322  20.541360
  1       0.010  0.25  20.55636  0.5913311  17.500927
  1       0.010  0.50  16.88389  0.6543641  14.213910
  1       0.010  1.00  14.61157  0.7287042  12.120024
  1       0.100  0.25  13.10584  0.7423513  10.368062
  1       0.100  0.50  12.85370  0.7452730   9.954657
  1       0.100  1.00  12.82733  0.7461031   9.883859
  2       0.001  0.25  26.62921  0.5345055  21.790102
  2       0.001  0.50  24.94301  0.5519514  20.540433
  2       0.001  1.00  21.73860  0.5836484  18.460125
  2       0.010  0.25  16.86316  0.6567902  14.185530
  2       0.010  0.50  14.55095  0.7320199  12.057272
  2       0.010  1.00  13.14879  0.7482009  10.542523
  2       0.100  0.25  11.64648  0.7896570   8.746140
  2       0.100  0.50  11.30828  0.7992738   8.273359
  2       0.100  1.00  11.14262  0.8030199   7.838693
  3       0.001  0.25  25.87886  0.5430390  21.150287
  3       0.001  0.50  23.20033  0.5713221  19.476751
  3       0.001  1.00  19.37650  0.5961471  16.542510
  3       0.010  0.25  15.40292  0.7145279  12.836045
  3       0.010  0.50  13.41590  0.7500729  10.915130
  3       0.010  1.00  12.68345  0.7555800   9.935722
  3       0.100  0.25  10.99468  0.8086509   7.823393
  3       0.100  0.50  10.85589  0.8120000   7.548959
  3       0.100  1.00  10.60268  0.8190416   7.290781

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 214 of 224 using same rms svmPoly 7 
Support Vector Machines with Polynomial Kernel 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 184, 183, 183, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  27.06447  0.4919469  22.118824
  1       0.001  0.50  26.58206  0.4927515  21.494022
  1       0.001  1.00  25.07176  0.5157914  20.348878
  1       0.010  0.25  21.05416  0.5563379  17.597192
  1       0.010  0.50  17.25443  0.6207069  14.390040
  1       0.010  1.00  14.71721  0.7125147  12.135595
  1       0.100  0.25  12.91295  0.7416050  10.176621
  1       0.100  0.50  12.65638  0.7429773   9.794481
  1       0.100  1.00  12.60940  0.7439249   9.694317
  2       0.001  0.25  26.58186  0.4927962  21.493389
  2       0.001  0.50  25.07136  0.5158778  20.347775
  2       0.001  1.00  22.16435  0.5447136  18.440143
  2       0.010  0.25  17.24487  0.6223424  14.366493
  2       0.010  0.50  14.65118  0.7152953  12.071475
  2       0.010  1.00  13.03673  0.7442563  10.383813
  2       0.100  0.25  11.60903  0.7813854   8.688584
  2       0.100  0.50  11.25597  0.7916666   8.189406
  2       0.100  1.00  11.02559  0.7977178   7.833461
  3       0.001  0.25  25.89824  0.5056733  20.899728
  3       0.001  0.50  23.41827  0.5304354  19.371193
  3       0.001  1.00  19.92624  0.5623463  16.766269
  3       0.010  0.25  15.67645  0.6821218  12.940548
  3       0.010  0.50  13.36448  0.7450182  10.792215
  3       0.010  1.00  12.54637  0.7529498   9.779093
  3       0.100  0.25  10.91948  0.8017376   7.775863
  3       0.100  0.50  10.74349  0.8054727   7.512398
  3       0.100  1.00  10.59409  0.8098420   7.353242

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 215 of 224 using max hudgins svmPoly 7 
Support Vector Machines with Polynomial Kernel 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 183, 185, 183, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.43429  0.4408185  21.528759
  1       0.001  0.50  24.41498  0.4566192  20.087915
  1       0.001  1.00  20.92770  0.4789238  17.540851
  1       0.010  0.25  17.76252  0.5552311  14.188800
  1       0.010  0.50  16.14806  0.6269588  12.830690
  1       0.010  1.00  15.12068  0.6591714  11.933715
  1       0.100  0.25  14.33758  0.6868280  11.297590
  1       0.100  0.50  13.81326  0.7096763  10.955144
  1       0.100  1.00  13.29067  0.7301994  10.623070
  2       0.001  0.25  24.41049  0.4563553  20.083473
  2       0.001  0.50  20.92320  0.4786963  17.532825
  2       0.001  1.00  18.45500  0.5329998  14.840994
  2       0.010  0.25  15.88929  0.6422568  12.581299
  2       0.010  0.50  14.42601  0.6918048  11.352171
  2       0.010  1.00  13.31944  0.7343200  10.443881
  2       0.100  0.25  10.73048  0.8150080   7.756167
  2       0.100  0.50  10.61073  0.8186487   7.584363
  2       0.100  1.00  10.56064  0.8214538   7.558783
  3       0.001  0.25  22.40494  0.4649120  18.731403
  3       0.001  0.50  19.21954  0.5054412  15.773790
  3       0.001  1.00  17.38054  0.5792479  13.870265
  3       0.010  0.25  14.53225  0.6976106  11.445616
  3       0.010  0.50  12.93052  0.7544560  10.064798
  3       0.010  1.00  11.68401  0.7913988   8.937770
  3       0.100  0.25  11.35436  0.7977960   8.011621
  3       0.100  0.50  11.62524  0.7922469   8.232611
  3       0.100  1.00  12.67626  0.7640733   8.825299

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 216 of 224 using same hudgins svmPoly 7 
Support Vector Machines with Polynomial Kernel 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 182, 184, 183, 184, 184, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  26.49150  0.3855524  21.262972
  1       0.001  0.50  24.63904  0.4051399  20.000469
  1       0.001  1.00  21.57743  0.4289228  17.854798
  1       0.010  0.25  18.20562  0.5214601  14.416877
  1       0.010  0.50  16.32456  0.6102045  12.857805
  1       0.010  1.00  15.16094  0.6560371  11.930362
  1       0.100  0.25  14.09884  0.6922496  11.066298
  1       0.100  0.50  13.55392  0.7126072  10.597835
  1       0.100  1.00  13.07816  0.7300138  10.176983
  2       0.001  0.25  24.63505  0.4047877  19.996020
  2       0.001  0.50  21.57102  0.4285144  17.846147
  2       0.001  1.00  18.93720  0.4920887  15.117225
  2       0.010  0.25  16.10041  0.6261436  12.657713
  2       0.010  0.50  14.62122  0.6853695  11.490256
  2       0.010  1.00  13.21468  0.7342362  10.301420
  2       0.100  0.25  10.92492  0.8028355   7.830400
  2       0.100  0.50  10.80393  0.8058976   7.591403
  2       0.100  1.00  10.67159  0.8108773   7.471843
  3       0.001  0.25  22.93601  0.4166298  18.898590
  3       0.001  0.50  19.75989  0.4555115  16.042981
  3       0.001  1.00  17.70082  0.5505984  13.989365
  3       0.010  0.25  14.69275  0.6930969  11.486727
  3       0.010  0.50  13.06350  0.7460660  10.143300
  3       0.010  1.00  11.81561  0.7801715   8.968429
  3       0.100  0.25  11.02447  0.7994754   7.875893
  3       0.100  0.50  11.39420  0.7899268   8.119715
  3       0.100  1.00  12.36287  0.7636147   8.671282

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 217 of 224 using max all ranger 7 
Random Forest 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 185, 183, 183, 182, 185, 185, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    8.014998  0.8997405  5.065419
   2    extratrees  8.853845  0.8908922  6.172015
  13    variance    8.033571  0.8859010  4.644872
  13    extratrees  7.812584  0.8968465  4.603505
  24    variance    8.219362  0.8812367  4.801809
  24    extratrees  7.734930  0.8973213  4.510447

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 218 of 224 using same all ranger 7 
Random Forest 

204 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 184, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    8.831802  0.8741492  5.294362
   2    extratrees  9.612630  0.8646238  6.474938
  13    variance    8.774866  0.8628295  4.818474
  13    extratrees  8.782570  0.8693227  4.934507
  24    variance    8.779664  0.8627836  4.864943
  24    extratrees  8.671499  0.8713993  4.812949

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 219 of 224 using max du ranger 7 
Random Forest 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 182, 183, 184, 183, 185, 185, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    8.362290  0.8918804  5.312010
   2    extratrees  9.338547  0.8785357  6.586412
  10    variance    8.119338  0.8871925  4.715249
  10    extratrees  8.196645  0.8863181  4.891339
  18    variance    8.119019  0.8863648  4.703552
  18    extratrees  8.096462  0.8869771  4.703341

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 220 of 224 using same du ranger 7 
Random Forest 

204 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 184, 184, 183, 183, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    9.028804  0.8670387  5.550092
   2    extratrees  9.864282  0.8566811  6.840263
  10    variance    8.872406  0.8598907  4.959982
  10    extratrees  8.899316  0.8620932  5.182071
  18    variance    8.877615  0.8598487  4.967534
  18    extratrees  8.783139  0.8634325  4.991789

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 221 of 224 using max rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 183, 184, 184, 183, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    7.723484  0.8947707  4.582522
  2     extratrees  8.209740  0.8942094  5.319518
  3     variance    8.007633  0.8832764  4.525549
  3     extratrees  7.858332  0.8967713  4.839773

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 222 of 224 using same rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

204 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 184, 183, 183, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    8.935030  0.8570379  4.990892
  2     extratrees  9.302291  0.8617816  5.725732
  3     variance    9.252460  0.8408411  4.972027
  3     extratrees  9.003818  0.8637967  5.222169

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 223 of 224 using max hudgins ranger 7 
Random Forest 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 184, 185, 183, 182, 184, 183, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     8.792147  0.8850803  5.793780
   2    extratrees  10.006634  0.8649773  7.299460
   7    variance     8.286536  0.8855253  4.924258
   7    extratrees   8.609660  0.8814611  5.517281
  12    variance     8.359134  0.8825600  4.867855
  12    extratrees   8.463464  0.8813786  5.182802

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 224 of 224 using same hudgins ranger 7 
Random Forest 

204 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 183, 183, 185, 185, 184, 184, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.338974  0.8632275  5.961639
   2    extratrees  10.508277  0.8470813  7.642345
   7    variance     8.965959  0.8613117  5.122288
   7    extratrees   9.213998  0.8605594  5.722680
  12    variance     8.988790  0.8590082  5.030285
  12    extratrees   9.047834  0.8618137  5.386796

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.
