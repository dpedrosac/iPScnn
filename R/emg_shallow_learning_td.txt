Processing  6 subjects



Now processing model 1 of 224 using max all lm 1 
Linear Regression 

1210 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1088, 1088, 1090, 1089, 1089, 1089, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  13.88713  0.6063513  10.59839

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 2 of 224 using same all lm 1 
Linear Regression 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 156, 156, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  14.25681  0.5880844  11.42966

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 3 of 224 using max du lm 1 
Linear Regression 

1210 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1089, 1089, 1089, 1089, 1089, 1088, ... 
Resampling results:

  RMSE      Rsquared   MAE    
  13.76371  0.6097686  10.7584

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 4 of 224 using same du lm 1 
Linear Regression 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 155, 156, 156, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  14.14554  0.6047297  11.36276

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 5 of 224 using max rms lm 1 
Linear Regression 

1210 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1089, 1088, 1089, 1088, 1089, 1089, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  15.05428  0.5294144  11.62678

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 6 of 224 using same rms lm 1 
Linear Regression 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 156, 155, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  14.60321  0.5552544  11.37598

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 7 of 224 using max hudgins lm 1 
Linear Regression 

1210 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1089, 1089, 1088, 1089, 1090, 1090, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  14.56764  0.5634518  11.30674

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 8 of 224 using same hudgins lm 1 
Linear Regression 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 156, 155, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  14.24865  0.5832956  11.31684

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 9 of 224 using max all knn 1 
k-Nearest Neighbors 

1210 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1089, 1089, 1089, 1090, 1089, 1088, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  12.14913  0.6946225  7.206291
  7  12.07820  0.6973429  7.341063
  9  12.10446  0.6961227  7.513553

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 10 of 224 using same all knn 1 
k-Nearest Neighbors 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 155, 155, 157, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE      
  5  13.76031  0.6147784   9.569591
  7  13.52594  0.6272349   9.814813
  9  13.83450  0.6097359  10.201033

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 11 of 224 using max du knn 1 
k-Nearest Neighbors 

1210 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1089, 1089, 1089, 1090, 1089, 1089, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  12.13780  0.6952889  7.171423
  7  12.08035  0.6974041  7.344965
  9  12.09083  0.6968805  7.514796

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 12 of 224 using same du knn 1 
k-Nearest Neighbors 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 156, 156, 155, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE      
  5  13.71107  0.6150068   9.496449
  7  13.43025  0.6312360   9.701062
  9  13.90930  0.6047868  10.212436

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 13 of 224 using max rms knn 1 
k-Nearest Neighbors 

1210 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1090, 1089, 1089, 1089, 1089, 1088, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  11.94358  0.7054360  6.808632
  7  11.64320  0.7185908  6.841040
  9  11.59557  0.7205845  6.987850

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 14 of 224 using same rms knn 1 
k-Nearest Neighbors 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 154, 156, 156, 157, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  13.62770  0.6150640  9.224699
  7  13.01359  0.6454605  9.096026
  9  13.03864  0.6483237  9.343285

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 15 of 224 using max hudgins knn 1 
k-Nearest Neighbors 

1210 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1089, 1089, 1089, 1090, 1088, 1089, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  15.13347  0.5337087  10.09386
  7  14.96676  0.5402986  10.18374
  9  14.88716  0.5435203  10.18255

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 16 of 224 using same hudgins knn 1 
k-Nearest Neighbors 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 157, 155, 155, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  15.29581  0.5193282  10.94389
  7  15.17966  0.5263770  11.10144
  9  15.18927  0.5249544  11.24799

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 17 of 224 using max all svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1210 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1088, 1090, 1089, 1089, 1089, 1088, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  16.05638  0.4878910  12.597570
  1       0.001  0.50  15.69206  0.5017386  12.073440
  1       0.001  1.00  15.44295  0.5123058  11.831468
  1       0.010  0.25  15.09760  0.5316233  11.515982
  1       0.010  0.50  14.77356  0.5500403  11.266496
  1       0.010  1.00  14.40259  0.5711756  11.000620
  1       0.100  0.25  13.97378  0.5958334  10.664540
  1       0.100  0.50  13.78055  0.6070407  10.444405
  1       0.100  1.00  13.78496  0.6075683  10.344741
  2       0.001  0.25  15.63115  0.5060584  12.016166
  2       0.001  0.50  15.32305  0.5199436  11.725133
  2       0.001  1.00  14.99377  0.5383946  11.418006
  2       0.010  0.25  13.93653  0.5981290  10.305852
  2       0.010  0.50  13.76922  0.6074331  10.028874
  2       0.010  1.00  13.65206  0.6140227   9.809467
  2       0.100  0.25  13.83032  0.6127091   9.361702
  2       0.100  0.50  13.63104  0.6265791   9.259488
  2       0.100  1.00  13.72992  0.6380593   9.185924
  3       0.001  0.25  15.36513  0.5192995  11.769989
  3       0.001  0.50  15.01357  0.5378445  11.436489
  3       0.001  1.00  14.61105  0.5606736  11.088749
  3       0.010  0.25  14.07468  0.5922936   9.922751
  3       0.010  0.50  13.96931  0.5997996   9.654688
  3       0.010  1.00  14.18052  0.5979943   9.475948
  3       0.100  0.25  44.31261  0.5807933  12.067916
  3       0.100  0.50  52.83997  0.5713041  12.993888
  3       0.100  1.00  59.61002  0.5460862  13.927126

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 18 of 224 using same all svmPoly 1 
Support Vector Machines with Polynomial Kernel 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 155, 157, 155, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE     
  1       0.001  0.25  19.50841  0.4442119  16.62234
  1       0.001  0.50  17.61414  0.4749252  14.85852
  1       0.001  1.00  16.20386  0.4955704  13.09288
  1       0.010  0.25  15.46253  0.5171701  11.90605
  1       0.010  0.50  15.11997  0.5311541  11.50386
  1       0.010  1.00  15.00531  0.5389821  11.37226
  1       0.100  0.25  14.74011  0.5550652  11.21650
  1       0.100  0.50  14.49404  0.5687536  11.09601
  1       0.100  1.00  14.20374  0.5853638  10.93093
  2       0.001  0.25  17.61125  0.4748828  14.85436
  2       0.001  0.50  16.19390  0.4959098  13.07654
  2       0.001  1.00  15.56171  0.5131763  12.05467
  2       0.010  0.25  14.93663  0.5455566  11.31438
  2       0.010  0.50  14.47468  0.5694863  10.88291
  2       0.010  1.00  14.18053  0.5860395  10.69178
  2       0.100  0.25  14.07958  0.5960747  10.61116
  2       0.100  0.50  14.27067  0.5899958  10.79427
  2       0.100  1.00  14.79528  0.5692693  11.26886
  3       0.001  0.25  16.67333  0.4858816  13.73862
  3       0.001  0.50  15.74847  0.5093600  12.36970
  3       0.001  1.00  15.33118  0.5233107  11.75232
  3       0.010  0.25  14.46746  0.5701974  10.88219
  3       0.010  0.50  14.17283  0.5856544  10.65150
  3       0.010  1.00  14.00826  0.5947066  10.57063
  3       0.100  0.25  15.95796  0.5284850  11.89526
  3       0.100  0.50  16.95404  0.5060101  12.71315
  3       0.100  1.00  18.93794  0.4631665  14.07137

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 19 of 224 using max du svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1210 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1089, 1090, 1089, 1089, 1089, 1089, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  16.55766  0.4632803  13.296580
  1       0.001  0.50  16.12960  0.4775514  12.519789
  1       0.001  1.00  15.89575  0.4874387  12.165810
  1       0.010  0.25  15.60583  0.5026374  11.891784
  1       0.010  0.50  15.33665  0.5172512  11.675033
  1       0.010  1.00  14.99191  0.5366969  11.424282
  1       0.100  0.25  14.54238  0.5625458  11.129245
  1       0.100  0.50  14.19147  0.5826007  10.911312
  1       0.100  1.00  13.94155  0.5970008  10.744398
  2       0.001  0.25  16.09400  0.4801381  12.489543
  2       0.001  0.50  15.82210  0.4923511  12.100760
  2       0.001  1.00  15.53831  0.5071438  11.829208
  2       0.010  0.25  14.46450  0.5676311  10.753501
  2       0.010  0.50  14.13491  0.5858078  10.400452
  2       0.010  1.00  13.89943  0.5991745  10.117755
  2       0.100  0.25  13.87551  0.6047652   9.600906
  2       0.100  0.50  13.99131  0.6033890   9.572476
  2       0.100  1.00  14.45960  0.6020897   9.601157
  3       0.001  0.25  15.86956  0.4913557  12.177112
  3       0.001  0.50  15.55656  0.5067244  11.847314
  3       0.001  1.00  15.23272  0.5240691  11.573047
  3       0.010  0.25  14.30532  0.5769049  10.304035
  3       0.010  0.50  14.08990  0.5896219   9.997520
  3       0.010  1.00  14.04900  0.5939379   9.774682
  3       0.100  0.25  40.40839  0.5479616  12.062398
  3       0.100  0.50  50.58742  0.5481704  13.106892
  3       0.100  1.00  68.31304  0.5398720  14.919377

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 20 of 224 using same du svmPoly 1 
Support Vector Machines with Polynomial Kernel 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 155, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE     
  1       0.001  0.25  20.40130  0.4155685  17.34566
  1       0.001  0.50  18.54926  0.4482261  15.81403
  1       0.001  1.00  17.07045  0.4724994  14.20374
  1       0.010  0.25  15.91186  0.4975075  12.53484
  1       0.010  0.50  15.65827  0.5058148  11.97795
  1       0.010  1.00  15.42688  0.5157948  11.65582
  1       0.100  0.25  15.22987  0.5255687  11.54544
  1       0.100  0.50  15.03520  0.5362705  11.41457
  1       0.100  1.00  14.80047  0.5484016  11.29788
  2       0.001  0.25  18.54754  0.4480545  15.81310
  2       0.001  0.50  17.06867  0.4725663  14.20088
  2       0.001  1.00  16.03741  0.4941207  12.74848
  2       0.010  0.25  15.45921  0.5161477  11.79710
  2       0.010  0.50  15.11357  0.5327832  11.40037
  2       0.010  1.00  14.72560  0.5512123  11.01466
  2       0.100  0.25  14.61113  0.5613406  11.13459
  2       0.100  0.50  14.73080  0.5616857  11.17618
  2       0.100  1.00  15.17947  0.5454835  11.50974
  3       0.001  0.25  17.67169  0.4658925  14.89199
  3       0.001  0.50  16.34733  0.4866132  13.28592
  3       0.001  1.00  15.81795  0.5001547  12.29389
  3       0.010  0.25  15.08512  0.5342825  11.38326
  3       0.010  0.50  14.70477  0.5525907  11.00376
  3       0.010  1.00  14.43249  0.5669047  10.83997
  3       0.100  0.25  15.68089  0.5238125  11.72006
  3       0.100  0.50  16.48226  0.4957335  12.33611
  3       0.100  1.00  18.38117  0.4390941  13.78190

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 21 of 224 using max rms svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1210 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1089, 1089, 1089, 1089, 1088, 1090, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  19.56911  0.4405034  16.744413
  1       0.001  0.50  17.36744  0.4700476  14.660198
  1       0.001  1.00  16.12578  0.4974424  12.870139
  1       0.010  0.25  15.46791  0.5191915  11.977301
  1       0.010  0.50  15.20857  0.5271340  11.697033
  1       0.010  1.00  15.11427  0.5299273  11.626522
  1       0.100  0.25  15.06424  0.5317398  11.593102
  1       0.100  0.50  15.06208  0.5317770  11.594975
  1       0.100  1.00  15.05897  0.5319301  11.596856
  2       0.001  0.25  17.36684  0.4700761  14.659353
  2       0.001  0.50  16.12429  0.4975588  12.869160
  2       0.001  1.00  15.56636  0.5158647  12.097742
  2       0.010  0.25  15.12015  0.5329163  11.630453
  2       0.010  0.50  14.92991  0.5415497  11.482758
  2       0.010  1.00  14.72823  0.5526535  11.321112
  2       0.100  0.25  14.43929  0.5790667  10.439018
  2       0.100  0.50  14.52073  0.5784199  10.373027
  2       0.100  1.00  14.62672  0.5764059  10.365696
  3       0.001  0.25  16.52605  0.4867634  13.474973
  3       0.001  0.50  15.75037  0.5096587  12.338751
  3       0.001  1.00  15.37106  0.5227160  11.873730
  3       0.010  0.25  14.88957  0.5456384  11.444364
  3       0.010  0.50  14.63282  0.5590832  11.229235
  3       0.010  1.00  14.43216  0.5697252  11.022638
  3       0.100  0.25  13.78825  0.6071704   9.777686
  3       0.100  0.50  13.58086  0.6177049   9.538818
  3       0.100  1.00  13.46281  0.6241639   9.359817

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 22 of 224 using same rms svmPoly 1 
Support Vector Machines with Polynomial Kernel 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 155, 156, 156, 157, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE     
  1       0.001  0.25  22.72033  0.4089073  18.91966
  1       0.001  0.50  21.85913  0.4089331  18.42899
  1       0.001  1.00  20.67454  0.4383580  17.57137
  1       0.010  0.25  18.07489  0.4790634  15.32940
  1       0.010  0.50  16.27274  0.5033274  13.10557
  1       0.010  1.00  15.52435  0.5230604  12.01039
  1       0.100  0.25  15.01188  0.5424574  11.51062
  1       0.100  0.50  14.81966  0.5490103  11.37506
  1       0.100  1.00  14.72149  0.5521384  11.32581
  2       0.001  0.25  21.85808  0.4088075  18.42889
  2       0.001  0.50  20.67414  0.4383002  17.57125
  2       0.001  1.00  18.82248  0.4673110  16.00746
  2       0.010  0.25  16.26756  0.5033707  13.09380
  2       0.010  0.50  15.51360  0.5235368  12.00679
  2       0.010  1.00  15.06866  0.5408351  11.54892
  2       0.100  0.25  14.51651  0.5685328  11.08381
  2       0.100  0.50  14.21332  0.5856555  10.80541
  2       0.100  1.00  14.01673  0.5960021  10.50549
  3       0.001  0.25  21.18799  0.4227244  17.98223
  3       0.001  0.50  19.69692  0.4556798  16.76490
  3       0.001  1.00  17.53756  0.4852563  14.72792
  3       0.010  0.25  15.76490  0.5147211  12.33674
  3       0.010  0.50  15.20620  0.5350495  11.66015
  3       0.010  1.00  14.89616  0.5479655  11.41501
  3       0.100  0.25  14.20737  0.5848352  10.65969
  3       0.100  0.50  14.04852  0.5941415  10.42355
  3       0.100  1.00  13.94879  0.5990350  10.23980

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 23 of 224 using max hudgins svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1210 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1089, 1089, 1089, 1089, 1090, 1090, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  17.23466  0.4477965  14.465317
  1       0.001  0.50  16.44276  0.4658985  13.072054
  1       0.001  1.00  16.07293  0.4795001  12.401538
  1       0.010  0.25  15.75282  0.4957656  11.986753
  1       0.010  0.50  15.46094  0.5113386  11.759171
  1       0.010  1.00  15.15034  0.5289464  11.527081
  1       0.100  0.25  14.82937  0.5473330  11.328457
  1       0.100  0.50  14.67800  0.5557303  11.251510
  1       0.100  1.00  14.59557  0.5605473  11.206043
  2       0.001  0.25  16.43083  0.4668504  13.057849
  2       0.001  0.50  16.04771  0.4812518  12.377343
  2       0.001  1.00  15.79253  0.4944207  12.019658
  2       0.010  0.25  14.85553  0.5471489  11.180250
  2       0.010  0.50  14.41836  0.5715427  10.759100
  2       0.010  1.00  14.09369  0.5892068  10.423918
  2       0.100  0.25  13.94023  0.6007229   9.768196
  2       0.100  0.50  13.90708  0.6043562   9.690855
  2       0.100  1.00  13.85028  0.6084657   9.615857
  3       0.001  0.25  16.13926  0.4768189  12.563492
  3       0.001  0.50  15.86782  0.4910897  12.127917
  3       0.001  1.00  15.53798  0.5078632  11.798286
  3       0.010  0.25  14.38175  0.5734016  10.650768
  3       0.010  0.50  14.04600  0.5915617  10.293228
  3       0.010  1.00  13.81315  0.6046070  10.006441
  3       0.100  0.25  14.38847  0.5855417   9.676799
  3       0.100  0.50  14.78404  0.5753852   9.768102
  3       0.100  1.00  15.40943  0.5604765   9.902698

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 24 of 224 using same hudgins svmPoly 1 
Support Vector Machines with Polynomial Kernel 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 155, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE     
  1       0.001  0.25  21.32322  0.3860958  18.16588
  1       0.001  0.50  20.02728  0.4128820  17.08856
  1       0.001  1.00  18.20450  0.4466593  15.55722
  1       0.010  0.25  16.46211  0.4754168  13.46700
  1       0.010  0.50  15.91281  0.4901275  12.43164
  1       0.010  1.00  15.63520  0.5016126  11.90125
  1       0.100  0.25  15.27350  0.5204856  11.56963
  1       0.100  0.50  15.00314  0.5360671  11.34960
  1       0.100  1.00  14.65753  0.5547744  11.19927
  2       0.001  0.25  20.02659  0.4125974  17.08751
  2       0.001  0.50  18.20390  0.4465890  15.55624
  2       0.001  1.00  16.80810  0.4673196  13.93566
  2       0.010  0.25  15.82245  0.4947397  12.30702
  2       0.010  0.50  15.49424  0.5090108  11.77918
  2       0.010  1.00  15.15812  0.5257875  11.44960
  2       0.100  0.25  14.56708  0.5608204  11.20767
  2       0.100  0.50  14.79189  0.5558332  11.40396
  2       0.100  1.00  14.72605  0.5652172  11.22867
  3       0.001  0.25  18.94369  0.4335499  16.20966
  3       0.001  0.50  17.37817  0.4586676  14.61039
  3       0.001  1.00  16.23732  0.4804668  13.05658
  3       0.010  0.25  15.51818  0.5086733  11.83871
  3       0.010  0.50  15.15626  0.5245708  11.44858
  3       0.010  1.00  14.73526  0.5483937  11.04656
  3       0.100  0.25  15.39970  0.5372584  11.64513
  3       0.100  0.50  15.86206  0.5278485  11.99029
  3       0.100  1.00  16.46964  0.5096906  12.45270

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 25 of 224 using max all ranger 1 
Random Forest 

1210 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1089, 1089, 1089, 1089, 1090, 1089, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    11.30032  0.7367649  7.214542
   2    extratrees  11.58816  0.7278731  7.902281
  13    variance    11.19111  0.7392852  6.802713
  13    extratrees  11.32531  0.7360193  7.266332
  24    variance    11.29199  0.7344781  6.869281
  24    extratrees  11.26677  0.7382235  7.141917

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = variance
 and min.node.size = 5.


Now processing model 26 of 224 using same all ranger 1 
Random Forest 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 155, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    12.93877  0.6570976  9.058726
   2    extratrees  13.29527  0.6495490  9.834975
  13    variance    12.83708  0.6580674  8.526282
  13    extratrees  12.89862  0.6571325  8.963680
  24    variance    12.97627  0.6509768  8.584486
  24    extratrees  12.75187  0.6640308  8.771247

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 27 of 224 using max du ranger 1 
Random Forest 

1210 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1088, 1090, 1089, 1089, 1089, 1090, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    11.54447  0.7258002  7.507150
   2    extratrees  12.04971  0.7058435  8.370532
  10    variance    11.23677  0.7369069  6.905079
  10    extratrees  11.69496  0.7187008  7.714502
  18    variance    11.26521  0.7350567  6.855946
  18    extratrees  11.56817  0.7241252  7.514832

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 28 of 224 using same du ranger 1 
Random Forest 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 155, 156, 156, 157, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE      
   2    variance    13.06596  0.6480651   9.286736
   2    extratrees  13.58721  0.6294813  10.201630
  10    variance    12.64337  0.6637068   8.517107
  10    extratrees  13.22486  0.6384738   9.399577
  18    variance    12.87625  0.6516179   8.599528
  18    extratrees  13.03614  0.6480914   9.186506

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 29 of 224 using max rms ranger 1 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1210 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1088, 1090, 1089, 1089, 1089, 1089, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    11.69584  0.7152321  7.034305
  2     extratrees  11.64563  0.7193855  7.489378
  3     variance    11.68818  0.7158271  6.915053
  3     extratrees  11.72850  0.7145565  7.441656

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 30 of 224 using same rms ranger 1 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 157, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    12.93075  0.6528679  8.919598
  2     extratrees  13.03111  0.6539664  9.344881
  3     variance    12.85619  0.6566803  8.652518
  3     extratrees  12.91855  0.6565413  9.093881

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = variance
 and min.node.size = 5.


Now processing model 31 of 224 using max hudgins ranger 1 
Random Forest 

1210 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1089, 1088, 1089, 1088, 1089, 1089, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    11.90564  0.7105844  7.933303
   2    extratrees  12.48281  0.6862906  8.837735
   7    variance    11.41001  0.7304818  7.150929
   7    extratrees  11.98248  0.7068076  8.095561
  12    variance    11.37610  0.7311472  6.971022
  12    extratrees  11.82959  0.7134441  7.851943

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 32 of 224 using same hudgins ranger 1 
Random Forest 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE      
   2    variance    13.45870  0.6257781   9.585204
   2    extratrees  13.90649  0.6100822  10.459616
   7    variance    12.83505  0.6533529   8.645597
   7    extratrees  13.54438  0.6183314   9.733422
  12    variance    12.64643  0.6623362   8.406010
  12    extratrees  13.32126  0.6297335   9.445282

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 33 of 224 using max all lm 2 
Linear Regression 

518 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 466, 467, 465, 466, 466, 466, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  12.50072  0.673781  9.625327

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 34 of 224 using same all lm 2 
Linear Regression 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 157, 156, 156, 155, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.96259  0.6786292  10.46882

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 35 of 224 using max du lm 2 
Linear Regression 

518 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 467, 466, 467, 465, 466, 467, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.56127  0.6717662  9.868749

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 36 of 224 using same du lm 2 
Linear Regression 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 156, 156, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.99621  0.6749895  10.65121

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 37 of 224 using max rms lm 2 
Linear Regression 

518 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 466, 467, 466, 465, 466, 466, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  13.6267  0.6138919  10.57181

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 38 of 224 using same rms lm 2 
Linear Regression 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 156, 157, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  13.56703  0.6398848  10.81225

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 39 of 224 using max hudgins lm 2 
Linear Regression 

518 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 465, 466, 465, 466, 466, 467, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.83539  0.6586604  10.20345

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 40 of 224 using same hudgins lm 2 
Linear Regression 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 156, 156, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.75178  0.6842529  10.50927

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 41 of 224 using max all knn 2 
k-Nearest Neighbors 

518 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 466, 466, 467, 466, 465, 467, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5   9.80557  0.7940776  5.617431
  7  10.00120  0.7856764  5.981749
  9   9.93652  0.7885561  6.165294

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 42 of 224 using same all knn 2 
k-Nearest Neighbors 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 155, 156, 155, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.28935  0.7807006  7.066365
  7  10.56696  0.7689584  7.400610
  9  10.93090  0.7538646  7.828415

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 43 of 224 using max du knn 2 
k-Nearest Neighbors 

518 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 466, 467, 467, 466, 466, 466, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.868553  0.7930688  5.648481
  7  10.029312  0.7862464  5.982203
  9   9.981309  0.7876496  6.178605

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 44 of 224 using same du knn 2 
k-Nearest Neighbors 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 155, 157, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.19864  0.7842319  6.958915
  7  10.57879  0.7708609  7.424699
  9  10.95327  0.7550245  7.853896

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 45 of 224 using max rms knn 2 
k-Nearest Neighbors 

518 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 466, 466, 466, 466, 467, 466, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.79778  0.7550117  5.793135
  7  10.50563  0.7670618  5.901093
  9  10.41776  0.7703921  6.076091

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 46 of 224 using same rms knn 2 
k-Nearest Neighbors 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 157, 156, 155, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.73809  0.7569883  6.952558
  7  10.70322  0.7601558  7.227401
  9  10.51208  0.7705808  7.540149

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 47 of 224 using max hudgins knn 2 
k-Nearest Neighbors 

518 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 466, 466, 467, 466, 466, 467, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  12.67353  0.6665928  8.278445
  7  12.49630  0.6755133  8.268086
  9  12.41896  0.6789846  8.375843

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 48 of 224 using same hudgins knn 2 
k-Nearest Neighbors 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 154, 156, 156, 157, 154, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  12.82177  0.6698419  9.119937
  7  12.56279  0.6843638  9.245255
  9  12.54745  0.6853135  9.294242

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 49 of 224 using max all svmPoly 2 
Support Vector Machines with Polynomial Kernel 

518 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 467, 465, 467, 466, 466, 466, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  15.96055  0.5397527  13.269744
  1       0.001  0.50  15.04980  0.5594456  11.828370
  1       0.001  1.00  14.54745  0.5742285  11.188156
  1       0.010  0.25  14.11509  0.5903267  10.812127
  1       0.010  0.50  13.81762  0.6051545  10.560752
  1       0.010  1.00  13.47307  0.6232156  10.287769
  1       0.100  0.25  12.99841  0.6485547   9.964906
  1       0.100  0.50  12.71566  0.6633021   9.734710
  1       0.100  1.00  12.56703  0.6711395   9.598890
  2       0.001  0.25  15.01154  0.5620388  11.790401
  2       0.001  0.50  14.48272  0.5784703  11.138603
  2       0.001  1.00  14.08076  0.5944462  10.772781
  2       0.010  0.25  12.84005  0.6592683   9.675028
  2       0.010  0.50  12.49015  0.6754579   9.270192
  2       0.010  1.00  12.30500  0.6846979   8.963982
  2       0.100  0.25  11.77175  0.7094485   8.333750
  2       0.100  0.50  11.61878  0.7165928   8.246059
  2       0.100  1.00  11.53490  0.7210925   8.243139
  3       0.001  0.25  14.59693  0.5757669  11.284402
  3       0.001  0.50  14.12241  0.5942539  10.806987
  3       0.001  1.00  13.72919  0.6132926  10.454879
  3       0.010  0.25  12.53940  0.6741664   9.137170
  3       0.010  0.50  12.34673  0.6845139   8.814489
  3       0.010  1.00  12.16926  0.6930142   8.579787
  3       0.100  0.25  12.79174  0.6779449   8.721250
  3       0.100  0.50  13.50110  0.6597739   9.107740
  3       0.100  1.00  15.46026  0.6156867   9.949507

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 50 of 224 using same all svmPoly 2 
Support Vector Machines with Polynomial Kernel 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 157, 154, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  19.10648  0.5138393  16.487635
  1       0.001  0.50  17.02209  0.5393274  14.426789
  1       0.001  1.00  15.56295  0.5614618  12.688651
  1       0.010  0.25  14.67678  0.5818788  11.511587
  1       0.010  0.50  14.42621  0.5898132  11.259297
  1       0.010  1.00  14.13599  0.6039779  11.041165
  1       0.100  0.25  13.69967  0.6270823  10.685133
  1       0.100  0.50  13.21498  0.6536030  10.385788
  1       0.100  1.00  12.88491  0.6717900  10.189680
  2       0.001  0.25  17.01752  0.5397001  14.422764
  2       0.001  0.50  15.54117  0.5631849  12.671273
  2       0.001  1.00  14.76686  0.5818994  11.632781
  2       0.010  0.25  13.68259  0.6349451  10.662503
  2       0.010  0.50  13.10347  0.6626979  10.170613
  2       0.010  1.00  12.70401  0.6831667   9.782409
  2       0.100  0.25  12.26846  0.6965622   9.297250
  2       0.100  0.50  12.35411  0.6922458   9.425396
  2       0.100  1.00  12.63803  0.6812690   9.726583
  3       0.001  0.25  15.97816  0.5531734  13.210398
  3       0.001  0.50  15.05468  0.5778254  12.066123
  3       0.001  1.00  14.44544  0.5932725  11.306085
  3       0.010  0.25  12.99638  0.6687583  10.027185
  3       0.010  0.50  12.57711  0.6891165   9.588543
  3       0.010  1.00  12.34466  0.6969333   9.284284
  3       0.100  0.25  14.35033  0.6255351  10.886507
  3       0.100  0.50  15.82024  0.5883954  11.762425
  3       0.100  1.00  18.00955  0.5414310  12.532081

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 51 of 224 using max du svmPoly 2 
Support Vector Machines with Polynomial Kernel 

518 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 466, 466, 466, 466, 466, 466, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  17.07733  0.5142539  14.552016
  1       0.001  0.50  15.66230  0.5354938  12.608493
  1       0.001  1.00  15.00083  0.5526244  11.632233
  1       0.010  0.25  14.59453  0.5675679  11.124975
  1       0.010  0.50  14.32650  0.5805756  10.905881
  1       0.010  1.00  14.02641  0.5959651  10.667082
  1       0.100  0.25  13.56309  0.6199086  10.332779
  1       0.100  0.50  13.19829  0.6386665  10.112693
  1       0.100  1.00  12.98903  0.6498506   9.984145
  2       0.001  0.25  15.64339  0.5370444  12.586712
  2       0.001  0.50  14.96513  0.5553808  11.600257
  2       0.001  1.00  14.59929  0.5693423  11.136796
  2       0.010  0.25  13.44514  0.6301272  10.130065
  2       0.010  0.50  12.96197  0.6534101   9.693784
  2       0.010  1.00  12.61597  0.6700697   9.328441
  2       0.100  0.25  12.07002  0.6959940   8.531861
  2       0.100  0.50  11.86834  0.7053226   8.397172
  2       0.100  1.00  11.74554  0.7108814   8.348580
  3       0.001  0.25  15.18948  0.5510636  11.915927
  3       0.001  0.50  14.69185  0.5671617  11.241669
  3       0.001  1.00  14.29839  0.5848144  10.873030
  3       0.010  0.25  12.89392  0.6569643   9.576924
  3       0.010  0.50  12.53734  0.6741832   9.148507
  3       0.010  1.00  12.32197  0.6849130   8.827318
  3       0.100  0.25  13.31224  0.6513658   8.996518
  3       0.100  0.50  14.72515  0.6166586   9.503931
  3       0.100  1.00  16.27360  0.5870172  10.145468

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 52 of 224 using same du svmPoly 2 
Support Vector Machines with Polynomial Kernel 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 157, 155, 155, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  20.32423  0.4956563  17.444248
  1       0.001  0.50  18.08316  0.5192802  15.537942
  1       0.001  1.00  16.35468  0.5403528  13.696958
  1       0.010  0.25  15.22427  0.5616354  12.120778
  1       0.010  0.50  14.78380  0.5711464  11.519714
  1       0.010  1.00  14.57302  0.5810239  11.329670
  1       0.100  0.25  14.19420  0.6011942  11.038775
  1       0.100  0.50  13.85574  0.6221491  10.790743
  1       0.100  1.00  13.44936  0.6461568  10.575709
  2       0.001  0.25  18.08238  0.5192278  15.537467
  2       0.001  0.50  16.34698  0.5410468  13.688875
  2       0.001  1.00  15.37664  0.5591945  12.394659
  2       0.010  0.25  14.26808  0.6033778  11.137901
  2       0.010  0.50  13.76124  0.6310153  10.734278
  2       0.010  1.00  13.16692  0.6615706  10.216841
  2       0.100  0.25  12.73096  0.6839975   9.614758
  2       0.100  0.50  12.90485  0.6741570   9.731991
  2       0.100  1.00  13.07465  0.6659760   9.854067
  3       0.001  0.25  17.00870  0.5318442  14.417379
  3       0.001  0.50  15.65146  0.5525785  12.725683
  3       0.001  1.00  14.96876  0.5694113  11.773343
  3       0.010  0.25  13.63169  0.6395398  10.593252
  3       0.010  0.50  13.06299  0.6670247  10.079288
  3       0.010  1.00  12.62680  0.6900991   9.610102
  3       0.100  0.25  14.03943  0.6328039  10.485864
  3       0.100  0.50  15.57084  0.5878812  11.518173
  3       0.100  1.00  17.77558  0.5339396  12.795669

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 53 of 224 using max rms svmPoly 2 
Support Vector Machines with Polynomial Kernel 

518 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 467, 466, 466, 466, 466, 467, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  21.26925  0.4753042  17.742493
  1       0.001  0.50  19.61979  0.5041239  16.506801
  1       0.001  1.00  17.03136  0.5430668  14.444025
  1       0.010  0.25  14.85276  0.5831799  11.778150
  1       0.010  0.50  14.26789  0.5987423  11.047258
  1       0.010  1.00  13.88273  0.6070000  10.665756
  1       0.100  0.25  13.72162  0.6110698  10.561931
  1       0.100  0.50  13.68148  0.6119815  10.548097
  1       0.100  1.00  13.64965  0.6127436  10.544190
  2       0.001  0.25  19.61913  0.5041441  16.506650
  2       0.001  0.50  17.03027  0.5431159  14.442574
  2       0.001  1.00  15.20156  0.5747846  12.263901
  2       0.010  0.25  14.21485  0.6012683  11.008636
  2       0.010  0.50  13.81612  0.6111840  10.614516
  2       0.010  1.00  13.57906  0.6195624  10.437376
  2       0.100  0.25  12.60263  0.6715144   9.384822
  2       0.100  0.50  12.55597  0.6740182   9.223615
  2       0.100  1.00  12.56191  0.6737308   9.167403
  3       0.001  0.25  18.14788  0.5283945  15.418927
  3       0.001  0.50  15.74887  0.5617323  13.017908
  3       0.001  1.00  14.64088  0.5883751  11.506162
  3       0.010  0.25  13.89460  0.6108421  10.683275
  3       0.010  0.50  13.56258  0.6221052  10.411745
  3       0.010  1.00  13.29762  0.6347616  10.194277
  3       0.100  0.25  12.32860  0.6847651   8.942330
  3       0.100  0.50  12.15383  0.6929526   8.715956
  3       0.100  1.00  11.96615  0.7012987   8.522731

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 54 of 224 using same rms svmPoly 2 
Support Vector Machines with Polynomial Kernel 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.50032  0.4820799  19.361828
  1       0.001  0.50  21.90418  0.4908707  18.752508
  1       0.001  1.00  20.63727  0.5189578  17.641245
  1       0.010  0.25  17.49505  0.5534536  14.962818
  1       0.010  0.50  15.57241  0.5818136  12.703709
  1       0.010  1.00  14.58801  0.6069154  11.580883
  1       0.100  0.25  13.97850  0.6221942  11.010716
  1       0.100  0.50  13.75644  0.6298415  10.875112
  1       0.100  1.00  13.66218  0.6343278  10.874064
  2       0.001  0.25  21.90411  0.4907829  18.752341
  2       0.001  0.50  20.63712  0.5189127  17.641265
  2       0.001  1.00  18.30637  0.5451338  15.754404
  2       0.010  0.25  15.56530  0.5823436  12.694300
  2       0.010  0.50  14.56508  0.6085605  11.563899
  2       0.010  1.00  14.08110  0.6216802  11.088923
  2       0.100  0.25  12.97609  0.6761653  10.083836
  2       0.100  0.50  12.60056  0.6963366   9.614472
  2       0.100  1.00  12.48214  0.7025062   9.380613
  3       0.001  0.25  21.25528  0.5030341  18.161088
  3       0.001  0.50  19.42913  0.5339377  16.673767
  3       0.001  1.00  16.88449  0.5611056  14.262265
  3       0.010  0.25  14.89584  0.5998880  11.897575
  3       0.010  0.50  14.16933  0.6207325  11.138254
  3       0.010  1.00  13.75773  0.6337509  10.827483
  3       0.100  0.25  12.51971  0.7006692   9.476408
  3       0.100  0.50  12.32307  0.7086949   9.223549
  3       0.100  1.00  12.24811  0.7106656   9.133593

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 55 of 224 using max hudgins svmPoly 2 
Support Vector Machines with Polynomial Kernel 

518 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 467, 466, 467, 466, 467, 466, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.76621  0.4862155  16.031692
  1       0.001  0.50  16.69139  0.5146791  14.126706
  1       0.001  1.00  15.52010  0.5333301  12.350716
  1       0.010  0.25  14.85248  0.5526334  11.404030
  1       0.010  0.50  14.55551  0.5657858  11.088340
  1       0.010  1.00  14.29785  0.5787336  10.904303
  1       0.100  0.25  13.82209  0.6045238  10.547589
  1       0.100  0.50  13.47671  0.6231356  10.336416
  1       0.100  1.00  13.21667  0.6367223  10.232423
  2       0.001  0.25  16.68526  0.5152620  14.119832
  2       0.001  0.50  15.50371  0.5346418  12.336421
  2       0.001  1.00  14.93543  0.5507744  11.522432
  2       0.010  0.25  14.08105  0.5946424  10.672219
  2       0.010  0.50  13.52639  0.6232821  10.189654
  2       0.010  1.00  12.97776  0.6511894   9.726435
  2       0.100  0.25  12.12428  0.6923942   8.672424
  2       0.100  0.50  11.94047  0.7010586   8.487785
  2       0.100  1.00  11.77401  0.7091458   8.335070
  3       0.001  0.25  15.84362  0.5281330  12.968992
  3       0.001  0.50  15.10541  0.5463235  11.752715
  3       0.001  1.00  14.69337  0.5615464  11.211522
  3       0.010  0.25  13.45726  0.6274346  10.096377
  3       0.010  0.50  12.89901  0.6548521   9.615800
  3       0.010  1.00  12.46226  0.6761316   9.198597
  3       0.100  0.25  12.42889  0.6827180   8.654966
  3       0.100  0.50  12.48464  0.6804235   8.814069
  3       0.100  1.00  12.85266  0.6676736   9.102561

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 56 of 224 using same hudgins svmPoly 2 
Support Vector Machines with Polynomial Kernel 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 157, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  21.44345  0.4814665  18.324448
  1       0.001  0.50  19.75480  0.5043907  17.041595
  1       0.001  1.00  17.71634  0.5243717  15.162727
  1       0.010  0.25  15.77253  0.5476265  12.853340
  1       0.010  0.50  15.13423  0.5609291  11.941110
  1       0.010  1.00  14.82526  0.5692048  11.510655
  1       0.100  0.25  14.58063  0.5821957  11.309306
  1       0.100  0.50  14.17390  0.6050302  10.983921
  1       0.100  1.00  13.70780  0.6302964  10.668798
  2       0.001  0.25  19.75251  0.5041499  17.040980
  2       0.001  0.50  17.71482  0.5245169  15.161837
  2       0.001  1.00  16.12832  0.5455750  13.416677
  2       0.010  0.25  14.95205  0.5737711  11.793498
  2       0.010  0.50  14.39016  0.5944506  11.183156
  2       0.010  1.00  13.94379  0.6185194  10.825468
  2       0.100  0.25  12.60201  0.6887071   9.596504
  2       0.100  0.50  12.64624  0.6864314   9.562781
  2       0.100  1.00  12.75503  0.6804211   9.597576
  3       0.001  0.25  18.41539  0.5175996  15.897848
  3       0.001  0.50  16.64240  0.5367102  14.010865
  3       0.001  1.00  15.53432  0.5538412  12.578421
  3       0.010  0.25  14.37237  0.5993514  11.212402
  3       0.010  0.50  13.82342  0.6264876  10.716341
  3       0.010  1.00  13.12403  0.6632677  10.130855
  3       0.100  0.25  12.85903  0.6750633   9.664878
  3       0.100  0.50  13.36017  0.6546384  10.007729
  3       0.100  1.00  14.95381  0.5967024  11.163177

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 57 of 224 using max all ranger 2 
Random Forest 

518 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 466, 466, 466, 466, 466, 467, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.07612  0.7870979  6.253317
   2    extratrees  10.35701  0.7805780  7.021803
  13    variance    10.05909  0.7842845  5.942419
  13    extratrees  10.12851  0.7844564  6.287537
  24    variance    10.24948  0.7759458  6.053671
  24    extratrees  10.06669  0.7865399  6.159611

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = variance
 and min.node.size = 5.


Now processing model 58 of 224 using same all ranger 2 
Random Forest 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 156, 156, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.87730  0.7592162  7.599012
   2    extratrees  11.23434  0.7562390  8.394109
  13    variance    10.92624  0.7504951  7.281819
  13    extratrees  11.03666  0.7538569  7.785228
  24    variance    11.15872  0.7400924  7.443925
  24    extratrees  10.99142  0.7549309  7.688516

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 59 of 224 using max du ranger 2 
Random Forest 

518 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 467, 467, 466, 466, 465, 467, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.23034  0.7792110  6.477702
   2    extratrees  10.65884  0.7658270  7.348405
  10    variance    10.10867  0.7813361  6.027311
  10    extratrees  10.40227  0.7715918  6.634902
  18    variance    10.21648  0.7762946  6.043534
  18    extratrees  10.30170  0.7752687  6.450982

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 60 of 224 using same du ranger 2 
Random Forest 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    11.14015  0.7510174  7.938651
   2    extratrees  11.53059  0.7474643  8.741458
  10    variance    11.14365  0.7430077  7.495852
  10    extratrees  11.26894  0.7466242  8.131461
  18    variance    11.27128  0.7360271  7.543010
  18    extratrees  11.24397  0.7457738  8.028857

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 61 of 224 using max rms ranger 2 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

518 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 466, 466, 466, 467, 465, 467, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    10.37275  0.7706792  5.974971
  2     extratrees  10.44283  0.7705597  6.602894
  3     variance    10.58876  0.7614501  5.952531
  3     extratrees  10.59451  0.7627675  6.515842

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 62 of 224 using same rms ranger 2 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 154, 155, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    11.20992  0.7421311  7.317458
  2     extratrees  11.16002  0.7540265  7.907984
  3     variance    11.16462  0.7423122  7.111984
  3     extratrees  11.17279  0.7494698  7.749972

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 63 of 224 using max hudgins ranger 2 
Random Forest 

518 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 466, 465, 467, 466, 467, 467, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.52674  0.7673069  6.895351
   2    extratrees  10.98080  0.7523800  7.772627
   7    variance    10.18452  0.7795565  6.176552
   7    extratrees  10.63600  0.7620422  7.037475
  12    variance    10.18485  0.7795474  6.040851
  12    extratrees  10.48785  0.7678803  6.769273

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 64 of 224 using same hudgins ranger 2 
Random Forest 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 157, 156, 156, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    11.18435  0.7530237  8.170749
   2    extratrees  11.85510  0.7345555  9.090859
   7    variance    10.96497  0.7543900  7.547776
   7    extratrees  11.41333  0.7415740  8.404654
  12    variance    10.89890  0.7551405  7.304353
  12    extratrees  11.31495  0.7446507  8.227846

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 65 of 224 using max all lm 3 
Linear Regression 

346 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 312, 311, 311, 311, 311, 313, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.10286  0.6986397  9.410327

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 66 of 224 using same all lm 3 
Linear Regression 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 156, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.56087  0.6906277  9.910892

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 67 of 224 using max du lm 3 
Linear Regression 

346 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 311, 313, 310, 313, 312, 311, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.10348  0.7013593  9.550204

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 68 of 224 using same du lm 3 
Linear Regression 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 155, 157, 156, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.34094  0.7006584  9.902712

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 69 of 224 using max rms lm 3 
Linear Regression 

346 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 312, 311, 313, 312, 311, 312, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  13.1882  0.6412911  10.32246

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 70 of 224 using same rms lm 3 
Linear Regression 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 157, 156, 156, 156, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  12.96232  0.662163  10.20299

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 71 of 224 using max hudgins lm 3 
Linear Regression 

346 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 310, 312, 312, 312, 311, 310, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.09767  0.7012461  9.700446

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 72 of 224 using same hudgins lm 3 
Linear Regression 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 154, 155, 155, 156, 157, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  12.50586  0.690637  10.20417

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 73 of 224 using max all knn 3 
k-Nearest Neighbors 

346 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 311, 311, 311, 311, 311, 311, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.602552  0.8008924  5.406411
  7  9.645387  0.7999381  5.752318
  9  9.918416  0.7896137  6.188562

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 74 of 224 using same all knn 3 
k-Nearest Neighbors 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 155, 156, 157, 157, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.15619  0.7875776  6.813836
  7  10.72108  0.7633225  7.500314
  9  10.87983  0.7560843  7.887856

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 75 of 224 using max du knn 3 
k-Nearest Neighbors 

346 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 311, 313, 311, 311, 311, 311, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.594711  0.8023654  5.353626
  7  9.626420  0.8030033  5.707578
  9  9.971274  0.7907382  6.166459

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 76 of 224 using same du knn 3 
k-Nearest Neighbors 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 154, 156, 154, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.13068  0.7872106  6.803982
  7  10.72728  0.7628397  7.540215
  9  10.91314  0.7566263  7.920600

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 77 of 224 using max rms knn 3 
k-Nearest Neighbors 

346 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 311, 310, 312, 310, 312, 313, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.873672  0.7923796  5.518963
  7   9.744879  0.7981063  5.664596
  9  10.015471  0.7883428  5.936791

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 78 of 224 using same rms knn 3 
k-Nearest Neighbors 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 155, 155, 156, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5  10.129302  0.7842842  6.317855
  7   9.953972  0.7938277  6.721916
  9  10.350632  0.7785818  7.246887

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 79 of 224 using max hudgins knn 3 
k-Nearest Neighbors 

346 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 312, 312, 311, 311, 311, 311, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  11.73199  0.7104173  7.527897
  7  11.92863  0.7012727  7.895207
  9  12.20431  0.6879072  8.253524

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 80 of 224 using same hudgins knn 3 
k-Nearest Neighbors 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 156, 155, 155, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  11.48481  0.7264283  8.237176
  7  11.65628  0.7172232  8.632983
  9  12.02008  0.6998902  8.991745

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 81 of 224 using max all svmPoly 3 
Support Vector Machines with Polynomial Kernel 

346 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 313, 311, 312, 312, 311, 311, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  16.68109  0.5458017  14.251244
  1       0.001  0.50  15.11822  0.5737244  12.105882
  1       0.001  1.00  14.37517  0.5944731  11.046596
  1       0.010  0.25  13.86967  0.6105855  10.525815
  1       0.010  0.50  13.59511  0.6231614  10.329538
  1       0.010  1.00  13.19503  0.6435517  10.045223
  1       0.100  0.25  12.65408  0.6720561   9.684886
  1       0.100  0.50  12.34922  0.6871585   9.545326
  1       0.100  1.00  12.13227  0.6978978   9.375783
  2       0.001  0.25  15.09852  0.5752058  12.088403
  2       0.001  0.50  14.32699  0.5972940  11.008944
  2       0.001  1.00  13.87554  0.6122896  10.533419
  2       0.010  0.25  12.69221  0.6728424   9.606050
  2       0.010  0.50  12.19835  0.6956750   9.178143
  2       0.010  1.00  11.84346  0.7103639   8.749183
  2       0.100  0.25  11.34308  0.7323178   7.980815
  2       0.100  0.50  11.16109  0.7402233   7.742336
  2       0.100  1.00  11.08457  0.7442599   7.634805
  3       0.001  0.25  14.55505  0.5916929  11.327296
  3       0.001  0.50  14.00248  0.6094715  10.674928
  3       0.001  1.00  13.54514  0.6285261  10.267149
  3       0.010  0.25  12.28669  0.6906037   9.079031
  3       0.010  0.50  12.09506  0.7002041   8.682760
  3       0.010  1.00  11.89693  0.7093532   8.402188
  3       0.100  0.25  13.52257  0.6692859   8.714332
  3       0.100  0.50  14.57767  0.6374400   9.285047
  3       0.100  1.00  16.48907  0.5925605  10.188657

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 82 of 224 using same all svmPoly 3 
Support Vector Machines with Polynomial Kernel 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 154, 156, 156, 156, 157, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.82763  0.5379511  16.240708
  1       0.001  0.50  16.44765  0.5639804  13.958094
  1       0.001  1.00  15.03032  0.5917806  12.094204
  1       0.010  0.25  14.10430  0.6137066  10.857468
  1       0.010  0.50  13.77119  0.6249823  10.543062
  1       0.010  1.00  13.50993  0.6370009  10.385634
  1       0.100  0.25  13.12765  0.6564090  10.192392
  1       0.100  0.50  12.78905  0.6741770   9.984987
  1       0.100  1.00  12.56495  0.6870703   9.905780
  2       0.001  0.25  16.43992  0.5643896  13.949865
  2       0.001  0.50  15.00714  0.5932624  12.071653
  2       0.001  1.00  14.21037  0.6135542  10.973819
  2       0.010  0.25  13.10161  0.6625209  10.007199
  2       0.010  0.50  12.56188  0.6879726   9.603843
  2       0.010  1.00  11.95160  0.7165762   9.061343
  2       0.100  0.25  11.39659  0.7362541   8.298997
  2       0.100  0.50  11.26557  0.7409885   8.178101
  2       0.100  1.00  11.35014  0.7369091   8.290734
  3       0.001  0.25  15.50590  0.5808263  12.716789
  3       0.001  0.50  14.47662  0.6080353  11.324058
  3       0.001  1.00  13.88770  0.6239327  10.677988
  3       0.010  0.25  12.46249  0.6941495   9.488707
  3       0.010  0.50  11.85306  0.7205619   8.924521
  3       0.010  1.00  11.68367  0.7275423   8.705884
  3       0.100  0.25  13.72916  0.6645885   9.959512
  3       0.100  0.50  15.55072  0.6176754  11.198710
  3       0.100  1.00  18.62422  0.5466792  12.917120

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 83 of 224 using max du svmPoly 3 
Support Vector Machines with Polynomial Kernel 

346 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 310, 311, 312, 312, 312, 310, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.01534  0.5226062  15.554090
  1       0.001  0.50  15.94518  0.5488118  13.256114
  1       0.001  1.00  14.93143  0.5712109  11.707424
  1       0.010  0.25  14.32845  0.5896175  10.842674
  1       0.010  0.50  14.05005  0.6009600  10.614246
  1       0.010  1.00  13.74047  0.6165586  10.389960
  1       0.100  0.25  13.20543  0.6449359  10.044714
  1       0.100  0.50  12.85647  0.6634275   9.848509
  1       0.100  1.00  12.61038  0.6763974   9.751419
  2       0.001  0.25  15.93144  0.5498471  13.241491
  2       0.001  0.50  14.90458  0.5730104  11.683518
  2       0.001  1.00  14.38904  0.5891928  10.918200
  2       0.010  0.25  13.33005  0.6422819  10.038526
  2       0.010  0.50  12.77082  0.6691358   9.588322
  2       0.010  1.00  12.32970  0.6897547   9.198138
  2       0.100  0.25  11.77129  0.7151045   8.315448
  2       0.100  0.50  11.64797  0.7212716   8.133281
  2       0.100  1.00  11.62122  0.7238495   8.037303
  3       0.001  0.25  15.22302  0.5651803  12.126509
  3       0.001  0.50  14.51596  0.5857590  11.104098
  3       0.001  1.00  14.10627  0.6016075  10.655535
  3       0.010  0.25  12.79276  0.6679827   9.516767
  3       0.010  0.50  12.43810  0.6855561   9.107764
  3       0.010  1.00  12.18663  0.6982762   8.729930
  3       0.100  0.25  13.44793  0.6653211   8.799596
  3       0.100  0.50  14.09353  0.6472836   9.112259
  3       0.100  1.00  15.49081  0.6192851   9.785617

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 84 of 224 using same du svmPoly 3 
Support Vector Machines with Polynomial Kernel 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  19.98929  0.5140910  17.152276
  1       0.001  0.50  17.78892  0.5461481  15.301722
  1       0.001  1.00  15.88312  0.5696287  13.220084
  1       0.010  0.25  14.59088  0.5969795  11.356916
  1       0.010  0.50  14.22556  0.6047263  10.864875
  1       0.010  1.00  13.92908  0.6170891  10.647087
  1       0.100  0.25  13.54543  0.6348037  10.403534
  1       0.100  0.50  13.26413  0.6487047  10.285467
  1       0.100  1.00  13.01476  0.6612788  10.187938
  2       0.001  0.25  17.78791  0.5461731  15.301576
  2       0.001  0.50  15.87294  0.5701890  13.207648
  2       0.001  1.00  14.76325  0.5935742  11.664273
  2       0.010  0.25  13.70159  0.6345304  10.442033
  2       0.010  0.50  13.18381  0.6578750   9.982216
  2       0.010  1.00  12.61263  0.6834467   9.563943
  2       0.100  0.25  11.67321  0.7252050   8.569857
  2       0.100  0.50  11.60853  0.7275367   8.457512
  2       0.100  1.00  11.55480  0.7290076   8.384741
  3       0.001  0.25  16.45804  0.5617614  13.942222
  3       0.001  0.50  15.09023  0.5856359  12.100458
  3       0.001  1.00  14.36060  0.6041740  11.032521
  3       0.010  0.25  13.11203  0.6621513   9.890058
  3       0.010  0.50  12.48446  0.6908311   9.426146
  3       0.010  1.00  11.92894  0.7142881   8.951611
  3       0.100  0.25  12.87354  0.6879492   9.275788
  3       0.100  0.50  14.02332  0.6499765  10.142496
  3       0.100  1.00  15.55473  0.6007253  11.331082

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 85 of 224 using max rms svmPoly 3 
Support Vector Machines with Polynomial Kernel 

346 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 311, 311, 312, 312, 311, 313, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  21.76987  0.4849308  18.463943
  1       0.001  0.50  20.47752  0.5099099  17.440073
  1       0.001  1.00  18.25850  0.5456983  15.686463
  1       0.010  0.25  15.18558  0.5854329  12.359752
  1       0.010  0.50  14.19365  0.6167057  11.097266
  1       0.010  1.00  13.65852  0.6304902  10.528917
  1       0.100  0.25  13.37246  0.6374455  10.344878
  1       0.100  0.50  13.32383  0.6384147  10.335316
  1       0.100  1.00  13.29843  0.6394224  10.353510
  2       0.001  0.25  20.47716  0.5098769  17.439968
  2       0.001  0.50  18.25772  0.5457183  15.685932
  2       0.001  1.00  15.69850  0.5745767  13.059460
  2       0.010  0.25  14.16045  0.6184085  11.077143
  2       0.010  0.50  13.60867  0.6337272  10.495171
  2       0.010  1.00  13.28814  0.6435493  10.275862
  2       0.100  0.25  12.19119  0.6992299   9.224476
  2       0.100  0.50  12.04447  0.7061596   8.949012
  2       0.100  1.00  12.02312  0.7069463   8.863952
  3       0.001  0.25  19.34911  0.5306531  16.530735
  3       0.001  0.50  16.61267  0.5640872  14.173910
  3       0.001  1.00  14.85568  0.5958756  11.944485
  3       0.010  0.25  13.76221  0.6310353  10.655836
  3       0.010  0.50  13.32958  0.6440044  10.286383
  3       0.010  1.00  13.02684  0.6567378  10.064835
  3       0.100  0.25  11.89319  0.7129795   8.712709
  3       0.100  0.50  11.78957  0.7169586   8.508251
  3       0.100  1.00  11.64566  0.7220435   8.276323

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 86 of 224 using same rms svmPoly 3 
Support Vector Machines with Polynomial Kernel 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 155, 155, 157, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.16895  0.4970320  19.130024
  1       0.001  0.50  21.56136  0.4988587  18.485108
  1       0.001  1.00  20.27758  0.5331153  17.370673
  1       0.010  0.25  17.07408  0.5808279  14.611515
  1       0.010  0.50  15.00320  0.6068848  12.200250
  1       0.010  1.00  13.99523  0.6315186  10.890334
  1       0.100  0.25  13.34539  0.6493803  10.310322
  1       0.100  0.50  13.20782  0.6518988  10.272827
  1       0.100  1.00  13.16229  0.6543372  10.305449
  2       0.001  0.25  21.56094  0.4987480  18.484867
  2       0.001  0.50  20.27715  0.5330745  17.370492
  2       0.001  1.00  17.89439  0.5674211  15.391466
  2       0.010  0.25  14.99836  0.6077048  12.196901
  2       0.010  0.50  13.96672  0.6331059  10.879276
  2       0.010  1.00  13.39370  0.6491369  10.347458
  2       0.100  0.25  12.56762  0.6878314   9.750915
  2       0.100  0.50  12.14947  0.7063238   9.357053
  2       0.100  1.00  12.02410  0.7125632   9.091570
  3       0.001  0.25  20.91357  0.5208554  17.925660
  3       0.001  0.50  19.03179  0.5557870  16.349714
  3       0.001  1.00  16.41564  0.5882417  13.911517
  3       0.010  0.25  14.31964  0.6231874  11.323936
  3       0.010  0.50  13.59797  0.6450426  10.530443
  3       0.010  1.00  13.15507  0.6581727  10.196940
  3       0.100  0.25  12.00427  0.7142632   9.087721
  3       0.100  0.50  11.78903  0.7214922   8.779790
  3       0.100  1.00  11.62457  0.7267504   8.566374

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 87 of 224 using max hudgins svmPoly 3 
Support Vector Machines with Polynomial Kernel 

346 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 312, 311, 310, 312, 312, 313, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  19.76802  0.4949255  16.971135
  1       0.001  0.50  17.48482  0.5242224  15.063254
  1       0.001  1.00  15.61278  0.5471114  12.726661
  1       0.010  0.25  14.69896  0.5711881  11.246499
  1       0.010  0.50  14.35811  0.5838828  10.843959
  1       0.010  1.00  14.05829  0.5981403  10.614137
  1       0.100  0.25  13.58596  0.6228917  10.289534
  1       0.100  0.50  13.11624  0.6469492  10.003120
  1       0.100  1.00  12.72011  0.6676008   9.824206
  2       0.001  0.25  17.48111  0.5244699  15.059076
  2       0.001  0.50  15.60380  0.5477826  12.716710
  2       0.001  1.00  14.82375  0.5685150  11.497016
  2       0.010  0.25  13.94314  0.6076997  10.536557
  2       0.010  0.50  13.43866  0.6328565  10.128970
  2       0.010  1.00  12.88558  0.6594184   9.688219
  2       0.100  0.25  11.95746  0.7039322   8.613276
  2       0.100  0.50  11.81812  0.7106617   8.429386
  2       0.100  1.00  11.81886  0.7123505   8.331011
  3       0.001  0.25  16.29176  0.5385132  13.749249
  3       0.001  0.50  15.09144  0.5619256  11.947703
  3       0.001  1.00  14.52721  0.5786790  11.016664
  3       0.010  0.25  13.45497  0.6315586  10.124625
  3       0.010  0.50  12.98598  0.6544471   9.679056
  3       0.010  1.00  12.59524  0.6743700   9.243332
  3       0.100  0.25  13.17390  0.6764644   8.600571
  3       0.100  0.50  14.92815  0.6378241   9.188321
  3       0.100  1.00  17.43255  0.5904566  10.094540

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 88 of 224 using same hudgins svmPoly 3 
Support Vector Machines with Polynomial Kernel 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 153, 155, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  21.07977  0.4788163  18.074779
  1       0.001  0.50  19.45676  0.5117946  16.763377
  1       0.001  1.00  17.29404  0.5378157  14.853006
  1       0.010  0.25  15.22143  0.5678082  12.286907
  1       0.010  0.50  14.56264  0.5833345  11.328550
  1       0.010  1.00  14.27101  0.5911303  10.951904
  1       0.100  0.25  13.92465  0.6075319  10.715306
  1       0.100  0.50  13.61886  0.6230656  10.529495
  1       0.100  1.00  13.26280  0.6425795  10.334821
  2       0.001  0.25  19.45422  0.5114729  16.763051
  2       0.001  0.50  17.29128  0.5378477  14.849992
  2       0.001  1.00  15.56927  0.5606129  12.751179
  2       0.010  0.25  14.36462  0.5960921  11.139610
  2       0.010  0.50  13.85411  0.6158454  10.627659
  2       0.010  1.00  13.29836  0.6431379  10.169764
  2       0.100  0.25  11.80535  0.7143684   8.796251
  2       0.100  0.50  11.68276  0.7194725   8.625590
  2       0.100  1.00  11.51000  0.7273605   8.433285
  3       0.001  0.25  18.08832  0.5290323  15.637510
  3       0.001  0.50  16.16977  0.5540771  13.616241
  3       0.001  1.00  14.98036  0.5738407  11.990253
  3       0.010  0.25  13.78157  0.6232485  10.557693
  3       0.010  0.50  13.17920  0.6498476  10.013081
  3       0.010  1.00  12.58357  0.6784328   9.524011
  3       0.100  0.25  12.27307  0.6973876   9.099003
  3       0.100  0.50  12.66844  0.6882192   9.312466
  3       0.100  1.00  13.67376  0.6576993  10.029194

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 89 of 224 using max all ranger 3 
Random Forest 

346 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 312, 311, 312, 312, 310, 312, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.871572  0.7950282  6.162321
   2    extratrees  10.017469  0.7969879  6.853810
  13    variance     9.927097  0.7895752  5.799129
  13    extratrees   9.813687  0.7978932  6.181034
  24    variance     9.965911  0.7876782  5.770581
  24    extratrees   9.732551  0.8003442  6.015340

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 90 of 224 using same all ranger 3 
Random Forest 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 157, 155, 155, 156, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.790845  0.8018336  6.771512
   2    extratrees  10.461305  0.7854223  7.725599
  13    variance     9.749665  0.7963490  6.367779
  13    extratrees   9.911203  0.7966304  6.828272
  24    variance     9.999665  0.7860670  6.470596
  24    extratrees   9.784365  0.8007996  6.650843

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = variance
 and min.node.size = 5.


Now processing model 91 of 224 using max du ranger 3 
Random Forest 

346 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 313, 312, 310, 313, 310, 311, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.909281  0.7977650  6.320882
   2    extratrees  10.242503  0.7912628  7.158342
  10    variance     9.730505  0.8012093  5.735124
  10    extratrees   9.998183  0.7942910  6.490850
  18    variance     9.842292  0.7962069  5.708419
  18    extratrees   9.880877  0.7986836  6.296717

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 92 of 224 using same du ranger 3 
Random Forest 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 154, 155, 157, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.879091  0.8032896  6.877225
   2    extratrees  10.656091  0.7806653  7.936916
  10    variance     9.756801  0.8001190  6.295023
  10    extratrees  10.099464  0.7913851  7.071343
  18    variance     9.845977  0.7950109  6.299022
  18    extratrees   9.977498  0.7953748  6.863104

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 93 of 224 using max rms ranger 3 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

346 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 312, 312, 312, 312, 311, 309, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    10.44989  0.7665579  6.048503
  2     extratrees  10.09809  0.7854538  6.491241
  3     variance    10.62858  0.7588878  6.037922
  3     extratrees  10.22672  0.7781783  6.416145

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 94 of 224 using same rms ranger 3 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 155, 155, 156, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    10.70176  0.7622154  6.928049
  2     extratrees  10.64405  0.7722069  7.379222
  3     variance    10.95136  0.7485567  6.952899
  3     extratrees  10.59411  0.7706790  7.198520

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 95 of 224 using max hudgins ranger 3 
Random Forest 

346 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 313, 310, 312, 312, 311, 311, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance    10.219937  0.7819998  6.667301
   2    extratrees  10.591228  0.7742549  7.518966
   7    variance     9.977011  0.7883455  6.019592
   7    extratrees  10.236260  0.7827584  6.809709
  12    variance    10.058510  0.7835957  5.885598
  12    extratrees  10.116032  0.7865214  6.578531

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 96 of 224 using same hudgins ranger 3 
Random Forest 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 157, 156, 156, 156, 154, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.31740  0.7858850  7.275017
   2    extratrees  11.08465  0.7637447  8.307543
   7    variance    10.02977  0.7891693  6.644047
   7    extratrees  10.55142  0.7743398  7.484856
  12    variance    10.01365  0.7889276  6.518935
  12    extratrees  10.39307  0.7796744  7.235758

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 97 of 224 using max all lm 4 
Linear Regression 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 155, 156, 156, 155, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.01671  0.7162868  9.281269

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 98 of 224 using same all lm 4 
Linear Regression 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 154, 155, 158, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.06894  0.7199046  9.342144

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 99 of 224 using max du lm 4 
Linear Regression 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 155, 156, 156, 156, 156, ... 
Resampling results:

  RMSE     Rsquared  MAE     
  12.1567  0.713855  9.577653

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 100 of 224 using same du lm 4 
Linear Regression 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 156, 157, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.21128  0.7084871  9.590414

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 101 of 224 using max rms lm 4 
Linear Regression 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 154, 154, 156, 156, ... 
Resampling results:

  RMSE     Rsquared   MAE    
  12.7763  0.6688638  9.85148

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 102 of 224 using same rms lm 4 
Linear Regression 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 157, 155, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.98544  0.6716836  10.07728

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 103 of 224 using max hudgins lm 4 
Linear Regression 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 154, 157, 156, 156, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.07509  0.7075088  9.795047

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 104 of 224 using same hudgins lm 4 
Linear Regression 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 154, 156, 155, 155, 156, 157, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.10048  0.7179383  9.635004

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 105 of 224 using max all knn 4 
k-Nearest Neighbors 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 155, 156, 157, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.91059  0.7501221  6.778770
  7  10.96916  0.7479978  7.252203
  9  10.97641  0.7507853  7.664318

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 106 of 224 using same all knn 4 
k-Nearest Neighbors 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 157, 156, 155, 156, 155, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.59056  0.7643766  6.551123
  7  10.22211  0.7815671  6.771848
  9  10.65150  0.7646291  7.416887

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 107 of 224 using max du knn 4 
k-Nearest Neighbors 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 155, 156, 155, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.83228  0.7535993  6.734211
  7  10.95308  0.7488204  7.232875
  9  11.01417  0.7477205  7.660157

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 108 of 224 using same du knn 4 
k-Nearest Neighbors 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 157, 155, 155, 157, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.45569  0.7711640  6.436289
  7  10.24485  0.7806791  6.766130
  9  10.62314  0.7676479  7.333960

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 109 of 224 using max rms knn 4 
k-Nearest Neighbors 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 156, 155, 155, 157, 155, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.72145  0.7510896  6.330444
  7  10.69067  0.7526971  6.633598
  9  10.73360  0.7533809  7.012980

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 110 of 224 using same rms knn 4 
k-Nearest Neighbors 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 156, 156, 155, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.991291  0.7910530  5.782145
  7  10.008341  0.7916476  6.061754
  9  10.088328  0.7901154  6.561037

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 111 of 224 using max hudgins knn 4 
k-Nearest Neighbors 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 155, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  11.90740  0.7075573  8.186853
  7  11.84985  0.7115426  8.574795
  9  12.07619  0.7034995  8.848437

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 112 of 224 using same hudgins knn 4 
k-Nearest Neighbors 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 155, 155, 156, 155, 156, ... 
Resampling results across tuning parameters:

  k  RMSE     Rsquared   MAE     
  5  11.8494  0.7124084  8.057306
  7  11.5882  0.7269734  8.258698
  9  11.8801  0.7151468  8.666938

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 113 of 224 using max all svmPoly 4 
Support Vector Machines with Polynomial Kernel 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 154, 156, 156, 157, 154, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.99878  0.5392072  16.225013
  1       0.001  0.50  16.48354  0.5658387  13.977860
  1       0.001  1.00  14.84303  0.5938545  11.763080
  1       0.010  0.25  14.01006  0.6178565  10.636465
  1       0.010  0.50  13.61872  0.6292104  10.262201
  1       0.010  1.00  13.35750  0.6392216  10.093013
  1       0.100  0.25  12.93717  0.6604584   9.826315
  1       0.100  0.50  12.48482  0.6846180   9.580054
  1       0.100  1.00  12.07956  0.7049788   9.339573
  2       0.001  0.25  16.47177  0.5662314  13.963252
  2       0.001  0.50  14.82127  0.5951949  11.741032
  2       0.001  1.00  14.09020  0.6171394  10.736154
  2       0.010  0.25  13.03617  0.6601316   9.839402
  2       0.010  0.50  12.36628  0.6906154   9.301585
  2       0.010  1.00  11.71097  0.7204743   8.788901
  2       0.100  0.25  11.29339  0.7373470   8.200909
  2       0.100  0.50  11.22558  0.7406943   8.112400
  2       0.100  1.00  11.54112  0.7291419   8.333725
  3       0.001  0.25  15.36437  0.5797284  12.461948
  3       0.001  0.50  14.31264  0.6117705  11.043933
  3       0.001  1.00  13.76710  0.6274697  10.416264
  3       0.010  0.25  12.29719  0.6954396   9.233672
  3       0.010  0.50  11.64813  0.7229622   8.675058
  3       0.010  1.00  11.35126  0.7354433   8.352057
  3       0.100  0.25  14.77946  0.6289639  10.032050
  3       0.100  0.50  16.29475  0.5911931  11.056940
  3       0.100  1.00  18.56218  0.5451552  12.292804

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 114 of 224 using same all svmPoly 4 
Support Vector Machines with Polynomial Kernel 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 154, 155, 156, 155, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.94524  0.5412701  16.483021
  1       0.001  0.50  16.36422  0.5710887  13.975402
  1       0.001  1.00  14.86623  0.5987056  11.752389
  1       0.010  0.25  14.00488  0.6239355  10.592383
  1       0.010  0.50  13.68220  0.6333462  10.259599
  1       0.010  1.00  13.40034  0.6450180  10.016473
  1       0.100  0.25  12.90827  0.6697720   9.686712
  1       0.100  0.50  12.43702  0.6952602   9.434257
  1       0.100  1.00  12.07607  0.7143956   9.228313
  2       0.001  0.25  16.35172  0.5717830  13.960603
  2       0.001  0.50  14.84981  0.5998754  11.733095
  2       0.001  1.00  14.10534  0.6222518  10.712589
  2       0.010  0.25  13.03030  0.6665701   9.799481
  2       0.010  0.50  12.41991  0.6954510   9.328327
  2       0.010  1.00  11.82809  0.7240060   8.882182
  2       0.100  0.25  11.41871  0.7442149   8.355607
  2       0.100  0.50  11.42001  0.7443608   8.300282
  2       0.100  1.00  11.69199  0.7335115   8.500359
  3       0.001  0.25  15.34747  0.5855756  12.438264
  3       0.001  0.50  14.33431  0.6159304  10.987430
  3       0.001  1.00  13.77893  0.6334787  10.377626
  3       0.010  0.25  12.34438  0.7008543   9.267955
  3       0.010  0.50  11.79534  0.7255364   8.841073
  3       0.010  1.00  11.42891  0.7424879   8.492592
  3       0.100  0.25  13.25557  0.6854714   9.126240
  3       0.100  0.50  14.39420  0.6544780   9.919717
  3       0.100  1.00  16.00729  0.6096848  10.864107

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 115 of 224 using max du svmPoly 4 
Support Vector Machines with Polynomial Kernel 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 154, 156, 157, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  20.03519  0.5150276  17.066316
  1       0.001  0.50  17.76236  0.5471747  15.242852
  1       0.001  1.00  15.68775  0.5695425  12.916127
  1       0.010  0.25  14.46502  0.6016339  11.093494
  1       0.010  0.50  14.05691  0.6116803  10.621684
  1       0.010  1.00  13.72581  0.6247143  10.300440
  1       0.100  0.25  13.35759  0.6423872  10.025042
  1       0.100  0.50  13.00802  0.6610915   9.855206
  1       0.100  1.00  12.65701  0.6796055   9.682955
  2       0.001  0.25  17.75750  0.5475145  15.239003
  2       0.001  0.50  15.67808  0.5702889  12.904412
  2       0.001  1.00  14.62738  0.5987988  11.415214
  2       0.010  0.25  13.56320  0.6382273  10.173719
  2       0.010  0.50  13.00660  0.6630621   9.726183
  2       0.010  1.00  12.42194  0.6913462   9.264855
  2       0.100  0.25  11.48927  0.7319695   8.368322
  2       0.100  0.50  11.48895  0.7325230   8.303682
  2       0.100  1.00  11.46834  0.7318470   8.266271
  3       0.001  0.25  16.49434  0.5635688  13.952402
  3       0.001  0.50  14.90228  0.5902626  11.809420
  3       0.001  1.00  14.25000  0.6096379  10.822646
  3       0.010  0.25  12.95146  0.6679171   9.674009
  3       0.010  0.50  12.34534  0.6961714   9.224135
  3       0.010  1.00  11.78427  0.7212562   8.752318
  3       0.100  0.25  13.54721  0.6517265   9.376598
  3       0.100  0.50  14.49953  0.6246305  10.035999
  3       0.100  1.00  16.06804  0.5832804  11.172513

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 116 of 224 using same du svmPoly 4 
Support Vector Machines with Polynomial Kernel 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 156, 156, 156, 155, 154, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  20.17980  0.5183671  17.459135
  1       0.001  0.50  17.75197  0.5476174  15.438952
  1       0.001  1.00  15.73329  0.5701416  12.809706
  1       0.010  0.25  14.58554  0.6012473  11.058972
  1       0.010  0.50  14.17283  0.6124338  10.640970
  1       0.010  1.00  13.86321  0.6240740  10.340219
  1       0.100  0.25  13.44012  0.6449196   9.970459
  1       0.100  0.50  13.11697  0.6630920   9.789789
  1       0.100  1.00  12.69656  0.6850042   9.582337
  2       0.001  0.25  17.74810  0.5479063  15.435720
  2       0.001  0.50  15.72239  0.5709154  12.798466
  2       0.001  1.00  14.74383  0.5986509  11.310374
  2       0.010  0.25  13.67604  0.6386637  10.222557
  2       0.010  0.50  13.10921  0.6632448   9.745801
  2       0.010  1.00  12.56518  0.6897788   9.306044
  2       0.100  0.25  11.78110  0.7277417   8.652641
  2       0.100  0.50  11.83457  0.7260513   8.592807
  2       0.100  1.00  11.90393  0.7233020   8.626623
  3       0.001  0.25  16.39165  0.5640501  13.922057
  3       0.001  0.50  15.00355  0.5916353  11.832941
  3       0.001  1.00  14.34059  0.6099378  10.827079
  3       0.010  0.25  13.04431  0.6684219   9.686706
  3       0.010  0.50  12.50225  0.6944598   9.298791
  3       0.010  1.00  11.89834  0.7221183   8.845149
  3       0.100  0.25  13.26215  0.6799106   9.068514
  3       0.100  0.50  14.11201  0.6553413   9.620267
  3       0.100  1.00  15.49877  0.6234725  10.618489

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 117 of 224 using max rms svmPoly 4 
Support Vector Machines with Polynomial Kernel 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 156, 156, 155, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.67880  0.4967598  18.923549
  1       0.001  0.50  21.70696  0.5004382  18.316229
  1       0.001  1.00  20.34835  0.5362315  17.246584
  1       0.010  0.25  17.08769  0.5807494  14.539543
  1       0.010  0.50  14.82577  0.6131417  11.860796
  1       0.010  1.00  13.84277  0.6430143  10.637609
  1       0.100  0.25  13.25321  0.6563546  10.086565
  1       0.100  0.50  13.03012  0.6615855   9.911233
  1       0.100  1.00  12.98585  0.6624969   9.911267
  2       0.001  0.25  21.70642  0.5003359  18.316151
  2       0.001  0.50  20.34787  0.5362595  17.246518
  2       0.001  1.00  18.07449  0.5734509  15.395441
  2       0.010  0.25  14.81064  0.6138496  11.841146
  2       0.010  0.50  13.82738  0.6443944  10.638506
  2       0.010  1.00  13.34128  0.6558741  10.163283
  2       0.100  0.25  12.32331  0.6977292   9.326440
  2       0.100  0.50  11.91800  0.7162318   8.961750
  2       0.100  1.00  11.73601  0.7258044   8.732052
  3       0.001  0.25  20.99412  0.5190707  17.782030
  3       0.001  0.50  19.17264  0.5607725  16.304129
  3       0.001  1.00  16.33708  0.5891041  13.813175
  3       0.010  0.25  14.11526  0.6365419  10.978365
  3       0.010  0.50  13.48024  0.6533719  10.291723
  3       0.010  1.00  13.00381  0.6665604   9.879735
  3       0.100  0.25  11.77297  0.7237994   8.697714
  3       0.100  0.50  11.50940  0.7352086   8.419097
  3       0.100  1.00  11.30941  0.7431605   8.188886

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 118 of 224 using same rms svmPoly 4 
Support Vector Machines with Polynomial Kernel 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 156, 157, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.85820  0.5050834  19.485203
  1       0.001  0.50  21.99625  0.5117334  18.796907
  1       0.001  1.00  20.49090  0.5438860  17.650583
  1       0.010  0.25  17.03289  0.5830281  14.717743
  1       0.010  0.50  14.88325  0.6174637  11.946093
  1       0.010  1.00  14.00288  0.6423357  10.790564
  1       0.100  0.25  13.39260  0.6563549  10.218789
  1       0.100  0.50  13.17635  0.6613735  10.073267
  1       0.100  1.00  13.12091  0.6628099  10.050088
  2       0.001  0.25  21.99579  0.5116652  18.796854
  2       0.001  0.50  20.49021  0.5438928  17.650490
  2       0.001  1.00  18.00713  0.5752309  15.620350
  2       0.010  0.25  14.86816  0.6184447  11.934338
  2       0.010  0.50  13.97961  0.6439216  10.790026
  2       0.010  1.00  13.44812  0.6563916  10.256004
  2       0.100  0.25  12.40829  0.6998514   9.443595
  2       0.100  0.50  12.07977  0.7158823   9.172877
  2       0.100  1.00  11.84478  0.7274319   8.934233
  3       0.001  0.25  21.20016  0.5304916  18.222473
  3       0.001  0.50  19.17743  0.5638639  16.599668
  3       0.001  1.00  16.31573  0.5897373  13.928567
  3       0.010  0.25  14.24539  0.6357634  11.127130
  3       0.010  0.50  13.62231  0.6533256  10.410878
  3       0.010  1.00  13.15594  0.6656346  10.046977
  3       0.100  0.25  11.89904  0.7257625   8.895068
  3       0.100  0.50  11.58892  0.7393898   8.560864
  3       0.100  1.00  11.40845  0.7474541   8.347844

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 119 of 224 using max hudgins svmPoly 4 
Support Vector Machines with Polynomial Kernel 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 157, 155, 155, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  21.18980  0.4904706  17.970076
  1       0.001  0.50  19.58066  0.5308784  16.722036
  1       0.001  1.00  17.23733  0.5549484  14.732773
  1       0.010  0.25  14.99145  0.5858874  11.902326
  1       0.010  0.50  14.40935  0.6001590  10.927556
  1       0.010  1.00  14.09783  0.6082369  10.599771
  1       0.100  0.25  13.68176  0.6273565  10.244323
  1       0.100  0.50  13.35932  0.6438739  10.038391
  1       0.100  1.00  13.03547  0.6624219   9.887720
  2       0.001  0.25  19.57959  0.5308819  16.722296
  2       0.001  0.50  17.23312  0.5551748  14.727971
  2       0.001  1.00  15.41484  0.5761566  12.519136
  2       0.010  0.25  14.22636  0.6099179  10.756372
  2       0.010  0.50  13.68134  0.6304019  10.236018
  2       0.010  1.00  13.14205  0.6544221   9.808039
  2       0.100  0.25  11.65816  0.7240526   8.651872
  2       0.100  0.50  11.55794  0.7282625   8.510580
  2       0.100  1.00  11.45977  0.7324291   8.373627
  3       0.001  0.25  18.21330  0.5454985  15.626235
  3       0.001  0.50  16.02012  0.5692635  13.371922
  3       0.001  1.00  14.82648  0.5900549  11.635530
  3       0.010  0.25  13.71122  0.6320876  10.247402
  3       0.010  0.50  13.03867  0.6610403   9.704165
  3       0.010  1.00  12.49814  0.6863033   9.280392
  3       0.100  0.25  12.33449  0.6928359   8.980848
  3       0.100  0.50  12.90514  0.6709296   9.235083
  3       0.100  1.00  13.74189  0.6480826   9.729618

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 120 of 224 using same hudgins svmPoly 4 
Support Vector Machines with Polynomial Kernel 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  21.46483  0.5009066  18.455752
  1       0.001  0.50  19.66178  0.5330666  17.099969
  1       0.001  1.00  17.22095  0.5574020  14.932235
  1       0.010  0.25  15.00533  0.5916817  11.883153
  1       0.010  0.50  14.34250  0.6070208  10.834955
  1       0.010  1.00  14.07727  0.6142593  10.571385
  1       0.100  0.25  13.62768  0.6334477  10.187035
  1       0.100  0.50  13.34884  0.6491414  10.026501
  1       0.100  1.00  12.90210  0.6717258   9.756204
  2       0.001  0.25  19.65957  0.5330192  17.099296
  2       0.001  0.50  17.21712  0.5575696  14.928151
  2       0.001  1.00  15.38164  0.5802514  12.377100
  2       0.010  0.25  14.12534  0.6185938  10.690505
  2       0.010  0.50  13.60399  0.6380282  10.216148
  2       0.010  1.00  13.05425  0.6624860   9.772093
  2       0.100  0.25  11.69574  0.7292225   8.817837
  2       0.100  0.50  11.50232  0.7375324   8.549998
  2       0.100  1.00  11.50683  0.7371746   8.462913
  3       0.001  0.25  18.16607  0.5469054  15.845417
  3       0.001  0.50  15.96233  0.5704191  13.334469
  3       0.001  1.00  14.77546  0.5985319  11.562354
  3       0.010  0.25  13.57599  0.6427230  10.188647
  3       0.010  0.50  12.96213  0.6683036   9.688420
  3       0.010  1.00  12.45743  0.6928834   9.318304
  3       0.100  0.25  12.21244  0.7083280   8.857521
  3       0.100  0.50  12.51635  0.6997422   8.909722
  3       0.100  1.00  13.17063  0.6825023   9.261176

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 121 of 224 using max all ranger 4 
Random Forest 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 155, 156, 156, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.38692  0.7755121  6.755789
   2    extratrees  10.80944  0.7670128  7.594250
  13    variance    10.32955  0.7729831  6.264906
  13    extratrees  10.38998  0.7756832  6.829242
  24    variance    10.40515  0.7686410  6.265691
  24    extratrees  10.25949  0.7801604  6.616943

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 122 of 224 using same all ranger 4 
Random Forest 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 155, 156, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.27886  0.7873168  6.645906
   2    extratrees  10.62071  0.7842964  7.501301
  13    variance    10.29824  0.7813257  6.250916
  13    extratrees  10.21601  0.7916650  6.692435
  24    variance    10.39863  0.7772083  6.293903
  24    extratrees  10.08638  0.7960304  6.475552

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 123 of 224 using max du ranger 4 
Random Forest 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 156, 155, 154, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.41477  0.7732094  6.801423
   2    extratrees  10.97974  0.7597342  7.794669
  10    variance    10.34070  0.7701406  6.233674
  10    extratrees  10.53715  0.7692422  7.041539
  18    variance    10.32593  0.7697123  6.153935
  18    extratrees  10.38046  0.7750784  6.793987

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = variance
 and min.node.size = 5.


Now processing model 124 of 224 using same du ranger 4 
Random Forest 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 156, 156, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.18830  0.7894810  6.727256
   2    extratrees  10.76543  0.7775636  7.755282
  10    variance    10.01130  0.7907878  6.126506
  10    extratrees  10.31339  0.7865551  6.938868
  18    variance    10.06234  0.7879186  6.106540
  18    extratrees  10.17041  0.7913156  6.723232

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 125 of 224 using max rms ranger 4 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 156, 155, 155, 155, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    11.30996  0.7363596  6.809859
  2     extratrees  11.13631  0.7504862  7.326527
  3     variance    11.48151  0.7273849  6.807231
  3     extratrees  11.18890  0.7446530  7.221725

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 126 of 224 using same rms ranger 4 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 157, 155, 155, 154, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    10.88915  0.7535690  6.474543
  2     extratrees  10.61072  0.7751716  7.050301
  3     variance    11.23808  0.7384048  6.543097
  3     extratrees  10.72548  0.7676975  6.917301

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 127 of 224 using max hudgins ranger 4 
Random Forest 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 156, 156, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.48709  0.7784878  7.040716
   2    extratrees  11.15933  0.7592757  8.079913
   7    variance    10.29238  0.7764727  6.455800
   7    extratrees  10.63193  0.7724116  7.293370
  12    variance    10.27786  0.7755103  6.333254
  12    extratrees  10.44341  0.7785813  7.030486

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 128 of 224 using same hudgins ranger 4 
Random Forest 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance    10.195295  0.7931445  6.924862
   2    extratrees  10.913775  0.7744528  8.017213
   7    variance     9.895403  0.7957032  6.249538
   7    extratrees  10.309762  0.7895804  7.156799
  12    variance     9.864315  0.7958065  6.110308
  12    extratrees  10.129009  0.7949439  6.903648

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 129 of 224 using max all lm 5 
Linear Regression 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 156, 156, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.63701  0.7259031  9.025727

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 130 of 224 using same all lm 5 
Linear Regression 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 154, 157, 156, 155, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.20751  0.7051998  9.655853

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 131 of 224 using max du lm 5 
Linear Regression 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 155, 157, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.53397  0.7278744  9.112478

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 132 of 224 using same du lm 5 
Linear Regression 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 157, 154, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.00423  0.7186879  9.574574

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 133 of 224 using max rms lm 5 
Linear Regression 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 157, 157, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.39261  0.6759582  9.764048

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 134 of 224 using same rms lm 5 
Linear Regression 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 155, 156, 156, 156, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  12.94928  0.664064  10.10087

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 135 of 224 using max hudgins lm 5 
Linear Regression 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 154, 157, 156, 156, 155, 154, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.69829  0.7196616  9.482937

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 136 of 224 using same hudgins lm 5 
Linear Regression 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 156, 156, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.05295  0.7050188  9.812869

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 137 of 224 using max all knn 5 
k-Nearest Neighbors 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 156, 155, 156, 155, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.45114  0.7660317  6.168133
  7  10.69111  0.7542628  6.834419
  9  10.54282  0.7636887  7.148537

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 138 of 224 using same all knn 5 
k-Nearest Neighbors 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 155, 157, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.77511  0.7500908  6.805539
  7  10.98511  0.7456612  7.261941
  9  10.91470  0.7499935  7.524243

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 139 of 224 using max du knn 5 
k-Nearest Neighbors 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 157, 156, 154, 157, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.31807  0.7607824  6.163848
  7  10.52706  0.7532876  6.819139
  9  10.44750  0.7589302  7.211988

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 140 of 224 using same du knn 5 
k-Nearest Neighbors 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.72799  0.7558843  6.761275
  7  10.75308  0.7548586  7.156461
  9  10.67359  0.7601811  7.361258

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 141 of 224 using max rms knn 5 
k-Nearest Neighbors 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 154, 155, 156, 155, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.939039  0.7799272  5.901189
  7  10.204593  0.7677413  6.254159
  9  10.276338  0.7658929  6.644119

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 142 of 224 using same rms knn 5 
k-Nearest Neighbors 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 157, 157, 155, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.11305  0.7807812  6.124579
  7  10.19599  0.7778244  6.531040
  9  10.59013  0.7618839  7.042153

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 143 of 224 using max hudgins knn 5 
k-Nearest Neighbors 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 157, 156, 155, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  12.01869  0.6932204  8.044135
  7  11.62666  0.7104699  8.154378
  9  11.47464  0.7161973  8.428755

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 144 of 224 using same hudgins knn 5 
k-Nearest Neighbors 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 157, 155, 155, 155, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  12.94380  0.6606962  8.774452
  7  12.70313  0.6702665  8.963097
  9  12.46653  0.6826241  9.102109

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 145 of 224 using max all svmPoly 5 
Support Vector Machines with Polynomial Kernel 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 155, 157, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.79456  0.5476462  15.775826
  1       0.001  0.50  16.29724  0.5766128  13.728287
  1       0.001  1.00  14.41599  0.6039663  11.444048
  1       0.010  0.25  13.50944  0.6267685  10.273812
  1       0.010  0.50  13.16415  0.6373336   9.940037
  1       0.010  1.00  12.88032  0.6491304   9.757043
  1       0.100  0.25  12.48317  0.6698430   9.504557
  1       0.100  0.50  12.07197  0.6930646   9.283603
  1       0.100  1.00  11.67090  0.7144960   9.066648
  2       0.001  0.25  16.28489  0.5771859  13.713293
  2       0.001  0.50  14.39230  0.6055919  11.425856
  2       0.001  1.00  13.60710  0.6250494  10.382206
  2       0.010  0.25  12.52214  0.6734236   9.531881
  2       0.010  0.50  11.88228  0.7042136   9.047310
  2       0.010  1.00  11.20415  0.7362876   8.465824
  2       0.100  0.25  10.80869  0.7534222   7.707782
  2       0.100  0.50  10.93686  0.7478082   7.773475
  2       0.100  1.00  11.11427  0.7403496   7.887555
  3       0.001  0.25  15.01855  0.5935144  12.284503
  3       0.001  0.50  13.85710  0.6207348  10.773914
  3       0.001  1.00  13.28398  0.6367974  10.099184
  3       0.010  0.25  11.75285  0.7121501   8.886717
  3       0.010  0.50  11.09038  0.7415597   8.302330
  3       0.010  1.00  10.71694  0.7593283   7.843916
  3       0.100  0.25  11.65734  0.7251936   8.265766
  3       0.100  0.50  12.54772  0.7111306   8.788398
  3       0.100  1.00  15.28778  0.6576331  10.194558

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 146 of 224 using same all svmPoly 5 
Support Vector Machines with Polynomial Kernel 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 155, 157, 156, 155, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.86564  0.5322499  16.378036
  1       0.001  0.50  16.55291  0.5641619  14.093431
  1       0.001  1.00  14.98552  0.5935912  11.990785
  1       0.010  0.25  14.05264  0.6167199  10.765445
  1       0.010  0.50  13.75233  0.6265882  10.509386
  1       0.010  1.00  13.40025  0.6418793  10.264800
  1       0.100  0.25  12.92970  0.6678339   9.933458
  1       0.100  0.50  12.53715  0.6899999   9.695220
  1       0.100  1.00  12.19219  0.7086013   9.491544
  2       0.001  0.25  16.54613  0.5642095  14.085376
  2       0.001  0.50  14.96648  0.5947354  11.971545
  2       0.001  1.00  14.14510  0.6151012  10.888692
  2       0.010  0.25  13.11701  0.6619010  10.060171
  2       0.010  0.50  12.49248  0.6922174   9.579809
  2       0.010  1.00  11.85490  0.7237307   9.048145
  2       0.100  0.25  11.46545  0.7401811   8.401279
  2       0.100  0.50  11.54952  0.7353028   8.426746
  2       0.100  1.00  11.75977  0.7279163   8.536830
  3       0.001  0.25  15.44789  0.5804015  12.600062
  3       0.001  0.50  14.36543  0.6102581  11.130767
  3       0.001  1.00  13.84092  0.6266734  10.591516
  3       0.010  0.25  12.42836  0.6972198   9.513401
  3       0.010  0.50  11.80956  0.7262200   8.955811
  3       0.010  1.00  11.39047  0.7448917   8.469377
  3       0.100  0.25  12.73660  0.7033082   9.202150
  3       0.100  0.50  12.88944  0.6965013   9.476364
  3       0.100  1.00  14.99884  0.6431468  10.470134

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 147 of 224 using max du svmPoly 5 
Support Vector Machines with Polynomial Kernel 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 155, 156, 156, 154, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  19.80505  0.5273032  16.596587
  1       0.001  0.50  17.64126  0.5579370  14.909378
  1       0.001  1.00  15.42005  0.5837999  12.774356
  1       0.010  0.25  13.99701  0.6099759  10.829071
  1       0.010  0.50  13.57634  0.6214605  10.213854
  1       0.010  1.00  13.32466  0.6316344  10.004052
  1       0.100  0.25  12.91354  0.6504753   9.736911
  1       0.100  0.50  12.58541  0.6687455   9.556410
  1       0.100  1.00  12.07392  0.6945802   9.285991
  2       0.001  0.25  17.63849  0.5581661  14.908172
  2       0.001  0.50  15.40799  0.5846825  12.761978
  2       0.001  1.00  14.18570  0.6059436  11.097695
  2       0.010  0.25  13.13014  0.6476384   9.942371
  2       0.010  0.50  12.53712  0.6744734   9.472976
  2       0.010  1.00  11.89784  0.7049865   8.992863
  2       0.100  0.25  10.82253  0.7522562   7.764861
  2       0.100  0.50  10.91565  0.7481066   7.762527
  2       0.100  1.00  11.04408  0.7425372   7.853891
  3       0.001  0.25  16.27785  0.5748826  13.699653
  3       0.001  0.50  14.49500  0.5992598  11.496535
  3       0.001  1.00  13.82637  0.6168868  10.533328
  3       0.010  0.25  12.42990  0.6825729   9.362508
  3       0.010  0.50  11.78271  0.7117678   8.881158
  3       0.010  1.00  11.09881  0.7426490   8.296659
  3       0.100  0.25  12.32994  0.7001117   8.494201
  3       0.100  0.50  12.52626  0.6944985   8.847794
  3       0.100  1.00  13.60028  0.6676649   9.541926

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 148 of 224 using same du svmPoly 5 
Support Vector Machines with Polynomial Kernel 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 155, 157, 155, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  20.00897  0.5050543  17.258597
  1       0.001  0.50  17.76518  0.5361168  15.344762
  1       0.001  1.00  15.80007  0.5663760  13.045160
  1       0.010  0.25  14.54314  0.5952593  11.143213
  1       0.010  0.50  14.14140  0.6084645  10.712436
  1       0.010  1.00  13.89382  0.6186807  10.507802
  1       0.100  0.25  13.43173  0.6427062  10.161693
  1       0.100  0.50  12.98043  0.6674077   9.867583
  1       0.100  1.00  12.63584  0.6841851   9.690180
  2       0.001  0.25  17.76421  0.5360664  15.344552
  2       0.001  0.50  15.79125  0.5668597  13.034517
  2       0.001  1.00  14.77337  0.5914473  11.574309
  2       0.010  0.25  13.72814  0.6332107  10.425544
  2       0.010  0.50  13.13336  0.6592013   9.972802
  2       0.010  1.00  12.52989  0.6876378   9.490454
  2       0.100  0.25  11.57831  0.7327176   8.387395
  2       0.100  0.50  11.61961  0.7291509   8.419071
  2       0.100  1.00  11.81294  0.7209502   8.528791
  3       0.001  0.25  16.54496  0.5553535  14.041095
  3       0.001  0.50  15.07256  0.5844286  12.024129
  3       0.001  1.00  14.34916  0.6031474  10.910213
  3       0.010  0.25  13.10331  0.6628292   9.928149
  3       0.010  0.50  12.50094  0.6889351   9.464866
  3       0.010  1.00  11.92309  0.7171208   8.944533
  3       0.100  0.25  12.61810  0.6894881   9.191218
  3       0.100  0.50  12.98101  0.6803740   9.408825
  3       0.100  1.00  14.22705  0.6475005  10.104050

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 149 of 224 using max rms svmPoly 5 
Support Vector Machines with Polynomial Kernel 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 155, 155, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.42261  0.5271986  18.262137
  1       0.001  0.50  21.52307  0.5271986  17.703638
  1       0.001  1.00  20.14689  0.5436601  16.743508
  1       0.010  0.25  17.03757  0.5938034  14.273134
  1       0.010  0.50  14.55062  0.6237386  11.673652
  1       0.010  1.00  13.51369  0.6503333  10.464463
  1       0.100  0.25  12.85992  0.6637033   9.831913
  1       0.100  0.50  12.68435  0.6670590   9.741441
  1       0.100  1.00  12.62281  0.6681808   9.728513
  2       0.001  0.25  21.52260  0.5271374  17.703609
  2       0.001  0.50  20.14663  0.5436238  16.743397
  2       0.001  1.00  18.01913  0.5827625  15.060656
  2       0.010  0.25  14.53699  0.6251630  11.660735
  2       0.010  0.50  13.48497  0.6522889  10.448437
  2       0.010  1.00  12.95484  0.6641799   9.937465
  2       0.100  0.25  11.86581  0.7101853   9.091033
  2       0.100  0.50  11.44569  0.7300598   8.698597
  2       0.100  1.00  11.18615  0.7404884   8.266980
  3       0.001  0.25  20.74358  0.5333736  17.219428
  3       0.001  0.50  18.99939  0.5671912  15.864064
  3       0.001  1.00  16.20831  0.5990352  13.573291
  3       0.010  0.25  13.75717  0.6437901  10.744216
  3       0.010  0.50  13.11204  0.6625173  10.075198
  3       0.010  1.00  12.63357  0.6737767   9.685898
  3       0.100  0.25  11.30145  0.7372896   8.433101
  3       0.100  0.50  11.06951  0.7458565   8.057756
  3       0.100  1.00  10.89492  0.7519163   7.740633

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 150 of 224 using same rms svmPoly 5 
Support Vector Machines with Polynomial Kernel 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 154, 156, 155, 156, 156, 155, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.15726  0.4890903  19.131843
  1       0.001  0.50  21.61986  0.4901602  18.526292
  1       0.001  1.00  20.35870  0.5275151  17.481978
  1       0.010  0.25  17.06622  0.5676537  14.673735
  1       0.010  0.50  15.05844  0.6066721  12.163145
  1       0.010  1.00  13.96936  0.6369003  10.870204
  1       0.100  0.25  13.38139  0.6508408  10.318314
  1       0.100  0.50  13.12224  0.6564553  10.151356
  1       0.100  1.00  13.04886  0.6583689  10.145059
  2       0.001  0.25  21.61955  0.4900115  18.525982
  2       0.001  0.50  20.35800  0.5274864  17.481888
  2       0.001  1.00  17.96458  0.5601907  15.586950
  2       0.010  0.25  15.03802  0.6073844  12.138982
  2       0.010  0.50  13.95782  0.6379090  10.872749
  2       0.010  1.00  13.43991  0.6514366  10.366215
  2       0.100  0.25  12.48694  0.6929798   9.605742
  2       0.100  0.50  12.09755  0.7107114   9.232610
  2       0.100  1.00  11.91308  0.7194854   8.988463
  3       0.001  0.25  20.98681  0.5089538  17.975116
  3       0.001  0.50  19.06921  0.5434556  16.493918
  3       0.001  1.00  16.44416  0.5734447  13.952741
  3       0.010  0.25  14.26989  0.6294522  11.235702
  3       0.010  0.50  13.61803  0.6479035  10.527193
  3       0.010  1.00  13.13398  0.6607079  10.107999
  3       0.100  0.25  11.94563  0.7185646   9.015116
  3       0.100  0.50  11.67362  0.7292368   8.657530
  3       0.100  1.00  11.46019  0.7374537   8.339126

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 151 of 224 using max hudgins svmPoly 5 
Support Vector Machines with Polynomial Kernel 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 154, 156, 155, 156, 157, 155, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  20.93977  0.5029398  17.378714
  1       0.001  0.50  19.33325  0.5311583  16.271193
  1       0.001  1.00  17.04794  0.5566113  14.438267
  1       0.010  0.25  14.58419  0.5897569  11.589304
  1       0.010  0.50  13.96532  0.6057130  10.628890
  1       0.010  1.00  13.72024  0.6125804  10.286234
  1       0.100  0.25  13.32303  0.6295102   9.995579
  1       0.100  0.50  12.98397  0.6466047   9.783762
  1       0.100  1.00  12.63864  0.6643531   9.577331
  2       0.001  0.25  19.33173  0.5311394  16.271036
  2       0.001  0.50  17.04490  0.5569093  14.435372
  2       0.001  1.00  15.08358  0.5822828  12.323069
  2       0.010  0.25  13.78200  0.6174493  10.479032
  2       0.010  0.50  13.31117  0.6359119  10.022814
  2       0.010  1.00  12.72003  0.6621108   9.561801
  2       0.100  0.25  10.87628  0.7499506   7.973177
  2       0.100  0.50  10.84593  0.7498070   7.836366
  2       0.100  1.00  10.84939  0.7485931   7.767228
  3       0.001  0.25  18.03648  0.5456926  15.232292
  3       0.001  0.50  15.82359  0.5752307  13.204332
  3       0.001  1.00  14.36445  0.5959028  11.281997
  3       0.010  0.25  13.25404  0.6419871  10.033003
  3       0.010  0.50  12.60263  0.6710467   9.449156
  3       0.010  1.00  11.94142  0.7017760   8.982907
  3       0.100  0.25  11.30755  0.7310705   8.071164
  3       0.100  0.50  11.87172  0.7159399   8.293810
  3       0.100  1.00  13.50184  0.6831344   9.063245

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 152 of 224 using same hudgins svmPoly 5 
Support Vector Machines with Polynomial Kernel 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 154, 156, 156, 155, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  21.12504  0.4782544  18.133983
  1       0.001  0.50  19.49549  0.5157814  16.873296
  1       0.001  1.00  17.35158  0.5458326  14.904776
  1       0.010  0.25  15.11746  0.5778804  12.068562
  1       0.010  0.50  14.50205  0.5909677  11.013457
  1       0.010  1.00  14.20093  0.6016899  10.721839
  1       0.100  0.25  13.80143  0.6190942  10.434973
  1       0.100  0.50  13.47902  0.6368967  10.238494
  1       0.100  1.00  13.10244  0.6575189  10.067206
  2       0.001  0.25  19.49424  0.5154894  16.871925
  2       0.001  0.50  17.34948  0.5458477  14.902591
  2       0.001  1.00  15.54674  0.5692554  12.692669
  2       0.010  0.25  14.28596  0.6040997  10.856532
  2       0.010  0.50  13.84743  0.6224836  10.512535
  2       0.010  1.00  13.25277  0.6491015  10.073904
  2       0.100  0.25  11.48761  0.7373810   8.513148
  2       0.100  0.50  11.46137  0.7377795   8.461496
  2       0.100  1.00  11.54899  0.7324916   8.494126
  3       0.001  0.25  18.12298  0.5307303  15.682469
  3       0.001  0.50  16.08993  0.5604262  13.481507
  3       0.001  1.00  14.90860  0.5834402  11.773568
  3       0.010  0.25  13.80498  0.6284658  10.514527
  3       0.010  0.50  13.15196  0.6568325   9.968132
  3       0.010  1.00  12.52062  0.6883709   9.552870
  3       0.100  0.25  11.81802  0.7213343   8.851064
  3       0.100  0.50  12.12862  0.7112215   9.024804
  3       0.100  1.00  13.32311  0.6784006   9.678588

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 153 of 224 using max all ranger 5 
Random Forest 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 157, 157, 155, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.996592  0.7797876  6.291465
   2    extratrees  10.098524  0.7879592  7.016951
  13    variance    10.136749  0.7701659  5.998398
  13    extratrees   9.941015  0.7839047  6.382673
  24    variance    10.135083  0.7698247  6.016624
  24    extratrees   9.976336  0.7809267  6.298023

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 154 of 224 using same all ranger 5 
Random Forest 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 156, 155, 155, 155, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance    10.056187  0.7903368  6.604118
   2    extratrees  10.353851  0.7901233  7.383643
  13    variance    10.085017  0.7814352  6.184933
  13    extratrees  10.047379  0.7904467  6.712395
  24    variance    10.053180  0.7822651  6.178165
  24    extratrees   9.998584  0.7904689  6.563150

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 155 of 224 using max du ranger 5 
Random Forest 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 155, 155, 155, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.943829  0.7798865  6.404059
   2    extratrees  10.151246  0.7824581  7.226856
  10    variance     9.888095  0.7755003  5.965871
  10    extratrees   9.947647  0.7808361  6.592047
  18    variance     9.898022  0.7747929  5.965950
  18    extratrees   9.921401  0.7796996  6.447823

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 156 of 224 using same du ranger 5 
Random Forest 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 155, 156, 157, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.37156  0.7764316  6.810917
   2    extratrees  10.73942  0.7709962  7.749885
  10    variance    10.38376  0.7685291  6.360701
  10    extratrees  10.56327  0.7678108  7.117782
  18    variance    10.33901  0.7700914  6.322168
  18    extratrees  10.45356  0.7721139  6.930832

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = variance
 and min.node.size = 5.


Now processing model 157 of 224 using max rms ranger 5 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 155, 156, 155, 156, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
  2     variance    10.055782  0.7714894  6.148082
  2     extratrees   9.806100  0.7908174  6.556557
  3     variance    10.306131  0.7596783  6.140380
  3     extratrees   9.876992  0.7837040  6.458340

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 158 of 224 using same rms ranger 5 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 154, 157, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    10.45944  0.7667403  6.573677
  2     extratrees  10.23005  0.7847334  6.959004
  3     variance    10.80239  0.7504873  6.595326
  3     extratrees  10.31607  0.7782911  6.898616

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 159 of 224 using max hudgins ranger 5 
Random Forest 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 155, 155, 156, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance    10.112448  0.7789649  6.665595
   2    extratrees  10.453752  0.7765379  7.579350
   7    variance    10.004673  0.7749990  6.169497
   7    extratrees  10.107396  0.7796897  6.898928
  12    variance    10.081457  0.7704457  6.127829
  12    extratrees   9.989506  0.7828676  6.698137

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 160 of 224 using same hudgins ranger 5 
Random Forest 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 157, 157, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.28028  0.7832101  6.952723
   2    extratrees  10.91220  0.7670881  8.012969
   7    variance    10.23006  0.7775392  6.387781
   7    extratrees  10.54044  0.7732461  7.341528
  12    variance    10.19399  0.7791331  6.263431
  12    extratrees  10.39091  0.7783488  7.103431

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 161 of 224 using max all lm 6 
Linear Regression 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 156, 157, 156, 154, 156, ... 
Resampling results:

  RMSE     Rsquared  MAE     
  11.5683  0.72704   9.040159

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 162 of 224 using same all lm 6 
Linear Regression 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 155, 155, 157, 156, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.05318  0.7148272  9.645888

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 163 of 224 using max du lm 6 
Linear Regression 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 158, 156, 156, 156, 156, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.54525  0.7287405  9.135703

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 164 of 224 using same du lm 6 
Linear Regression 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 155, 155, 156, 157, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.86073  0.7162432  9.669275

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 165 of 224 using max rms lm 6 
Linear Regression 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 157, 155, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.48167  0.6805795  9.901779

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 166 of 224 using same rms lm 6 
Linear Regression 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 156, 156, 155, ... 
Resampling results:

  RMSE     Rsquared  MAE     
  13.0427  0.656712  10.30496

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 167 of 224 using max hudgins lm 6 
Linear Regression 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 156, 156, 155, 155, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.81776  0.7163855  9.600636

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 168 of 224 using same hudgins lm 6 
Linear Regression 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 157, 155, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.00117  0.7161081  9.747549

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 169 of 224 using max all knn 6 
k-Nearest Neighbors 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 154, 156, 155, 157, 156, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.954399  0.7883193  6.218705
  7   9.960739  0.7883088  6.722835
  9  10.090579  0.7827793  6.999797

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 170 of 224 using same all knn 6 
k-Nearest Neighbors 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 155, 156, 157, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.12745  0.7823172  6.189808
  7  10.27241  0.7777385  6.799189
  9  10.71336  0.7597853  7.467164

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 171 of 224 using max du knn 6 
k-Nearest Neighbors 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 154, 155, 156, 158, 155, 157, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.970618  0.7849165  6.312057
  7   9.870690  0.7904710  6.732811
  9  10.001741  0.7847714  7.014366

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 172 of 224 using same du knn 6 
k-Nearest Neighbors 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.11107  0.7830727  6.215488
  7  10.40983  0.7734356  6.887023
  9  10.66275  0.7639241  7.453422

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 173 of 224 using max rms knn 6 
k-Nearest Neighbors 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 158, 154, 157, 155, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.532396  0.7998599  5.899121
  7  9.607579  0.7983352  6.219023
  9  9.458662  0.8057487  6.488839

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 174 of 224 using same rms knn 6 
k-Nearest Neighbors 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 155, 155, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.895664  0.7911256  6.049684
  7  10.053736  0.7819065  6.328868
  9  10.281683  0.7742547  6.858607

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 175 of 224 using max hudgins knn 6 
k-Nearest Neighbors 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 155, 155, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  11.31100  0.7340106  7.860348
  7  11.51075  0.7212854  8.082790
  9  11.47020  0.7226892  8.294749

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 176 of 224 using same hudgins knn 6 
k-Nearest Neighbors 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 155, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  12.00191  0.7071693  8.196658
  7  12.16791  0.6965090  8.481627
  9  12.49419  0.6790372  9.055684

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 177 of 224 using max all svmPoly 6 
Support Vector Machines with Polynomial Kernel 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 157, 155, 156, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.83334  0.5544667  16.069783
  1       0.001  0.50  16.33201  0.5850837  13.819016
  1       0.001  1.00  14.61479  0.6079583  11.663799
  1       0.010  0.25  13.57776  0.6317086  10.539986
  1       0.010  0.50  13.28109  0.6419768  10.262053
  1       0.010  1.00  13.04193  0.6515662  10.100135
  1       0.100  0.25  12.51511  0.6800503   9.758737
  1       0.100  0.50  12.06121  0.7055688   9.443794
  1       0.100  1.00  11.64331  0.7266300   9.139923
  2       0.001  0.25  16.32122  0.5855451  13.807549
  2       0.001  0.50  14.59620  0.6094375  11.648100
  2       0.001  1.00  13.78458  0.6270908  10.724205
  2       0.010  0.25  12.53406  0.6857185   9.683025
  2       0.010  0.50  11.80864  0.7183881   9.147791
  2       0.010  1.00  11.23662  0.7465553   8.640611
  2       0.100  0.25  10.84563  0.7625374   7.898768
  2       0.100  0.50  10.86381  0.7606261   7.976184
  2       0.100  1.00  11.15941  0.7509010   8.231973
  3       0.001  0.25  15.11524  0.6009345  12.354034
  3       0.001  0.50  14.09552  0.6213089  11.046828
  3       0.001  1.00  13.32085  0.6429288  10.316220
  3       0.010  0.25  11.81200  0.7201873   9.141582
  3       0.010  0.50  11.25827  0.7443549   8.561949
  3       0.010  1.00  11.08792  0.7524569   8.152042
  3       0.100  0.25  13.35227  0.6947199   9.173615
  3       0.100  0.50  15.45430  0.6616046  10.168738
  3       0.100  1.00  19.85396  0.6097702  11.868022

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 178 of 224 using same all svmPoly 6 
Support Vector Machines with Polynomial Kernel 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.81962  0.5318624  16.273983
  1       0.001  0.50  16.45763  0.5596424  13.881393
  1       0.001  1.00  15.00051  0.5860666  11.919485
  1       0.010  0.25  14.06435  0.6097110  10.831200
  1       0.010  0.50  13.72722  0.6212020  10.520478
  1       0.010  1.00  13.44564  0.6342666  10.298123
  1       0.100  0.25  12.89402  0.6649764   9.920867
  1       0.100  0.50  12.44603  0.6896997   9.595203
  1       0.100  1.00  12.08898  0.7076287   9.410897
  2       0.001  0.25  16.45191  0.5597658  13.875691
  2       0.001  0.50  14.98246  0.5872085  11.904080
  2       0.001  1.00  14.23231  0.6056302  10.980088
  2       0.010  0.25  13.07894  0.6598830  10.034931
  2       0.010  0.50  12.51088  0.6879652   9.610483
  2       0.010  1.00  11.86663  0.7189734   9.025136
  2       0.100  0.25  11.20547  0.7422511   8.149370
  2       0.100  0.50  11.20574  0.7398262   8.213199
  2       0.100  1.00  11.37047  0.7341377   8.469121
  3       0.001  0.25  15.39616  0.5751278  12.427187
  3       0.001  0.50  14.48597  0.5985296  11.235193
  3       0.001  1.00  13.81953  0.6199665  10.641110
  3       0.010  0.25  12.44315  0.6926068   9.528152
  3       0.010  0.50  11.84800  0.7174273   8.943377
  3       0.010  1.00  11.46992  0.7320111   8.438481
  3       0.100  0.25  13.22258  0.6876064   9.136624
  3       0.100  0.50  13.94934  0.6706186   9.777293
  3       0.100  1.00  16.31224  0.6185866  10.971123

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 179 of 224 using max du svmPoly 6 
Support Vector Machines with Polynomial Kernel 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  19.91448  0.5296710  16.902491
  1       0.001  0.50  17.58536  0.5544177  15.040677
  1       0.001  1.00  15.49100  0.5849409  12.834764
  1       0.010  0.25  14.23760  0.6050213  11.103848
  1       0.010  0.50  13.63629  0.6203678  10.504250
  1       0.010  1.00  13.43363  0.6271037  10.346359
  1       0.100  0.25  12.98647  0.6483658  10.050106
  1       0.100  0.50  12.43323  0.6772644   9.665130
  1       0.100  1.00  12.03408  0.6978329   9.436618
  2       0.001  0.25  17.58283  0.5544749  15.039304
  2       0.001  0.50  15.48078  0.5856809  12.824639
  2       0.001  1.00  14.34876  0.6037473  11.318225
  2       0.010  0.25  13.08109  0.6516915  10.108911
  2       0.010  0.50  12.49204  0.6790528   9.639538
  2       0.010  1.00  11.82677  0.7085528   9.171200
  2       0.100  0.25  11.10203  0.7402823   8.153191
  2       0.100  0.50  11.26197  0.7342530   8.196634
  2       0.100  1.00  11.46211  0.7275496   8.374185
  3       0.001  0.25  16.33098  0.5778228  13.798756
  3       0.001  0.50  14.67831  0.5983538  11.735049
  3       0.001  1.00  13.92816  0.6150470  10.767254
  3       0.010  0.25  12.47276  0.6824277   9.626337
  3       0.010  0.50  11.78397  0.7100098   9.069105
  3       0.010  1.00  11.30341  0.7311941   8.542205
  3       0.100  0.25  11.75120  0.7254434   8.506218
  3       0.100  0.50  13.06078  0.6982820   9.186496
  3       0.100  1.00  18.27686  0.6286383  11.128546

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 180 of 224 using same du svmPoly 6 
Support Vector Machines with Polynomial Kernel 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 155, 157, 156, 155, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  20.01795  0.5152178  17.200565
  1       0.001  0.50  17.73380  0.5375279  15.256787
  1       0.001  1.00  15.75094  0.5637329  12.931585
  1       0.010  0.25  14.59888  0.5870561  11.248022
  1       0.010  0.50  14.08045  0.6017717  10.769788
  1       0.010  1.00  13.83664  0.6125142  10.537589
  1       0.100  0.25  13.39652  0.6352473  10.234969
  1       0.100  0.50  12.93105  0.6621946   9.941894
  1       0.100  1.00  12.51305  0.6850714   9.645751
  2       0.001  0.25  17.73265  0.5374694  15.257000
  2       0.001  0.50  15.74350  0.5643156  12.924149
  2       0.001  1.00  14.79117  0.5840976  11.555562
  2       0.010  0.25  13.66787  0.6284119  10.486759
  2       0.010  0.50  13.15316  0.6544869  10.054228
  2       0.010  1.00  12.51026  0.6862678   9.584819
  2       0.100  0.25  11.74836  0.7228680   8.543558
  2       0.100  0.50  11.70221  0.7230811   8.501942
  2       0.100  1.00  11.95484  0.7115722   8.753752
  3       0.001  0.25  16.45989  0.5550831  13.878317
  3       0.001  0.50  15.07516  0.5788212  11.947550
  3       0.001  1.00  14.36848  0.5955399  11.019088
  3       0.010  0.25  13.07536  0.6618602   9.993489
  3       0.010  0.50  12.41199  0.6925086   9.463142
  3       0.010  1.00  11.94650  0.7136621   8.971211
  3       0.100  0.25  12.32348  0.6981926   8.909110
  3       0.100  0.50  13.89739  0.6549621   9.851759
  3       0.100  1.00  16.75417  0.6100601  11.259312

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 181 of 224 using max rms svmPoly 6 
Support Vector Machines with Polynomial Kernel 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 154, 157, 156, 157, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.46426  0.5138710  18.588549
  1       0.001  0.50  21.47634  0.5196313  18.063342
  1       0.001  1.00  20.17599  0.5552290  17.105274
  1       0.010  0.25  16.99444  0.5961090  14.483769
  1       0.010  0.50  14.71047  0.6267546  11.891210
  1       0.010  1.00  13.70631  0.6516885  10.767205
  1       0.100  0.25  12.88599  0.6691123  10.038759
  1       0.100  0.50  12.73307  0.6741986   9.975369
  1       0.100  1.00  12.61911  0.6767869   9.977913
  2       0.001  0.25  21.47577  0.5195930  18.063455
  2       0.001  0.50  20.17557  0.5552167  17.105230
  2       0.001  1.00  17.99149  0.5870116  15.334354
  2       0.010  0.25  14.69087  0.6278371  11.871115
  2       0.010  0.50  13.68102  0.6530886  10.754651
  2       0.010  1.00  12.94272  0.6696811  10.086755
  2       0.100  0.25  11.86466  0.7187245   9.233523
  2       0.100  0.50  11.46309  0.7374754   8.781701
  2       0.100  1.00  11.31215  0.7440922   8.421648
  3       0.001  0.25  20.77665  0.5417521  17.603151
  3       0.001  0.50  19.07152  0.5714968  16.149069
  3       0.001  1.00  16.25176  0.6042143  13.751211
  3       0.010  0.25  13.97348  0.6464059  11.085378
  3       0.010  0.50  13.18203  0.6657915  10.292023
  3       0.010  1.00  12.70485  0.6790577   9.905334
  3       0.100  0.25  11.36271  0.7436876   8.582887
  3       0.100  0.50  11.16267  0.7516918   8.153076
  3       0.100  1.00  10.96485  0.7590947   7.898746

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 182 of 224 using same rms svmPoly 6 
Support Vector Machines with Polynomial Kernel 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 155, 157, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.31765  0.4883552  19.017990
  1       0.001  0.50  21.68662  0.4957632  18.403684
  1       0.001  1.00  20.37397  0.5360090  17.427417
  1       0.010  0.25  17.12611  0.5704504  14.657743
  1       0.010  0.50  15.09374  0.6044877  12.129017
  1       0.010  1.00  14.10501  0.6309045  11.014653
  1       0.100  0.25  13.44408  0.6453903  10.475486
  1       0.100  0.50  13.24217  0.6501314  10.348348
  1       0.100  1.00  13.17703  0.6521874  10.332688
  2       0.001  0.25  21.68598  0.4956527  18.403656
  2       0.001  0.50  20.37339  0.5359824  17.427437
  2       0.001  1.00  18.00861  0.5615658  15.527644
  2       0.010  0.25  15.08445  0.6051930  12.121480
  2       0.010  0.50  14.07982  0.6322373  10.994570
  2       0.010  1.00  13.53676  0.6452588  10.533336
  2       0.100  0.25  12.52465  0.6930408   9.654602
  2       0.100  0.50  12.01158  0.7135874   9.094059
  2       0.100  1.00  11.88502  0.7179378   8.805282
  3       0.001  0.25  20.99852  0.5205317  17.902348
  3       0.001  0.50  19.13941  0.5493676  16.451974
  3       0.001  1.00  16.47395  0.5787791  13.927327
  3       0.010  0.25  14.44671  0.6238373  11.398133
  3       0.010  0.50  13.72839  0.6428023  10.679970
  3       0.010  1.00  13.23599  0.6558440  10.303080
  3       0.100  0.25  11.91635  0.7190880   8.938632
  3       0.100  0.50  11.64610  0.7276462   8.529457
  3       0.100  1.00  11.44366  0.7344551   8.231494

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 183 of 224 using max hudgins svmPoly 6 
Support Vector Machines with Polynomial Kernel 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 157, 155, 156, 154, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  20.97247  0.5109324  17.760515
  1       0.001  0.50  19.38732  0.5428039  16.542561
  1       0.001  1.00  17.06159  0.5657675  14.534795
  1       0.010  0.25  14.78772  0.5955409  11.832664
  1       0.010  0.50  14.08768  0.6095673  10.898132
  1       0.010  1.00  13.71594  0.6202367  10.504837
  1       0.100  0.25  13.39235  0.6332796  10.295119
  1       0.100  0.50  13.04903  0.6515325  10.116262
  1       0.100  1.00  12.66367  0.6719857   9.932710
  2       0.001  0.25  19.38564  0.5427674  16.543175
  2       0.001  0.50  17.05941  0.5659821  14.533186
  2       0.001  1.00  15.13631  0.5920610  12.390970
  2       0.010  0.25  13.86690  0.6235056  10.718223
  2       0.010  0.50  13.25926  0.6466469  10.200477
  2       0.010  1.00  12.73860  0.6710235   9.833324
  2       0.100  0.25  11.06241  0.7489762   8.217490
  2       0.100  0.50  11.16770  0.7442743   8.179902
  2       0.100  1.00  11.43673  0.7362226   8.280832
  3       0.001  0.25  18.00776  0.5522292  15.440667
  3       0.001  0.50  15.76000  0.5858531  13.186696
  3       0.001  1.00  14.50705  0.6020069  11.490643
  3       0.010  0.25  13.24612  0.6517004  10.226999
  3       0.010  0.50  12.67563  0.6759123   9.789782
  3       0.010  1.00  11.91651  0.7095219   9.194659
  3       0.100  0.25  12.14255  0.7164804   8.682749
  3       0.100  0.50  11.94402  0.7189589   8.689056
  3       0.100  1.00  12.53192  0.7039856   9.032665

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 184 of 224 using same hudgins svmPoly 6 
Support Vector Machines with Polynomial Kernel 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 157, 156, 155, 155, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  21.15478  0.4878715  18.069667
  1       0.001  0.50  19.46497  0.5179572  16.784773
  1       0.001  1.00  17.23503  0.5393552  14.731782
  1       0.010  0.25  15.10772  0.5734494  11.953907
  1       0.010  0.50  14.50428  0.5875751  11.094826
  1       0.010  1.00  14.11406  0.5988729  10.686674
  1       0.100  0.25  13.77711  0.6152046  10.464778
  1       0.100  0.50  13.41896  0.6340569  10.233647
  1       0.100  1.00  12.97716  0.6590195   9.966918
  2       0.001  0.25  19.46308  0.5179530  16.784452
  2       0.001  0.50  17.23398  0.5394510  14.732172
  2       0.001  1.00  15.42273  0.5670988  12.434755
  2       0.010  0.25  14.32469  0.5996479  10.964968
  2       0.010  0.50  13.74371  0.6217550  10.442854
  2       0.010  1.00  13.26423  0.6461084  10.048205
  2       0.100  0.25  11.58160  0.7314447   8.516683
  2       0.100  0.50  11.51998  0.7338464   8.471897
  2       0.100  1.00  11.64146  0.7273451   8.573945
  3       0.001  0.25  18.08240  0.5318399  15.630598
  3       0.001  0.50  15.99381  0.5584870  13.302209
  3       0.001  1.00  14.88199  0.5800190  11.675980
  3       0.010  0.25  13.75585  0.6255291  10.495379
  3       0.010  0.50  13.21644  0.6513788  10.030738
  3       0.010  1.00  12.58856  0.6810936   9.575466
  3       0.100  0.25  12.50986  0.6983181   8.955579
  3       0.100  0.50  12.93940  0.6923802   9.013191
  3       0.100  1.00  13.92549  0.6682007   9.477424

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 185 of 224 using max all ranger 6 
Random Forest 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 157, 156, 157, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    9.365030  0.8150337  6.135119
   2    extratrees  9.649723  0.8162953  6.936308
  13    variance    9.489710  0.8041694  5.842949
  13    extratrees  9.537821  0.8081791  6.323444
  24    variance    9.546007  0.8018466  5.936388
  24    extratrees  9.498219  0.8079589  6.187560

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 186 of 224 using same all ranger 6 
Random Forest 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 154, 157, 156, 157, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.908397  0.7907103  6.502408
   2    extratrees  10.177171  0.7927945  7.301265
  13    variance    10.063664  0.7786646  6.260234
  13    extratrees   9.921075  0.7904861  6.604126
  24    variance    10.198328  0.7725758  6.356143
  24    extratrees   9.821094  0.7936633  6.438002

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 187 of 224 using max du ranger 6 
Random Forest 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 154, 155, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    9.382622  0.8104139  6.228447
   2    extratrees  9.716202  0.8098497  7.119781
  10    variance    9.363598  0.8049728  5.853549
  10    extratrees  9.540894  0.8040115  6.490752
  18    variance    9.345454  0.8054807  5.858010
  18    extratrees  9.497222  0.8037330  6.339937

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = variance
 and min.node.size = 5.


Now processing model 188 of 224 using same du ranger 6 
Random Forest 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 155, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.16245  0.7876535  6.672584
   2    extratrees  10.52994  0.7844238  7.602246
  10    variance    10.20995  0.7762837  6.319745
  10    extratrees  10.18731  0.7872341  6.860922
  18    variance    10.34562  0.7691320  6.388615
  18    extratrees  10.15903  0.7869320  6.713726

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 189 of 224 using max rms ranger 6 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 155, 157, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    9.755185  0.7967538  5.974938
  2     extratrees  9.651340  0.8082396  6.592274
  3     variance    9.886415  0.7910241  5.884073
  3     extratrees  9.735986  0.8016186  6.460677

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 190 of 224 using same rms ranger 6 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 154, 155, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    10.26374  0.7781082  6.403453
  2     extratrees  10.27026  0.7848289  6.972328
  3     variance    10.45635  0.7692532  6.334234
  3     extratrees  10.34661  0.7781235  6.881281

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 191 of 224 using max hudgins ranger 6 
Random Forest 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 157, 156, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.552581  0.8025406  6.513125
   2    extratrees  10.003159  0.7984217  7.474976
   7    variance     9.545011  0.7965498  6.118305
   7    extratrees   9.656985  0.8001322  6.762803
  12    variance     9.534913  0.7972554  6.098357
  12    extratrees   9.550435  0.8032101  6.580621

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 192 of 224 using same hudgins ranger 6 
Random Forest 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 155, 156, 157, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.37131  0.7770219  6.927844
   2    extratrees  10.73138  0.7770982  7.894848
   7    variance    10.20526  0.7746225  6.401190
   7    extratrees  10.31701  0.7805380  7.158957
  12    variance    10.22479  0.7730954  6.345602
  12    extratrees  10.26828  0.7808233  7.001905

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 193 of 224 using max all lm 7 
Linear Regression 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 155, 156, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.68796  0.7126319  9.714631

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 194 of 224 using same all lm 7 
Linear Regression 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 157, 157, 155, 157, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  13.73912  0.6736976  10.72353

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 195 of 224 using max du lm 7 
Linear Regression 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 155, 156, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.92781  0.7282544  9.474818

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 196 of 224 using same du lm 7 
Linear Regression 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 156, 156, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  13.12496  0.6870929  10.44017

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 197 of 224 using max rms lm 7 
Linear Regression 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 156, 156, 155, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.81489  0.6651203  10.19016

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 198 of 224 using same rms lm 7 
Linear Regression 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 154, 156, 156, 156, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  13.3373  0.6509933  10.61409

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 199 of 224 using max hudgins lm 7 
Linear Regression 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 156, 155, 156, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  11.97238  0.7166317  9.651963

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 200 of 224 using same hudgins lm 7 
Linear Regression 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 154, 155, 155, 157, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  12.71214  0.6842257  10.44173

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 201 of 224 using max all knn 7 
k-Nearest Neighbors 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 154, 156, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.04667  0.7823798  6.076382
  7  10.30790  0.7732329  6.670092
  9  10.62011  0.7620856  7.130117

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 202 of 224 using same all knn 7 
k-Nearest Neighbors 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 155, 156, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.923312  0.7931790  6.055139
  7  10.202605  0.7822070  6.676354
  9  10.728904  0.7627664  7.379251

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 203 of 224 using max du knn 7 
k-Nearest Neighbors 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 156, 156, 155, ... 
Resampling results across tuning parameters:

  k  RMSE       Rsquared   MAE     
  5   9.980347  0.7864594  6.029905
  7  10.222640  0.7788012  6.601981
  9  10.572853  0.7658284  7.093496

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 204 of 224 using same du knn 7 
k-Nearest Neighbors 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 155, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.15168  0.7853028  6.157354
  7  10.35890  0.7785224  6.816115
  9  10.78918  0.7633247  7.439326

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 205 of 224 using max rms knn 7 
k-Nearest Neighbors 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 155, 156, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  9.764442  0.7960370  5.682151
  7  9.712772  0.7983867  6.000086
  9  9.798307  0.7965380  6.357297

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 206 of 224 using same rms knn 7 
k-Nearest Neighbors 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 154, 157, 156, 156, 155, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  10.43494  0.7724496  6.253535
  7  10.51962  0.7695954  6.710082
  9  10.62111  0.7656371  7.111617

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 207 of 224 using max hudgins knn 7 
k-Nearest Neighbors 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 155, 156, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  11.91948  0.7084521  7.861538
  7  12.04631  0.7019355  8.353258
  9  12.19657  0.6966824  8.594465

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 208 of 224 using same hudgins knn 7 
k-Nearest Neighbors 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 157, 157, 155, 156, 155, 155, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  12.09726  0.7003415  8.004800
  7  12.09419  0.7001129  8.356762
  9  12.26225  0.6969388  8.874976

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 209 of 224 using max all svmPoly 7 
Support Vector Machines with Polynomial Kernel 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 155, 156, 155, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.90609  0.5346797  16.184051
  1       0.001  0.50  16.46638  0.5671893  13.807316
  1       0.001  1.00  14.93137  0.5970936  11.818592
  1       0.010  0.25  13.96910  0.6212054  10.689163
  1       0.010  0.50  13.58010  0.6314433  10.421299
  1       0.010  1.00  13.18309  0.6474930  10.146982
  1       0.100  0.25  12.74044  0.6719422   9.872516
  1       0.100  0.50  12.32974  0.6947128   9.624139
  1       0.100  1.00  11.82553  0.7195877   9.244117
  2       0.001  0.25  16.46266  0.5674790  13.804590
  2       0.001  0.50  14.92038  0.5978535  11.814013
  2       0.001  1.00  14.11959  0.6184544  10.853892
  2       0.010  0.25  12.98390  0.6659351   9.955920
  2       0.010  0.50  12.37282  0.6929538   9.481290
  2       0.010  1.00  11.77010  0.7214093   8.962897
  2       0.100  0.25  11.14028  0.7447123   8.014945
  2       0.100  0.50  11.06119  0.7475163   7.862658
  2       0.100  1.00  11.11074  0.7469895   7.766854
  3       0.001  0.25  15.35712  0.5853298  12.368616
  3       0.001  0.50  14.42532  0.6109191  11.174524
  3       0.001  1.00  13.76112  0.6297438  10.537081
  3       0.010  0.25  12.32804  0.6967967   9.378574
  3       0.010  0.50  11.76785  0.7215031   8.856147
  3       0.010  1.00  11.34550  0.7389267   8.330603
  3       0.100  0.25  12.37985  0.7053350   8.432441
  3       0.100  0.50  13.23288  0.6738968   9.071133
  3       0.100  1.00  15.10615  0.6227191  10.120888

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 210 of 224 using same all svmPoly 7 
Support Vector Machines with Polynomial Kernel 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 155, 156, 155, 157, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  18.73337  0.5241757  16.266160
  1       0.001  0.50  16.54181  0.5537366  13.836053
  1       0.001  1.00  15.26476  0.5890373  12.095341
  1       0.010  0.25  14.35151  0.6132647  11.070831
  1       0.010  0.50  14.02668  0.6210062  10.880333
  1       0.010  1.00  13.72945  0.6328844  10.715304
  1       0.100  0.25  13.28459  0.6556006  10.387064
  1       0.100  0.50  12.79088  0.6797603  10.020276
  1       0.100  1.00  12.44253  0.6970749   9.754994
  2       0.001  0.25  16.54099  0.5536657  13.834385
  2       0.001  0.50  15.25655  0.5893919  12.089794
  2       0.001  1.00  14.50379  0.6093960  11.202941
  2       0.010  0.25  13.51787  0.6501288  10.502581
  2       0.010  0.50  12.96476  0.6750298  10.076425
  2       0.010  1.00  12.43589  0.6994583   9.547183
  2       0.100  0.25  12.10897  0.7126816   8.727656
  2       0.100  0.50  12.72157  0.6930627   8.911229
  2       0.100  1.00  13.17271  0.6843743   9.038277
  3       0.001  0.25  15.63505  0.5773754  12.619489
  3       0.001  0.50  14.76501  0.6020123  11.428093
  3       0.001  1.00  14.18800  0.6201492  10.955222
  3       0.010  0.25  12.87219  0.6806602   9.920532
  3       0.010  0.50  12.37633  0.7015032   9.392362
  3       0.010  1.00  12.11816  0.7115512   8.974549
  3       0.100  0.25  16.70957  0.6226442  10.386569
  3       0.100  0.50  18.36611  0.5878982  11.303814
  3       0.100  1.00  20.43467  0.5352713  12.705428

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 211 of 224 using max du svmPoly 7 
Support Vector Machines with Polynomial Kernel 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 155, 156, 155, 155, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  20.02930  0.5116469  17.085109
  1       0.001  0.50  17.74884  0.5423814  15.177260
  1       0.001  1.00  15.74658  0.5700773  12.866643
  1       0.010  0.25  14.54612  0.5987457  11.162151
  1       0.010  0.50  14.01569  0.6124688  10.656936
  1       0.010  1.00  13.66489  0.6247924  10.421891
  1       0.100  0.25  13.18576  0.6489182  10.103899
  1       0.100  0.50  12.84887  0.6678038   9.920708
  1       0.100  1.00  12.47504  0.6889498   9.694058
  2       0.001  0.25  17.74717  0.5423762  15.177106
  2       0.001  0.50  15.73825  0.5705455  12.857518
  2       0.001  1.00  14.74548  0.5953459  11.478714
  2       0.010  0.25  13.64338  0.6359848  10.395397
  2       0.010  0.50  13.04901  0.6611398   9.945112
  2       0.010  1.00  12.44678  0.6910756   9.478177
  2       0.100  0.25  11.36719  0.7427751   8.241196
  2       0.100  0.50  11.24807  0.7461398   8.100251
  2       0.100  1.00  11.26931  0.7444178   8.087536
  3       0.001  0.25  16.39529  0.5593098  13.696737
  3       0.001  0.50  15.03985  0.5879954  11.836765
  3       0.001  1.00  14.29876  0.6075824  10.904865
  3       0.010  0.25  12.99151  0.6674945   9.893743
  3       0.010  0.50  12.40599  0.6935439   9.366138
  3       0.010  1.00  11.83963  0.7201605   8.860069
  3       0.100  0.25  11.93396  0.7141771   8.378541
  3       0.100  0.50  12.41813  0.6964220   8.696758
  3       0.100  1.00  13.59367  0.6660121   9.525867

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 212 of 224 using same du svmPoly 7 
Support Vector Machines with Polynomial Kernel 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 157, 156, 155, 157, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  19.92784  0.5016259  17.291496
  1       0.001  0.50  17.85406  0.5321748  15.339665
  1       0.001  1.00  15.92192  0.5629682  12.969478
  1       0.010  0.25  14.85034  0.5894897  11.425000
  1       0.010  0.50  14.41445  0.6018483  11.110051
  1       0.010  1.00  14.07632  0.6132382  10.887038
  1       0.100  0.25  13.65368  0.6363180  10.590376
  1       0.100  0.50  13.23330  0.6586883  10.292246
  1       0.100  1.00  12.76565  0.6836444   9.904726
  2       0.001  0.25  17.85432  0.5317205  15.339713
  2       0.001  0.50  15.91606  0.5631152  12.961495
  2       0.001  1.00  15.04557  0.5856140  11.646997
  2       0.010  0.25  14.05751  0.6229852  10.843513
  2       0.010  0.50  13.50173  0.6479436  10.442528
  2       0.010  1.00  12.93165  0.6766432   9.958447
  2       0.100  0.25  12.11733  0.7155264   8.739008
  2       0.100  0.50  12.39569  0.7052778   8.867990
  2       0.100  1.00  13.19621  0.6893881   9.179532
  3       0.001  0.25  16.53572  0.5480096  13.794299
  3       0.001  0.50  15.30390  0.5796634  12.072323
  3       0.001  1.00  14.62309  0.5985175  11.257557
  3       0.010  0.25  13.43738  0.6533110  10.340622
  3       0.010  0.50  12.84557  0.6810270   9.827068
  3       0.010  1.00  12.43021  0.6989296   9.349567
  3       0.100  0.25  16.05476  0.6304876  10.228506
  3       0.100  0.50  17.60897  0.6038477  10.876640
  3       0.100  1.00  18.89797  0.5750591  11.684895

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 213 of 224 using max rms svmPoly 7 
Support Vector Machines with Polynomial Kernel 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 156, 156, 155, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.78069  0.5008024  18.771518
  1       0.001  0.50  21.77091  0.5009399  18.250191
  1       0.001  1.00  20.36865  0.5301029  17.316616
  1       0.010  0.25  17.18518  0.5741686  14.667233
  1       0.010  0.50  14.94474  0.6057674  11.947089
  1       0.010  1.00  14.03374  0.6320502  10.897198
  1       0.100  0.25  13.27591  0.6512095  10.300900
  1       0.100  0.50  12.99928  0.6586128  10.137765
  1       0.100  1.00  12.92628  0.6588352  10.127935
  2       0.001  0.25  21.76988  0.5008265  18.250352
  2       0.001  0.50  20.36814  0.5300523  17.316487
  2       0.001  1.00  18.10616  0.5596786  15.457255
  2       0.010  0.25  14.93555  0.6062584  11.940308
  2       0.010  0.50  14.01182  0.6331674  10.883675
  2       0.010  1.00  13.33659  0.6507455  10.318029
  2       0.100  0.25  12.19806  0.7008844   9.433398
  2       0.100  0.50  11.75557  0.7194793   9.003621
  2       0.100  1.00  11.52415  0.7281731   8.564025
  3       0.001  0.25  20.99677  0.5171460  17.788504
  3       0.001  0.50  19.19856  0.5453552  16.373411
  3       0.001  1.00  16.47904  0.5801332  13.860641
  3       0.010  0.25  14.33578  0.6249844  11.258128
  3       0.010  0.50  13.52275  0.6467936  10.455906
  3       0.010  1.00  13.06264  0.6610372  10.138813
  3       0.100  0.25  11.67181  0.7234351   8.811695
  3       0.100  0.50  11.35853  0.7339246   8.292956
  3       0.100  1.00  11.21243  0.7392220   7.997233

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 214 of 224 using same rms svmPoly 7 
Support Vector Machines with Polynomial Kernel 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 154, 156, 156, 155, 156, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  22.39954  0.4896623  19.318321
  1       0.001  0.50  21.79355  0.4901154  18.704554
  1       0.001  1.00  20.37598  0.5259686  17.624860
  1       0.010  0.25  17.24429  0.5623619  14.770044
  1       0.010  0.50  15.30986  0.5938677  12.277489
  1       0.010  1.00  14.40398  0.6192691  11.195277
  1       0.100  0.25  13.69712  0.6374860  10.694875
  1       0.100  0.50  13.51553  0.6419038  10.623418
  1       0.100  1.00  13.45423  0.6404422  10.630160
  2       0.001  0.25  21.79329  0.4899230  18.704068
  2       0.001  0.50  20.37508  0.5259251  17.624592
  2       0.001  1.00  17.92201  0.5485733  15.478317
  2       0.010  0.25  15.30336  0.5943935  12.273920
  2       0.010  0.50  14.38412  0.6200931  11.183713
  2       0.010  1.00  13.80166  0.6357772  10.747457
  2       0.100  0.25  12.88529  0.6736700  10.045616
  2       0.100  0.50  12.52501  0.6886031   9.658304
  2       0.100  1.00  12.41274  0.6934657   9.372012
  3       0.001  0.25  21.09176  0.5093762  18.138451
  3       0.001  0.50  19.07057  0.5408308  16.571040
  3       0.001  1.00  16.63347  0.5703595  14.035138
  3       0.010  0.25  14.67462  0.6132082  11.522810
  3       0.010  0.50  13.99478  0.6309358  10.887408
  3       0.010  1.00  13.50504  0.6456509  10.562039
  3       0.100  0.25  12.45083  0.6929170   9.530204
  3       0.100  0.50  12.20797  0.7023728   9.036011
  3       0.100  1.00  12.09794  0.7066350   8.824820

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 215 of 224 using max hudgins svmPoly 7 
Support Vector Machines with Polynomial Kernel 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 155, 155, 156, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  21.16754  0.4927818  17.931264
  1       0.001  0.50  19.47099  0.5177219  16.687297
  1       0.001  1.00  17.21542  0.5491029  14.615585
  1       0.010  0.25  15.05264  0.5864659  11.853035
  1       0.010  0.50  14.41062  0.6017033  10.980761
  1       0.010  1.00  13.96861  0.6136636  10.614122
  1       0.100  0.25  13.56599  0.6289099  10.344607
  1       0.100  0.50  13.28390  0.6441690  10.173450
  1       0.100  1.00  12.92889  0.6637383   9.966058
  2       0.001  0.25  19.46823  0.5175501  16.686789
  2       0.001  0.50  17.21461  0.5492475  14.615742
  2       0.001  1.00  15.38426  0.5776898  12.364058
  2       0.010  0.25  14.19480  0.6144690  10.821446
  2       0.010  0.50  13.63369  0.6332628  10.368941
  2       0.010  1.00  13.07939  0.6563384   9.924229
  2       0.100  0.25  11.52575  0.7324835   8.555124
  2       0.100  0.50  11.40315  0.7372816   8.350622
  2       0.100  1.00  11.38766  0.7354138   8.199857
  3       0.001  0.25  18.09023  0.5365232  15.546756
  3       0.001  0.50  15.93023  0.5662186  13.156932
  3       0.001  1.00  14.84167  0.5925791  11.614891
  3       0.010  0.25  13.66872  0.6360470  10.389893
  3       0.010  0.50  13.00555  0.6621847   9.862339
  3       0.010  1.00  12.32446  0.6937801   9.289794
  3       0.100  0.25  11.61639  0.7246671   8.403396
  3       0.100  0.50  11.78375  0.7175498   8.468191
  3       0.100  1.00  12.38336  0.6939916   8.860061

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 216 of 224 using same hudgins svmPoly 7 
Support Vector Machines with Polynomial Kernel 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 155, 156, 157, 156, 155, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared   MAE      
  1       0.001  0.25  21.17267  0.4837440  18.279445
  1       0.001  0.50  19.32492  0.5117706  16.791542
  1       0.001  1.00  17.24777  0.5405298  14.644178
  1       0.010  0.25  15.34693  0.5787931  12.102677
  1       0.010  0.50  14.73821  0.5940242  11.341311
  1       0.010  1.00  14.36882  0.6031065  11.078693
  1       0.100  0.25  14.04880  0.6150508  10.878097
  1       0.100  0.50  13.79804  0.6286936  10.742525
  1       0.100  1.00  13.46897  0.6466621  10.523453
  2       0.001  0.25  19.32242  0.5112945  16.789895
  2       0.001  0.50  17.24725  0.5402550  14.643035
  2       0.001  1.00  15.66771  0.5710399  12.561000
  2       0.010  0.25  14.59191  0.6031959  11.230576
  2       0.010  0.50  14.02435  0.6220179  10.830043
  2       0.010  1.00  13.56346  0.6435907  10.468629
  2       0.100  0.25  12.26997  0.7064864   9.145065
  2       0.100  0.50  12.40788  0.7035084   9.056270
  2       0.100  1.00  13.25010  0.6777298   9.324755
  3       0.001  0.25  18.14225  0.5311097  15.653109
  3       0.001  0.50  16.16766  0.5610541  13.328540
  3       0.001  1.00  15.19718  0.5831548  11.887574
  3       0.010  0.25  14.09203  0.6228928  10.875755
  3       0.010  0.50  13.46460  0.6496849  10.363873
  3       0.010  1.00  12.87488  0.6777704   9.858200
  3       0.100  0.25  14.39794  0.6547355   9.750176
  3       0.100  0.50  16.26398  0.6330822  10.405526
  3       0.100  1.00  18.80270  0.5977855  11.361945

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 217 of 224 using max all ranger 7 
Random Forest 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    9.341486  0.8152589  6.233329
   2    extratrees  9.878125  0.8045335  7.044157
  13    variance    9.306996  0.8118039  5.892263
  13    extratrees  9.349346  0.8128273  6.194611
  24    variance    9.525851  0.8030727  6.010630
  24    extratrees  9.326775  0.8130804  6.058550

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = variance
 and min.node.size = 5.


Now processing model 218 of 224 using same all ranger 7 
Random Forest 

173 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 156, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.45688  0.7754745  7.005147
   2    extratrees  10.66458  0.7767315  7.636108
  13    variance    10.81505  0.7543459  6.986383
  13    extratrees  10.39288  0.7767920  6.976788
  24    variance    11.05996  0.7438528  7.203959
  24    extratrees  10.38806  0.7759096  6.868274

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 219 of 224 using max du ranger 7 
Random Forest 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 155, 156, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.745329  0.8030337  6.489355
   2    extratrees  10.238949  0.7922929  7.337115
  10    variance     9.536386  0.8056904  6.030404
  10    extratrees   9.762740  0.7999395  6.530866
  18    variance     9.723868  0.7973347  6.146869
  18    extratrees   9.659932  0.8037133  6.356202

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 220 of 224 using same du ranger 7 
Random Forest 

173 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 155, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.51220  0.7762251  7.137655
   2    extratrees  10.80965  0.7749544  7.831647
  10    variance    10.67776  0.7641730  6.981279
  10    extratrees  10.52786  0.7742103  7.196061
  18    variance    10.89592  0.7550736  7.130664
  18    extratrees  10.49190  0.7738733  7.078718

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 221 of 224 using max rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 155, 156, 156, 156, 156, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    9.432576  0.8052195  5.842906
  2     extratrees  9.913765  0.7989208  6.651465
  3     variance    9.573052  0.7981347  5.714198
  3     extratrees  9.863035  0.7964981  6.475922

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 222 of 224 using same rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

173 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 155, 156, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    10.36046  0.7776254  6.663708
  2     extratrees  10.54853  0.7778696  7.286708
  3     variance    10.45129  0.7729351  6.544121
  3     extratrees  10.56413  0.7728842  7.193153

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 223 of 224 using max hudgins ranger 7 
Random Forest 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 156, 156, 156, 155, 155, 155, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE       Rsquared   MAE     
   2    variance     9.886983  0.8049227  6.741342
   2    extratrees  10.505368  0.7903980  7.686308
   7    variance     9.557711  0.8103271  6.209750
   7    extratrees   9.922449  0.8013772  6.861955
  12    variance     9.657667  0.8055271  6.215342
  12    extratrees   9.695889  0.8095938  6.613085

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 224 of 224 using same hudgins ranger 7 
Random Forest 

173 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 154, 157, 156, 157, 156, 156, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    10.71949  0.7715684  7.441879
   2    extratrees  11.08338  0.7678808  8.196817
   7    variance    10.69403  0.7646826  7.068453
   7    extratrees  10.61971  0.7742470  7.455843
  12    variance    10.89996  0.7548788  7.149666
  12    extratrees  10.53337  0.7770577  7.291871

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.
