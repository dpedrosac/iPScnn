Processing  33 subjects



Now processing model 1 of 224 using max all lm 1 
Linear Regression 

6426 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5783, 5782, 5784, 5783, 5783, 5784, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  20.5267  0.1039683  16.21283

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 2 of 224 using same all lm 1 
Linear Regression 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 827, 827, 827, 826, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.72386  0.1041718  16.20875

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 3 of 224 using max du lm 1 
Linear Regression 

6426 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5784, 5783, 5783, 5784, 5783, 5784, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.58871  0.09835296  16.23436

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 4 of 224 using same du lm 1 
Linear Regression 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 827, 826, 825, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.80871  0.09386304  16.30455

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 5 of 224 using max rms lm 1 
Linear Regression 

6426 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5784, 5783, 5783, 5782, 5784, 5784, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.00254  0.06170862  16.81468

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 6 of 224 using same rms lm 1 
Linear Regression 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 825, 825, 825, 827, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.04287  0.07169549  16.76552

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 7 of 224 using max hudgins lm 1 
Linear Regression 

6426 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5782, 5783, 5785, 5784, 5782, 5783, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.77005  0.08245584  16.52738

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 8 of 224 using same hudgins lm 1 
Linear Regression 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 828, 825, 828, 825, 826, 826, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.05316  0.07265849  16.72223

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 9 of 224 using max all knn 1 
k-Nearest Neighbors 

6426 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5782, 5783, 5785, 5784, 5783, 5784, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.72616  0.3481426  11.73110
  7  17.45103  0.3587650  11.81089
  9  17.40399  0.3586447  11.92293

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 10 of 224 using same all knn 1 
k-Nearest Neighbors 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 825, 826, 826, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.04784  0.1966479  14.25040
  7  19.75296  0.2038877  14.18687
  9  19.64654  0.2048787  14.19674

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 11 of 224 using max du knn 1 
k-Nearest Neighbors 

6426 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5783, 5782, 5784, 5784, 5783, 5785, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.71915  0.3486553  11.72036
  7  17.47604  0.3570945  11.82226
  9  17.42233  0.3573063  11.93197

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 12 of 224 using same du knn 1 
k-Nearest Neighbors 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 826, 827, 827, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.01074  0.1989464  14.25467
  7  19.85392  0.1974559  14.25141
  9  19.75361  0.1968336  14.24651

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 13 of 224 using max rms knn 1 
k-Nearest Neighbors 

6426 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5783, 5784, 5784, 5784, 5783, 5783, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.14331  0.3862457  11.23575
  7  16.93509  0.3942435  11.31856
  9  16.91761  0.3929886  11.46420

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 14 of 224 using same rms knn 1 
k-Nearest Neighbors 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 825, 828, 826, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.76632  0.2096634  13.93543
  7  19.51818  0.2144756  13.97396
  9  19.54244  0.2082018  14.12370

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 15 of 224 using max hudgins knn 1 
k-Nearest Neighbors 

6426 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5785, 5785, 5783, 5784, 5783, 5783, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.05183  0.1203755  15.26983
  7  20.56183  0.1353389  15.04929
  9  20.39101  0.1389776  15.00246

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 16 of 224 using same hudgins knn 1 
k-Nearest Neighbors 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 827, 825, 827, 826, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared    MAE     
  5  22.69083  0.04900772  16.93313
  7  22.06911  0.05808423  16.55887
  9  21.72136  0.06699903  16.33589

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 17 of 224 using max all svmPoly 1 
Support Vector Machines with Polynomial Kernel 

6426 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5783, 5784, 5783, 5783, 5785, 5784, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.71677  0.05212398  16.29769
  1       0.001  0.50  21.62524  0.06112139  16.17872
  1       0.001  1.00  21.47321  0.07005611  16.06679
  1       0.010  0.25  21.22814  0.07915824  15.93405
  1       0.010  0.50  21.10942  0.08284025  15.86621
  1       0.010  1.00  21.09501  0.08491371  15.83326
  1       0.100  0.25  21.09186  0.08702684  15.80278
  1       0.100  0.50  21.09956  0.08758520  15.79422
  1       0.100  1.00  21.11259  0.08816483  15.78636
  2       0.001  0.25  21.43725  0.07759280  16.02722
  2       0.001  0.50  21.17866  0.09164711  15.86465
  2       0.001  1.00  20.91696  0.10195418  15.71416
  2       0.010  0.25  20.55782  0.12670112  15.21546
  2       0.010  0.50  20.45468  0.13420677  15.02592
  2       0.010  1.00  20.34921  0.14202546  14.86402
  2       0.100  0.25  19.94255  0.17338990  14.33681
  2       0.100  0.50  19.88959  0.17766194  14.24529
  2       0.100  1.00  19.85702  0.18075187  14.17242
  3       0.001  0.25  21.15745  0.09705048  15.85353
  3       0.001  0.50  20.88329  0.10641176  15.69877
  3       0.001  1.00  20.78636  0.11287092  15.57582
  3       0.010  0.25  20.33805  0.14412550  14.81339
  3       0.010  0.50  20.24058  0.15215458  14.68800
  3       0.010  1.00  20.12798  0.16015024  14.56135
  3       0.100  0.25  20.09427  0.18083536  14.02050
  3       0.100  0.50  20.23082  0.18032845  14.00669
  3       0.100  1.00  20.58252  0.17326594  14.08633

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 18 of 224 using same all svmPoly 1 
Support Vector Machines with Polynomial Kernel 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 825, 826, 826, 828, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.04801  0.05635362  16.45712
  1       0.001  0.50  21.98773  0.05350174  16.37731
  1       0.001  1.00  21.92639  0.05361487  16.30168
  1       0.010  0.25  21.83037  0.06293414  16.18556
  1       0.010  0.50  21.72392  0.07122740  16.08870
  1       0.010  1.00  21.56907  0.07764356  16.01462
  1       0.100  0.25  21.39663  0.08171154  15.95030
  1       0.100  0.50  21.32201  0.08536368  15.88577
  1       0.100  1.00  21.27691  0.08889518  15.83110
  2       0.001  0.25  21.97074  0.05700028  16.34101
  2       0.001  0.50  21.87630  0.06145290  16.24609
  2       0.001  1.00  21.73744  0.07179428  16.11724
  2       0.010  0.25  21.03242  0.11010861  15.69705
  2       0.010  0.50  20.93934  0.11123325  15.61624
  2       0.010  1.00  20.92844  0.11171687  15.57347
  2       0.100  0.25  20.82903  0.12767196  15.32650
  2       0.100  0.50  20.98572  0.12542735  15.38540
  2       0.100  1.00  21.20772  0.12510069  15.44675
  3       0.001  0.25  21.88411  0.06496542  16.24326
  3       0.001  0.50  21.71570  0.07526177  16.10938
  3       0.001  1.00  21.50891  0.08865977  15.94612
  3       0.010  0.25  20.81126  0.12088147  15.46846
  3       0.010  0.50  20.74801  0.12443597  15.33703
  3       0.010  1.00  20.71928  0.12837545  15.26052
  3       0.100  0.25  26.94014  0.07756061  17.19178
  3       0.100  0.50  30.19478  0.06527327  18.12430
  3       0.100  1.00  33.00711  0.05412123  19.04417

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 19 of 224 using max du svmPoly 1 
Support Vector Machines with Polynomial Kernel 

6426 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5783, 5784, 5784, 5783, 5782, 5784, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.85550  0.04404697  16.35364
  1       0.001  0.50  21.72106  0.05352710  16.23868
  1       0.001  1.00  21.60688  0.06360445  16.12123
  1       0.010  0.25  21.38290  0.07416396  15.99690
  1       0.010  0.50  21.22568  0.07957849  15.93682
  1       0.010  1.00  21.10955  0.08235084  15.90225
  1       0.100  0.25  21.07811  0.08298650  15.87207
  1       0.100  0.50  21.09383  0.08248558  15.86812
  1       0.100  1.00  21.10535  0.08213765  15.86630
  2       0.001  0.25  21.63786  0.06238153  16.16072
  2       0.001  0.50  21.46209  0.07649104  16.00787
  2       0.001  1.00  21.22251  0.08870554  15.87885
  2       0.010  0.25  20.68358  0.11987202  15.48933
  2       0.010  0.50  20.59286  0.12500619  15.32854
  2       0.010  1.00  20.48359  0.13088819  15.17115
  2       0.100  0.25  20.18494  0.15443483  14.72899
  2       0.100  0.50  20.15205  0.15789314  14.65696
  2       0.100  1.00  20.11640  0.16142787  14.57716
  3       0.001  0.25  21.47021  0.07832376  16.01537
  3       0.001  0.50  21.22701  0.09219845  15.87387
  3       0.001  1.00  20.96803  0.10259726  15.75207
  3       0.010  0.25  20.42441  0.13704386  15.07572
  3       0.010  0.50  20.33200  0.14233493  14.94724
  3       0.010  1.00  20.24881  0.14830206  14.83744
  3       0.100  0.25  20.28631  0.16366117  14.38431
  3       0.100  0.50  20.31155  0.16640337  14.32675
  3       0.100  1.00  20.41420  0.16706140  14.30817

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 20 of 224 using same du svmPoly 1 
Support Vector Machines with Polynomial Kernel 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 828, 826, 827, 826, 825, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.07638  0.05462385  16.50892
  1       0.001  0.50  22.02208  0.05083689  16.43315
  1       0.001  1.00  21.99123  0.04970717  16.35481
  1       0.010  0.25  21.96073  0.05718609  16.25339
  1       0.010  0.50  21.83946  0.06818716  16.13634
  1       0.010  1.00  21.69257  0.07615317  16.05241
  1       0.100  0.25  21.53288  0.07917542  16.00976
  1       0.100  0.50  21.44469  0.08118170  15.98921
  1       0.100  1.00  21.39781  0.08111416  15.99959
  2       0.001  0.25  22.01587  0.05250613  16.41578
  2       0.001  0.50  21.97205  0.05333203  16.32523
  2       0.001  1.00  21.92729  0.06052330  16.23142
  2       0.010  0.25  21.38564  0.09849009  15.85784
  2       0.010  0.50  21.16603  0.10373772  15.77599
  2       0.010  1.00  20.99520  0.10517596  15.72442
  2       0.100  0.25  20.96012  0.11763754  15.54110
  2       0.100  0.50  21.21812  0.11383614  15.65316
  2       0.100  1.00  21.61321  0.10831768  15.80986
  3       0.001  0.25  21.98472  0.05475509  16.33831
  3       0.001  0.50  21.92305  0.06103325  16.24152
  3       0.001  1.00  21.77482  0.07408425  16.09959
  3       0.010  0.25  21.15109  0.10397172  15.76777
  3       0.010  0.50  20.92670  0.10987940  15.64069
  3       0.010  1.00  20.90820  0.11394841  15.52915
  3       0.100  0.25  25.35537  0.08117339  16.93136
  3       0.100  0.50  27.60269  0.07030130  17.55194
  3       0.100  1.00  30.27190  0.05766448  18.41870

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 21 of 224 using max rms svmPoly 1 
Support Vector Machines with Polynomial Kernel 

6426 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5782, 5784, 5784, 5784, 5783, 5784, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.01468  0.05892351  16.56270
  1       0.001  0.50  21.87807  0.05675675  16.52192
  1       0.001  1.00  21.71791  0.05271909  16.49356
  1       0.010  0.25  21.57587  0.05017775  16.46861
  1       0.010  0.50  21.55086  0.04895685  16.46772
  1       0.010  1.00  21.53781  0.04819024  16.46796
  1       0.100  0.25  21.53136  0.04772113  16.46813
  1       0.100  0.50  21.52921  0.04753287  16.46822
  1       0.100  1.00  21.52838  0.04741436  16.46825
  2       0.001  0.25  21.87205  0.05782271  16.51498
  2       0.001  0.50  21.70222  0.05454631  16.47914
  2       0.001  1.00  21.56827  0.05346101  16.44350
  2       0.010  0.25  21.08859  0.09965792  15.99525
  2       0.010  0.50  20.83390  0.11285009  15.78685
  2       0.010  1.00  20.85475  0.11239854  15.63001
  2       0.100  0.25  20.88464  0.10819111  15.45326
  2       0.100  0.50  20.89005  0.10805235  15.45303
  2       0.100  1.00  20.89156  0.10802849  15.45300
  3       0.001  0.25  21.77023  0.05737896  16.48410
  3       0.001  0.50  21.59063  0.05584095  16.43638
  3       0.001  1.00  21.50571  0.05771285  16.38728
  3       0.010  0.25  20.82862  0.11455527  15.67798
  3       0.010  0.50  20.88059  0.11068437  15.55081
  3       0.010  1.00  20.85583  0.10992678  15.46000
  3       0.100  0.25  20.60093  0.13030639  14.85189
  3       0.100  0.50  20.60037  0.13098435  14.83998
  3       0.100  1.00  20.59964  0.13122432  14.83813

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 22 of 224 using same rms svmPoly 1 
Support Vector Machines with Polynomial Kernel 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 828, 826, 825, 826, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.68767  0.07094284  16.68206
  1       0.001  0.50  22.36143  0.07104975  16.60378
  1       0.001  1.00  22.15954  0.06970942  16.53847
  1       0.010  0.25  21.97843  0.06570806  16.46501
  1       0.010  0.50  21.88263  0.06106861  16.44190
  1       0.010  1.00  21.79477  0.05702583  16.43864
  1       0.100  0.25  21.72066  0.05531656  16.43427
  1       0.100  0.50  21.69224  0.05501925  16.43473
  1       0.100  1.00  21.67777  0.05499142  16.43436
  2       0.001  0.25  22.35944  0.07137840  16.60265
  2       0.001  0.50  22.15797  0.07023821  16.53599
  2       0.001  1.00  22.02751  0.06743110  16.47429
  2       0.010  0.25  21.77688  0.07220289  16.35409
  2       0.010  0.50  21.59151  0.07805187  16.25281
  2       0.010  1.00  21.41384  0.09255042  16.10990
  2       0.100  0.25  20.93863  0.11657099  15.52680
  2       0.100  0.50  20.93187  0.11589319  15.51629
  2       0.100  1.00  20.94902  0.11544308  15.52678
  3       0.001  0.25  22.20490  0.07144825  16.55485
  3       0.001  0.50  22.08502  0.06917092  16.49083
  3       0.001  1.00  21.93399  0.06668081  16.44283
  3       0.010  0.25  21.53090  0.08984780  16.16612
  3       0.010  0.50  21.30930  0.10469441  16.00368
  3       0.010  1.00  21.05158  0.11770214  15.80230
  3       0.100  0.25  20.68402  0.13584673  15.11732
  3       0.100  0.50  20.64675  0.14015543  15.00315
  3       0.100  1.00  20.63833  0.14176041  14.95250

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 23 of 224 using max hudgins svmPoly 1 
Support Vector Machines with Polynomial Kernel 

6426 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5783, 5783, 5782, 5784, 5783, 5783, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.99606  0.03430824  16.49074
  1       0.001  0.50  21.92267  0.03762563  16.42187
  1       0.001  1.00  21.79945  0.04450422  16.33856
  1       0.010  0.25  21.64578  0.05523293  16.22376
  1       0.010  0.50  21.46233  0.06260395  16.14953
  1       0.010  1.00  21.35315  0.06638876  16.10141
  1       0.100  0.25  21.37083  0.06642964  16.07826
  1       0.100  0.50  21.38184  0.06626276  16.07360
  1       0.100  1.00  21.38435  0.06625568  16.07108
  2       0.001  0.25  21.88632  0.04086342  16.38650
  2       0.001  0.50  21.72692  0.05150482  16.27637
  2       0.001  1.00  21.55961  0.06352724  16.14017
  2       0.010  0.25  20.89026  0.10514757  15.73737
  2       0.010  0.50  20.86660  0.10739070  15.64717
  2       0.010  1.00  20.75988  0.11123141  15.52766
  2       0.100  0.25  20.53130  0.12958602  15.11158
  2       0.100  0.50  20.52690  0.13014115  15.05824
  2       0.100  1.00  20.54364  0.12965102  15.00283
  3       0.001  0.25  21.73738  0.05151555  16.28269
  3       0.001  0.50  21.55659  0.06545442  16.13109
  3       0.001  1.00  21.30523  0.08017463  15.98307
  3       0.010  0.25  20.74251  0.11802701  15.45952
  3       0.010  0.50  20.57947  0.12534474  15.25656
  3       0.010  1.00  20.49595  0.13115796  15.12193
  3       0.100  0.25  20.32255  0.15137467  14.62147
  3       0.100  0.50  20.29453  0.15451828  14.54132
  3       0.100  1.00  20.41786  0.15154342  14.52978

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 24 of 224 using same hudgins svmPoly 1 
Support Vector Machines with Polynomial Kernel 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 827, 825, 826, 825, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.40005  0.05051789  16.58760
  1       0.001  0.50  22.12073  0.04909946  16.52523
  1       0.001  1.00  22.09100  0.04556733  16.47008
  1       0.010  0.25  22.09151  0.04561760  16.39262
  1       0.010  0.50  22.11277  0.04788104  16.35495
  1       0.010  1.00  22.02910  0.05310325  16.28823
  1       0.100  0.25  21.88772  0.05898220  16.23193
  1       0.100  0.50  21.74227  0.06246133  16.20328
  1       0.100  1.00  21.66306  0.06427056  16.18209
  2       0.001  0.25  22.11675  0.04978198  16.51845
  2       0.001  0.50  22.08585  0.04669893  16.45743
  2       0.001  1.00  22.06522  0.04732319  16.38278
  2       0.010  0.25  21.75903  0.07496408  16.07647
  2       0.010  0.50  21.53400  0.08429690  15.97546
  2       0.010  1.00  21.30179  0.08965713  15.91899
  2       0.100  0.25  21.10706  0.10181707  15.79572
  2       0.100  0.50  21.20147  0.10059947  15.80879
  2       0.100  1.00  21.39693  0.09708397  15.89117
  3       0.001  0.25  22.09030  0.04847405  16.47628
  3       0.001  0.50  22.05827  0.04800922  16.39278
  3       0.001  1.00  22.05465  0.05194251  16.32262
  3       0.010  0.25  21.51707  0.08682665  15.96500
  3       0.010  0.50  21.30625  0.09009547  15.93656
  3       0.010  1.00  21.15365  0.09434587  15.85814
  3       0.100  0.25  22.70447  0.08456561  16.53413
  3       0.100  0.50  23.98203  0.07377622  17.01455
  3       0.100  1.00  25.76394  0.06224949  17.69598

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 25 of 224 using max all ranger 1 
Growing trees.. Progress: 71%. Estimated remaining time: 12 seconds.
Growing trees.. Progress: 75%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 72%. Estimated remaining time: 12 seconds.
Growing trees.. Progress: 77%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 82%. Estimated remaining time: 6 seconds.
Growing trees.. Progress: 77%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 83%. Estimated remaining time: 6 seconds.
Growing trees.. Progress: 73%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 79%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 73%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 73%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 77%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 72%. Estimated remaining time: 12 seconds.
Growing trees.. Progress: 80%. Estimated remaining time: 7 seconds.
Growing trees.. Progress: 74%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 79%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 74%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 79%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 75%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 69%. Estimated remaining time: 14 seconds.
Growing trees.. Progress: 74%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 79%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 75%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 77%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 78%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 74%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 81%. Estimated remaining time: 7 seconds.
Growing trees.. Progress: 74%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 81%. Estimated remaining time: 7 seconds.
Growing trees.. Progress: 74%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 80%. Estimated remaining time: 7 seconds.
Growing trees.. Progress: 73%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 71%. Estimated remaining time: 12 seconds.
Growing trees.. Progress: 76%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 75%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 74%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 85%. Estimated remaining time: 5 seconds.
Growing trees.. Progress: 70%. Estimated remaining time: 13 seconds.
Growing trees.. Progress: 77%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 74%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 72%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 78%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 76%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 78%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 78%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 73%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 79%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 73%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 80%. Estimated remaining time: 7 seconds.
Growing trees.. Progress: 75%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 75%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 80%. Estimated remaining time: 7 seconds.
Growing trees.. Progress: 76%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 79%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 79%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 79%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 75%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 78%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 76%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 78%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 79%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 77%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 77%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 74%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 74%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 78%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 73%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 76%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 80%. Estimated remaining time: 7 seconds.
Growing trees.. Progress: 82%. Estimated remaining time: 6 seconds.
Growing trees.. Progress: 78%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 76%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 73%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 78%. Estimated remaining time: 8 seconds.
Growing trees.. Progress: 77%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 80%. Estimated remaining time: 7 seconds.
Growing trees.. Progress: 76%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 76%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 77%. Estimated remaining time: 9 seconds.
Growing trees.. Progress: 74%. Estimated remaining time: 11 seconds.
Growing trees.. Progress: 82%. Estimated remaining time: 6 seconds.
Growing trees.. Progress: 75%. Estimated remaining time: 10 seconds.
Growing trees.. Progress: 89%. Estimated remaining time: 3 seconds.
Random Forest 

6426 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5783, 5782, 5783, 5783, 5783, 5785, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.18300  0.4548386  11.24055
   2    extratrees  16.76861  0.4268952  12.08928
  13    variance    15.83008  0.4687462  10.47851
  13    extratrees  15.89623  0.4711324  10.87463
  24    variance    16.00902  0.4558423  10.61606
  24    extratrees  15.82827  0.4737133  10.74452

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 26 of 224 using same all ranger 1 
Random Forest 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 825, 828, 825, 825, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.54736  0.2802167  13.56279
   2    extratrees  18.75958  0.2704236  14.00024
  13    variance    18.60417  0.2733726  13.47520
  13    extratrees  18.54483  0.2794308  13.55850
  24    variance    18.79357  0.2594281  13.66902
  24    extratrees  18.51929  0.2810086  13.52075

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 27 of 224 using max du ranger 1 
Random Forest 

6426 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5782, 5783, 5785, 5785, 5783, 5783, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.80858  0.4131702  11.95130
   2    extratrees  17.57902  0.3646661  12.89068
  10    variance    15.96984  0.4599392  10.67185
  10    extratrees  16.42063  0.4368999  11.45182
  18    variance    16.03480  0.4542691  10.63636
  18    extratrees  16.26775  0.4453985  11.24305

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 28 of 224 using same du ranger 1 
Random Forest 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 825, 826, 828, 828, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.88211  0.2560320  13.93994
   2    extratrees  19.17125  0.2368559  14.43816
  10    variance    18.75307  0.2640634  13.65359
  10    extratrees  18.91618  0.2520024  13.94434
  18    variance    18.86085  0.2565892  13.74563
  18    extratrees  18.87783  0.2550664  13.89524

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 29 of 224 using max rms ranger 1 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

6426 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5785, 5783, 5783, 5783, 5783, 5784, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.22913  0.4397194  10.80908
  2     extratrees  16.26438  0.4427065  11.32266
  3     variance    16.28802  0.4361706  10.75436
  3     extratrees  16.23116  0.4424130  11.19841

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 30 of 224 using same rms ranger 1 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 827, 826, 825, 827, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    18.37789  0.2932165  13.05031
  2     extratrees  18.27908  0.3026860  13.42825
  3     variance    18.46640  0.2888931  13.05965
  3     extratrees  18.26263  0.3019472  13.34577

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 31 of 224 using max hudgins ranger 1 
Random Forest 

6426 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5784, 5783, 5784, 5783, 5783, 5783, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.69048  0.3460431  12.86458
   2    extratrees  18.29203  0.3053494  13.60340
   7    variance    16.59332  0.4161581  11.39347
   7    extratrees  17.13438  0.3847062  12.18247
  12    variance    16.49960  0.4211841  11.17253
  12    extratrees  16.87838  0.4009405  11.87133

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 32 of 224 using same hudgins ranger 1 
Random Forest 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 828, 826, 826, 826, 827, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    19.56126  0.1983441  14.65145
   2    extratrees  19.69216  0.1897074  14.97299
   7    variance    19.25305  0.2240415  14.18246
   7    extratrees  19.50436  0.2028156  14.52727
  12    variance    19.22265  0.2281759  14.11839
  12    extratrees  19.43471  0.2089666  14.43438

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 33 of 224 using max all lm 2 
Linear Regression 

2754 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2478, 2479, 2479, 2479, 2479, 2478, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.52518  0.1061542  16.09767

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 34 of 224 using same all lm 2 
Linear Regression 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 826, 826, 827, 826, ... 
Resampling results:

  RMSE      Rsquared    MAE    
  20.85185  0.09963332  16.2959

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 35 of 224 using max du lm 2 
Linear Regression 

2754 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2479, 2478, 2479, 2478, 2477, 2479, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.57784  0.1009949  16.08894

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 36 of 224 using same du lm 2 
Linear Regression 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 827, 826, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.80562  0.1021766  16.27696

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 37 of 224 using max rms lm 2 
Linear Regression 

2754 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2479, 2479, 2479, 2479, 2478, 2477, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.97345  0.06484806  16.75148

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 38 of 224 using same rms lm 2 
Linear Regression 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 826, 827, 826, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.09252  0.07702037  16.81417

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 39 of 224 using max hudgins lm 2 
Linear Regression 

2754 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2479, 2478, 2479, 2479, 2479, 2480, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.70777  0.09006408  16.39926

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 40 of 224 using same hudgins lm 2 
Linear Regression 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 827, 827, 825, 826, ... 
Resampling results:

  RMSE      Rsquared    MAE   
  20.93673  0.09332768  16.528

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 41 of 224 using max all knn 2 
k-Nearest Neighbors 

2754 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2479, 2478, 2479, 2478, 2479, 2481, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.33421  0.3737116  11.26009
  7  17.29726  0.3694847  11.46339
  9  17.45394  0.3553439  11.73584

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 42 of 224 using same all knn 2 
k-Nearest Neighbors 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 827, 826, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.86249  0.2174798  13.73936
  7  19.57622  0.2221491  13.86919
  9  19.37704  0.2290101  13.85300

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 43 of 224 using max du knn 2 
k-Nearest Neighbors 

2754 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2480, 2478, 2478, 2479, 2477, 2477, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.34942  0.3730658  11.28391
  7  17.26611  0.3717901  11.47162
  9  17.40971  0.3587785  11.71885

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 44 of 224 using same du knn 2 
k-Nearest Neighbors 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 826, 826, 828, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.89700  0.2152692  13.79473
  7  19.67132  0.2161744  13.95873
  9  19.42219  0.2258919  13.90034

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 45 of 224 using max rms knn 2 
k-Nearest Neighbors 

2754 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2478, 2478, 2478, 2479, 2479, 2478, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.16789  0.3827261  11.25299
  7  17.17113  0.3780490  11.43587
  9  17.15895  0.3763661  11.52780

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 46 of 224 using same rms knn 2 
k-Nearest Neighbors 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 826, 826, 828, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.80320  0.2810268  12.92784
  7  18.85969  0.2688812  13.26667
  9  18.94680  0.2585041  13.54317

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 47 of 224 using max hudgins knn 2 
k-Nearest Neighbors 

2754 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2478, 2478, 2478, 2479, 2477, 2478, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.60678  0.1469853  14.85907
  7  20.27848  0.1555042  14.75355
  9  20.08926  0.1617166  14.70291

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 48 of 224 using same hudgins knn 2 
k-Nearest Neighbors 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 826, 826, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.23242  0.1231212  15.52312
  7  20.81592  0.1335955  15.30578
  9  20.59803  0.1429484  15.12059

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 49 of 224 using max all svmPoly 2 
Support Vector Machines with Polynomial Kernel 

2754 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2479, 2479, 2479, 2478, 2478, 2480, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.79112  0.05476112  16.37828
  1       0.001  0.50  21.65461  0.05795969  16.27108
  1       0.001  1.00  21.52934  0.06578275  16.13469
  1       0.010  0.25  21.32371  0.07685080  15.94747
  1       0.010  0.50  21.12606  0.08377442  15.80761
  1       0.010  1.00  21.07107  0.08640669  15.71400
  1       0.100  0.25  21.06003  0.08878994  15.66274
  1       0.100  0.50  21.04248  0.09097546  15.63148
  1       0.100  1.00  21.05376  0.09191099  15.60967
  2       0.001  0.25  21.57394  0.06994983  16.17378
  2       0.001  0.50  21.34017  0.08400055  15.97763
  2       0.001  1.00  21.08568  0.09950671  15.75974
  2       0.010  0.25  20.58641  0.12946349  15.21108
  2       0.010  0.50  20.47527  0.13647006  15.03252
  2       0.010  1.00  20.35327  0.14598229  14.81155
  2       0.100  0.25  19.83188  0.18585366  14.14175
  2       0.100  0.50  19.76411  0.19156871  14.02660
  2       0.100  1.00  19.74473  0.19429908  13.94016
  3       0.001  0.25  21.33872  0.09134186  15.96766
  3       0.001  0.50  21.04700  0.10829666  15.73969
  3       0.001  1.00  20.81327  0.11842792  15.51839
  3       0.010  0.25  20.26858  0.15423485  14.65162
  3       0.010  0.50  20.19070  0.16074278  14.50943
  3       0.010  1.00  20.11303  0.16682996  14.39891
  3       0.100  0.25  20.01245  0.18935342  13.91340
  3       0.100  0.50  20.30176  0.18273812  13.99899
  3       0.100  1.00  20.69093  0.17482361  14.14671

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 50 of 224 using same all svmPoly 2 
Support Vector Machines with Polynomial Kernel 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 825, 826, 826, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.10254  0.06121276  16.68892
  1       0.001  0.50  21.89941  0.06164253  16.60554
  1       0.001  1.00  21.74392  0.06636936  16.50963
  1       0.010  0.25  21.55133  0.07734264  16.35272
  1       0.010  0.50  21.34229  0.08832238  16.19162
  1       0.010  1.00  21.16271  0.09539484  16.02835
  1       0.100  0.25  21.09895  0.09813159  15.90439
  1       0.100  0.50  21.07508  0.09883842  15.87148
  1       0.100  1.00  21.09557  0.09859049  15.87266
  2       0.001  0.25  21.87679  0.06487087  16.56819
  2       0.001  0.50  21.69662  0.07285189  16.45068
  2       0.001  1.00  21.48005  0.08609555  16.28629
  2       0.010  0.25  20.97288  0.11237584  15.87547
  2       0.010  0.50  20.93127  0.11500999  15.73806
  2       0.010  1.00  20.86184  0.12153870  15.59040
  2       0.100  0.25  20.50102  0.15533134  14.92180
  2       0.100  0.50  20.42105  0.16196276  14.82766
  2       0.100  1.00  20.43438  0.16609484  14.77730
  3       0.001  0.25  21.73220  0.07438351  16.46013
  3       0.001  0.50  21.50023  0.08776754  16.30216
  3       0.001  1.00  21.21166  0.10205236  16.08293
  3       0.010  0.25  20.83967  0.13023306  15.47596
  3       0.010  0.50  20.74316  0.13690499  15.26905
  3       0.010  1.00  20.70955  0.14202893  15.13343
  3       0.100  0.25  26.51175  0.07493636  16.93963
  3       0.100  0.50  28.33889  0.06627800  17.56415
  3       0.100  1.00  32.01345  0.05423570  18.62162

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 51 of 224 using max du svmPoly 2 
Support Vector Machines with Polynomial Kernel 

2754 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2480, 2480, 2477, 2478, 2477, 2479, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.91392  0.04846259  16.42013
  1       0.001  0.50  21.79442  0.04996441  16.32661
  1       0.001  1.00  21.65837  0.05849710  16.19341
  1       0.010  0.25  21.47073  0.07182377  16.00861
  1       0.010  0.50  21.26681  0.08037335  15.87821
  1       0.010  1.00  21.11043  0.08526586  15.77793
  1       0.100  0.25  21.03884  0.08791470  15.69744
  1       0.100  0.50  21.05037  0.08766729  15.68631
  1       0.100  1.00  21.06614  0.08713594  15.67873
  2       0.001  0.25  21.74879  0.05582338  16.27884
  2       0.001  0.50  21.59395  0.06728240  16.12349
  2       0.001  1.00  21.37023  0.08219123  15.93840
  2       0.010  0.25  20.71770  0.12351036  15.42767
  2       0.010  0.50  20.62336  0.12657343  15.29876
  2       0.010  1.00  20.51155  0.13188843  15.13879
  2       0.100  0.25  20.06780  0.16615839  14.46238
  2       0.100  0.50  20.01455  0.17101546  14.37846
  2       0.100  1.00  19.97006  0.17555395  14.28899
  3       0.001  0.25  21.61014  0.06913674  16.14087
  3       0.001  0.50  21.37972  0.08644853  15.95130
  3       0.001  1.00  21.10298  0.10427073  15.73927
  3       0.010  0.25  20.43891  0.14183136  15.02996
  3       0.010  0.50  20.29892  0.14999282  14.80268
  3       0.010  1.00  20.22758  0.15456847  14.66340
  3       0.100  0.25  20.18482  0.17206932  14.24853
  3       0.100  0.50  20.38591  0.16689440  14.30836
  3       0.100  1.00  20.95660  0.15392882  14.50833

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 52 of 224 using same du svmPoly 2 
Support Vector Machines with Polynomial Kernel 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 826, 826, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.23073  0.05557938  16.73951
  1       0.001  0.50  22.08327  0.05369037  16.66489
  1       0.001  1.00  21.91593  0.05732998  16.57639
  1       0.010  0.25  21.73423  0.06747154  16.44010
  1       0.010  0.50  21.56579  0.08030378  16.28426
  1       0.010  1.00  21.34928  0.09139127  16.12878
  1       0.100  0.25  21.11441  0.09825590  15.97116
  1       0.100  0.50  21.08868  0.09993515  15.91718
  1       0.100  1.00  21.09753  0.09980552  15.90144
  2       0.001  0.25  22.07348  0.05476032  16.64898
  2       0.001  0.50  21.89310  0.06028522  16.54974
  2       0.001  1.00  21.73036  0.07024805  16.43110
  2       0.010  0.25  21.17353  0.10635738  16.07815
  2       0.010  0.50  21.02159  0.10974666  15.96319
  2       0.010  1.00  20.98985  0.11368968  15.85211
  2       0.100  0.25  20.69216  0.14172880  15.23103
  2       0.100  0.50  20.67263  0.14475564  15.16724
  2       0.100  1.00  20.66408  0.14713891  15.14103
  3       0.001  0.25  21.95551  0.05980401  16.56947
  3       0.001  0.50  21.76884  0.06961135  16.45073
  3       0.001  1.00  21.56623  0.08365636  16.29493
  3       0.010  0.25  21.05601  0.11657646  15.89619
  3       0.010  0.50  21.00087  0.12425763  15.67882
  3       0.010  1.00  20.91392  0.12988214  15.47932
  3       0.100  0.25  24.91818  0.08126184  16.68988
  3       0.100  0.50  27.34208  0.06647553  17.36322
  3       0.100  1.00  29.48596  0.05793230  18.02039

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 53 of 224 using max rms svmPoly 2 
Support Vector Machines with Polynomial Kernel 

2754 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2480, 2478, 2479, 2478, 2479, 2478, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.17493  0.06284675  16.55297
  1       0.001  0.50  21.98129  0.06237439  16.50530
  1       0.001  1.00  21.84003  0.06179800  16.47182
  1       0.010  0.25  21.64440  0.06050240  16.44784
  1       0.010  0.50  21.54257  0.05782956  16.43810
  1       0.010  1.00  21.48721  0.05600155  16.43114
  1       0.100  0.25  21.45615  0.05518084  16.42798
  1       0.100  0.50  21.44747  0.05496074  16.42778
  1       0.100  1.00  21.44261  0.05484370  16.42770
  2       0.001  0.25  21.97803  0.06305187  16.50191
  2       0.001  0.50  21.83199  0.06299087  16.46442
  2       0.001  1.00  21.67507  0.06315381  16.43708
  2       0.010  0.25  21.27881  0.09633265  16.12869
  2       0.010  0.50  21.01625  0.11386392  15.89067
  2       0.010  1.00  20.80248  0.12509088  15.59968
  2       0.100  0.25  20.75632  0.12184436  15.19083
  2       0.100  0.50  20.76493  0.12153753  15.18726
  2       0.100  1.00  20.76723  0.12142729  15.18586
  3       0.001  0.25  21.87409  0.06377112  16.47250
  3       0.001  0.50  21.73227  0.06483331  16.43725
  3       0.001  1.00  21.58034  0.06560388  16.40190
  3       0.010  0.25  20.90640  0.12384203  15.71224
  3       0.010  0.50  20.84852  0.12330440  15.45655
  3       0.010  1.00  20.75031  0.12253200  15.28480
  3       0.100  0.25  20.43853  0.14701388  14.55539
  3       0.100  0.50  20.45216  0.14844541  14.50601
  3       0.100  1.00  20.47269  0.14869104  14.49696

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.25.


Now processing model 54 of 224 using same rms svmPoly 2 
Support Vector Machines with Polynomial Kernel 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 826, 827, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.50835  0.07209810  16.90715
  1       0.001  0.50  22.38796  0.07498874  16.83014
  1       0.001  1.00  22.16874  0.07487169  16.73771
  1       0.010  0.25  21.81954  0.07292478  16.65708
  1       0.010  0.50  21.60162  0.06973867  16.59949
  1       0.010  1.00  21.51178  0.06839955  16.58211
  1       0.100  0.25  21.46616  0.06629703  16.58069
  1       0.100  0.50  21.44012  0.06625202  16.57503
  1       0.100  1.00  21.43393  0.06620364  16.57426
  2       0.001  0.25  22.38734  0.07524663  16.82908
  2       0.001  0.50  22.16656  0.07521715  16.73576
  2       0.001  1.00  21.91926  0.07429580  16.66708
  2       0.010  0.25  21.52173  0.07982290  16.50641
  2       0.010  0.50  21.32147  0.08776610  16.39355
  2       0.010  1.00  21.09909  0.10202521  16.22442
  2       0.100  0.25  20.90158  0.12395987  15.60531
  2       0.100  0.50  20.95649  0.12200888  15.61082
  2       0.100  1.00  20.98297  0.12133471  15.61891
  3       0.001  0.25  22.28356  0.07569191  16.77138
  3       0.001  0.50  22.03780  0.07539956  16.69178
  3       0.001  1.00  21.73580  0.07355018  16.63006
  3       0.010  0.25  21.28663  0.09667869  16.31687
  3       0.010  0.50  21.01912  0.11201017  16.11670
  3       0.010  1.00  20.87746  0.12261865  15.92406
  3       0.100  0.25  20.63775  0.14343904  15.14276
  3       0.100  0.50  20.56562  0.14665105  15.00684
  3       0.100  1.00  20.54416  0.14838123  14.93190

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 55 of 224 using max hudgins svmPoly 2 
Support Vector Machines with Polynomial Kernel 

2754 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2478, 2479, 2478, 2479, 2479, 2480, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.05044  0.04239992  16.52151
  1       0.001  0.50  21.94551  0.04018861  16.46685
  1       0.001  1.00  21.85169  0.04097269  16.38818
  1       0.010  0.25  21.70071  0.05020507  16.27233
  1       0.010  0.50  21.54308  0.05829164  16.16910
  1       0.010  1.00  21.35626  0.06701597  16.05327
  1       0.100  0.25  21.28937  0.07153592  15.95834
  1       0.100  0.50  21.28371  0.07221158  15.93146
  1       0.100  1.00  21.26615  0.07334121  15.90296
  2       0.001  0.25  21.93227  0.04231611  16.44796
  2       0.001  0.50  21.81411  0.04523478  16.35462
  2       0.001  1.00  21.67640  0.05414842  16.24220
  2       0.010  0.25  20.95482  0.11139260  15.66911
  2       0.010  0.50  20.84483  0.11455530  15.53812
  2       0.010  1.00  20.73041  0.11760912  15.42712
  2       0.100  0.25  20.49340  0.13645385  14.96280
  2       0.100  0.50  20.43629  0.14039588  14.88661
  2       0.100  1.00  20.38797  0.14368611  14.81128
  3       0.001  0.25  21.84463  0.04586969  16.36942
  3       0.001  0.50  21.68987  0.05506976  16.25299
  3       0.001  1.00  21.48785  0.06977575  16.09062
  3       0.010  0.25  20.79037  0.11871402  15.46412
  3       0.010  0.50  20.62319  0.12588640  15.27036
  3       0.010  1.00  20.49847  0.13360583  15.07255
  3       0.100  0.25  20.41854  0.15260251  14.58932
  3       0.100  0.50  20.45566  0.15467225  14.51531
  3       0.100  1.00  20.45080  0.15828888  14.42930

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 56 of 224 using same hudgins svmPoly 2 
Support Vector Machines with Polynomial Kernel 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 826, 826, 825, 828, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.28877  0.05282565  16.82319
  1       0.001  0.50  22.20472  0.04910686  16.74509
  1       0.001  1.00  22.07181  0.04761252  16.68261
  1       0.010  0.25  21.90997  0.05300827  16.59715
  1       0.010  0.50  21.79573  0.06002327  16.52515
  1       0.010  1.00  21.62589  0.07010752  16.42052
  1       0.100  0.25  21.34315  0.08339567  16.25048
  1       0.100  0.50  21.31723  0.08647431  16.20335
  1       0.100  1.00  21.24856  0.08982468  16.14633
  2       0.001  0.25  22.20052  0.04946347  16.73815
  2       0.001  0.50  22.06191  0.04876328  16.66958
  2       0.001  1.00  21.93706  0.05269803  16.60176
  2       0.010  0.25  21.51191  0.08534277  16.29607
  2       0.010  0.50  21.23394  0.09557700  16.14957
  2       0.010  1.00  21.14456  0.09854302  16.07838
  2       0.100  0.25  20.98584  0.12263838  15.62253
  2       0.100  0.50  20.97534  0.12659258  15.55751
  2       0.100  1.00  20.93854  0.13055013  15.50113
  3       0.001  0.25  22.11089  0.04902291  16.68641
  3       0.001  0.50  21.98487  0.05210600  16.61753
  3       0.001  1.00  21.81962  0.06085527  16.52425
  3       0.010  0.25  21.29751  0.09391071  16.19488
  3       0.010  0.50  21.16707  0.10266447  16.03245
  3       0.010  1.00  21.16778  0.10713285  15.91756
  3       0.100  0.25  22.27753  0.09775900  16.09135
  3       0.100  0.50  24.00776  0.08333305  16.65808
  3       0.100  1.00  26.44932  0.07136766  17.40493

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 57 of 224 using max all ranger 2 
Random Forest 

2754 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2478, 2478, 2479, 2479, 2480, 2478, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.12903  0.4593135  11.04939
   2    extratrees  16.67832  0.4352097  11.82664
  13    variance    15.91097  0.4641834  10.50581
  13    extratrees  15.96786  0.4680705  10.82380
  24    variance    16.06367  0.4533659  10.65452
  24    extratrees  15.92933  0.4683505  10.73072

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = variance
 and min.node.size = 5.


Now processing model 58 of 224 using same all ranger 2 
Random Forest 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 827, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.61381  0.3614521  12.47985
   2    extratrees  18.10222  0.3357180  13.16206
  13    variance    17.52920  0.3622866  12.15286
  13    extratrees  17.65726  0.3587294  12.48413
  24    variance    17.73612  0.3478172  12.35495
  24    extratrees  17.55840  0.3648727  12.39343

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = variance
 and min.node.size = 5.


Now processing model 59 of 224 using max du ranger 2 
Random Forest 

2754 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2478, 2479, 2477, 2478, 2478, 2480, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.62814  0.4261997  11.60583
   2    extratrees  17.38697  0.3799437  12.50146
  10    variance    15.97791  0.4606045  10.63648
  10    extratrees  16.44822  0.4362831  11.32944
  18    variance    16.04133  0.4554049  10.64385
  18    extratrees  16.34990  0.4407700  11.18913

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 60 of 224 using same du ranger 2 
Random Forest 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 827, 826, 825, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.84352  0.3460352  12.76671
   2    extratrees  18.50840  0.3018258  13.58175
  10    variance    17.54767  0.3609063  12.23671
  10    extratrees  17.99038  0.3342273  12.84762
  18    variance    17.68256  0.3514831  12.31611
  18    extratrees  17.90980  0.3387563  12.75978

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 61 of 224 using max rms ranger 2 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

2754 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2477, 2480, 2479, 2478, 2478, 2479, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.36580  0.4316784  10.81640
  2     extratrees  16.34000  0.4395789  11.28272
  3     variance    16.44780  0.4268726  10.78367
  3     extratrees  16.33067  0.4369967  11.18085

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 62 of 224 using same rms ranger 2 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 826, 826, 827, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.73617  0.3485132  12.21771
  2     extratrees  17.70127  0.3553076  12.66702
  3     variance    17.83782  0.3443602  12.16818
  3     extratrees  17.67446  0.3544079  12.56706

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 63 of 224 using max hudgins ranger 2 
Random Forest 

2754 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2478, 2480, 2478, 2479, 2478, 2478, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.50386  0.3597920  12.47928
   2    extratrees  18.13215  0.3178963  13.23855
   7    variance    16.61812  0.4154387  11.27540
   7    extratrees  17.20399  0.3808493  12.08079
  12    variance    16.52495  0.4204982  11.07169
  12    extratrees  17.01449  0.3925182  11.82583

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 64 of 224 using same hudgins ranger 2 
Random Forest 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 826, 825, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.40302  0.3050849  13.38027
   2    extratrees  18.87699  0.2739412  14.03096
   7    variance    17.92692  0.3353115  12.68213
   7    extratrees  18.33620  0.3106705  13.20577
  12    variance    17.93543  0.3346286  12.58616
  12    extratrees  18.20521  0.3198135  13.04522

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 65 of 224 using max all lm 3 
Linear Regression 

1836 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1652, 1652, 1653, 1654, 1654, 1653, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.70774  0.1033335  16.22525

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 66 of 224 using same all lm 3 
Linear Regression 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 826, 825, 826, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.65614  0.1036291  16.26812

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 67 of 224 using max du lm 3 
Linear Regression 

1836 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1652, 1653, 1652, 1651, 1652, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.73315  0.09912768  16.18437

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 68 of 224 using same du lm 3 
Linear Regression 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 827, 826, 827, 827, 826, ... 
Resampling results:

  RMSE     Rsquared    MAE     
  20.7345  0.09782761  16.20868

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 69 of 224 using max rms lm 3 
Linear Regression 

1836 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1653, 1651, 1652, 1651, 1653, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.13468  0.06391517  16.85889

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 70 of 224 using same rms lm 3 
Linear Regression 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 824, 828, 826, 827, 826, 827, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.97728  0.07287447  16.71593

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 71 of 224 using max hudgins lm 3 
Linear Regression 

1836 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1653, 1652, 1652, 1654, 1651, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.86228  0.08873622  16.50005

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 72 of 224 using same hudgins lm 3 
Linear Regression 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 825, 826, 826, 826, ... 
Resampling results:

  RMSE      Rsquared    MAE    
  20.76637  0.09389976  16.4484

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 73 of 224 using max all knn 3 
k-Nearest Neighbors 

1836 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1652, 1654, 1652, 1652, 1653, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.06369  0.3344280  11.70556
  7  17.80889  0.3427532  11.93487
  9  17.79770  0.3399106  12.15993

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 74 of 224 using same all knn 3 
k-Nearest Neighbors 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 827, 825, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.09807  0.2613208  12.98211
  7  18.94382  0.2589318  13.17895
  9  18.83717  0.2608951  13.24008

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 75 of 224 using max du knn 3 
k-Nearest Neighbors 

1836 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1651, 1652, 1651, 1651, 1653, 1654, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.10787  0.3313324  11.74895
  7  17.78960  0.3435985  11.93686
  9  17.80412  0.3393337  12.15375

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 76 of 224 using same du knn 3 
k-Nearest Neighbors 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 826, 826, 825, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.03002  0.2669176  12.95078
  7  18.91877  0.2631319  13.15058
  9  18.79307  0.2651099  13.20904

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 77 of 224 using max rms knn 3 
k-Nearest Neighbors 

1836 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1652, 1653, 1651, 1654, 1654, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.51009  0.3687648  11.53205
  7  17.44236  0.3669606  11.75613
  9  17.53275  0.3578342  12.04628

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 78 of 224 using same rms knn 3 
k-Nearest Neighbors 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 826, 825, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.52138  0.2935168  12.73354
  7  18.49235  0.2868658  13.01984
  9  18.58887  0.2768482  13.14483

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 79 of 224 using max hudgins knn 3 
k-Nearest Neighbors 

1836 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1651, 1652, 1653, 1652, 1653, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.72263  0.1530665  14.89499
  7  20.34067  0.1610112  14.71744
  9  20.23036  0.1614357  14.72243

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 80 of 224 using same hudgins knn 3 
k-Nearest Neighbors 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 826, 825, 826, 828, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.11225  0.1208675  15.50764
  7  20.77023  0.1273979  15.36462
  9  20.61066  0.1291699  15.35673

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 81 of 224 using max all svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1836 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1651, 1654, 1652, 1652, 1652, 1653, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.05162  0.05510908  16.51517
  1       0.001  0.50  21.91609  0.05680969  16.42774
  1       0.001  1.00  21.77512  0.06172732  16.28768
  1       0.010  0.25  21.58412  0.07292926  16.07703
  1       0.010  0.50  21.35598  0.08211249  15.90273
  1       0.010  1.00  21.26321  0.08584448  15.76471
  1       0.100  0.25  21.23948  0.08799163  15.68186
  1       0.100  0.50  21.22114  0.08980235  15.64618
  1       0.100  1.00  21.23535  0.09047084  15.64907
  2       0.001  0.25  21.83880  0.06603050  16.35201
  2       0.001  0.50  21.63480  0.07826135  16.15029
  2       0.001  1.00  21.38204  0.09432684  15.91364
  2       0.010  0.25  20.86453  0.12436437  15.28365
  2       0.010  0.50  20.76972  0.12959976  15.13605
  2       0.010  1.00  20.66910  0.13680821  14.96056
  2       0.100  0.25  20.14165  0.17700903  14.28343
  2       0.100  0.50  20.04546  0.18347892  14.19132
  2       0.100  1.00  19.99533  0.18765753  14.10174
  3       0.001  0.25  21.63393  0.08474012  16.14203
  3       0.001  0.50  21.36362  0.10262518  15.90085
  3       0.001  1.00  21.09472  0.11367642  15.65283
  3       0.010  0.25  20.53291  0.14701654  14.78318
  3       0.010  0.50  20.45464  0.15349905  14.59898
  3       0.010  1.00  20.41769  0.15722324  14.50596
  3       0.100  0.25  20.90717  0.16567560  14.44913
  3       0.100  0.50  22.23840  0.15149079  14.76549
  3       0.100  1.00  24.82593  0.13379512  15.22858

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 82 of 224 using same all svmPoly 3 
Support Vector Machines with Polynomial Kernel 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 824, 827, 827, 828, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.96052  0.05417386  16.55030
  1       0.001  0.50  21.81234  0.05272738  16.44334
  1       0.001  1.00  21.66475  0.05601112  16.33702
  1       0.010  0.25  21.46148  0.06781020  16.17415
  1       0.010  0.50  21.25429  0.08023336  16.02557
  1       0.010  1.00  21.05873  0.08992963  15.88890
  1       0.100  0.25  21.00893  0.09349596  15.79751
  1       0.100  0.50  21.01159  0.09458386  15.78136
  1       0.100  1.00  21.01265  0.09539109  15.78716
  2       0.001  0.25  21.77115  0.05778416  16.38572
  2       0.001  0.50  21.59717  0.06700876  16.26399
  2       0.001  1.00  21.35151  0.07959380  16.10649
  2       0.010  0.25  20.75354  0.11825716  15.60906
  2       0.010  0.50  20.65640  0.12233540  15.48601
  2       0.010  1.00  20.57824  0.12940054  15.36262
  2       0.100  0.25  19.99259  0.17420352  14.54472
  2       0.100  0.50  19.97193  0.17766313  14.47791
  2       0.100  1.00  20.07472  0.17468077  14.51357
  3       0.001  0.25  21.61633  0.07100401  16.26419
  3       0.001  0.50  21.35662  0.08446042  16.10020
  3       0.001  1.00  21.04882  0.09926926  15.90116
  3       0.010  0.25  20.46532  0.14071569  15.22738
  3       0.010  0.50  20.33242  0.14977110  15.03361
  3       0.010  1.00  20.21872  0.15860666  14.85154
  3       0.100  0.25  24.73298  0.09991109  16.27370
  3       0.100  0.50  26.67305  0.08825316  16.82766
  3       0.100  1.00  30.29514  0.07158604  17.93898

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 83 of 224 using max du svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1836 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1651, 1653, 1653, 1654, 1652, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.17980  0.04823897  16.55955
  1       0.001  0.50  22.04874  0.04884583  16.48189
  1       0.001  1.00  21.92324  0.05255814  16.36822
  1       0.010  0.25  21.75054  0.06608240  16.14901
  1       0.010  0.50  21.54576  0.07640531  15.98845
  1       0.010  1.00  21.31171  0.08447599  15.83530
  1       0.100  0.25  21.20235  0.08840871  15.71621
  1       0.100  0.50  21.20781  0.08854795  15.69034
  1       0.100  1.00  21.22173  0.08849664  15.66888
  2       0.001  0.25  22.01847  0.05279497  16.44833
  2       0.001  0.50  21.85596  0.06037226  16.30495
  2       0.001  1.00  21.68408  0.07491520  16.11084
  2       0.010  0.25  20.98710  0.11996817  15.52386
  2       0.010  0.50  20.90479  0.12295738  15.37825
  2       0.010  1.00  20.82625  0.12616059  15.22816
  2       0.100  0.25  20.35659  0.15991809  14.54594
  2       0.100  0.50  20.29768  0.16497788  14.49224
  2       0.100  1.00  20.27987  0.16746834  14.46093
  3       0.001  0.25  21.88637  0.06201745  16.32384
  3       0.001  0.50  21.68721  0.07938783  16.12197
  3       0.001  1.00  21.43354  0.09690114  15.90594
  3       0.010  0.25  20.76065  0.13422009  15.19454
  3       0.010  0.50  20.62130  0.14081077  14.96042
  3       0.010  1.00  20.50393  0.14743681  14.76943
  3       0.100  0.25  21.09040  0.14668312  14.72875
  3       0.100  0.50  21.68632  0.13749590  14.93155
  3       0.100  1.00  22.99681  0.12081404  15.28043

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 84 of 224 using same du svmPoly 3 
Support Vector Machines with Polynomial Kernel 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 826, 825, 826, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.05813  0.05245020  16.59723
  1       0.001  0.50  21.94231  0.04980178  16.49532
  1       0.001  1.00  21.83025  0.05126548  16.39152
  1       0.010  0.25  21.64921  0.06232511  16.23667
  1       0.010  0.50  21.46124  0.07580742  16.08428
  1       0.010  1.00  21.22262  0.08852436  15.94711
  1       0.100  0.25  21.02385  0.09677413  15.82284
  1       0.100  0.50  20.98544  0.09837174  15.77373
  1       0.100  1.00  21.00830  0.09682375  15.79928
  2       0.001  0.25  21.92365  0.05188839  16.46983
  2       0.001  0.50  21.79354  0.05686158  16.35570
  2       0.001  1.00  21.63734  0.06646406  16.23316
  2       0.010  0.25  20.97928  0.11067891  15.82157
  2       0.010  0.50  20.81228  0.11568950  15.70866
  2       0.010  1.00  20.74951  0.11918620  15.63094
  2       0.100  0.25  20.20019  0.15929416  14.88606
  2       0.100  0.50  20.13281  0.16609846  14.76169
  2       0.100  1.00  20.10695  0.17062486  14.67970
  3       0.001  0.25  21.81834  0.05869715  16.37154
  3       0.001  0.50  21.66364  0.06726363  16.25604
  3       0.001  1.00  21.42404  0.08226513  16.09004
  3       0.010  0.25  20.74390  0.12491627  15.62003
  3       0.010  0.50  20.57210  0.13633476  15.42643
  3       0.010  1.00  20.45097  0.14341089  15.24334
  3       0.100  0.25  24.02273  0.09934312  16.27482
  3       0.100  0.50  27.08711  0.08308490  17.03354
  3       0.100  1.00  30.04060  0.07052358  17.84169

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 85 of 224 using max rms svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1836 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1652, 1652, 1652, 1652, 1652, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.37010  0.06107631  16.66776
  1       0.001  0.50  22.30112  0.06120873  16.60867
  1       0.001  1.00  22.08023  0.06062894  16.57045
  1       0.010  0.25  21.90971  0.06040949  16.54455
  1       0.010  0.50  21.79073  0.05965482  16.54122
  1       0.010  1.00  21.71662  0.05812933  16.54143
  1       0.100  0.25  21.66528  0.05698368  16.54111
  1       0.100  0.50  21.64532  0.05652090  16.54217
  1       0.100  1.00  21.63903  0.05626269  16.54335
  2       0.001  0.25  22.29928  0.06178866  16.60638
  2       0.001  0.50  22.07462  0.06153361  16.56548
  2       0.001  1.00  21.93816  0.06217798  16.53524
  2       0.010  0.25  21.57185  0.09106362  16.29199
  2       0.010  0.50  21.30340  0.10798120  16.07204
  2       0.010  1.00  21.01433  0.12464138  15.74636
  2       0.100  0.25  20.89314  0.12552119  15.17059
  2       0.100  0.50  20.90031  0.12493184  15.16787
  2       0.100  1.00  20.90136  0.12460175  15.16621
  3       0.001  0.25  22.16672  0.06223880  16.58057
  3       0.001  0.50  21.98395  0.06314769  16.53661
  3       0.001  1.00  21.84376  0.06456105  16.51203
  3       0.010  0.25  21.18264  0.12213170  15.88140
  3       0.010  0.50  20.98020  0.12697159  15.54845
  3       0.010  1.00  20.88895  0.12643965  15.30431
  3       0.100  0.25  20.55368  0.14908538  14.57348
  3       0.100  0.50  20.57024  0.14913155  14.52252
  3       0.100  1.00  20.57787  0.15041174  14.49175

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.25.


Now processing model 86 of 224 using same rms svmPoly 3 
Support Vector Machines with Polynomial Kernel 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 828, 826, 825, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.35380  0.06904716  16.79903
  1       0.001  0.50  22.24420  0.07041220  16.71909
  1       0.001  1.00  22.05515  0.06948635  16.63383
  1       0.010  0.25  21.80184  0.06665878  16.55216
  1       0.010  0.50  21.56403  0.06371249  16.49120
  1       0.010  1.00  21.43212  0.06172371  16.46685
  1       0.100  0.25  21.36582  0.06110889  16.44598
  1       0.100  0.50  21.34762  0.06110353  16.44064
  1       0.100  1.00  21.34149  0.06108864  16.44000
  2       0.001  0.25  22.24373  0.07078220  16.71738
  2       0.001  0.50  22.05203  0.06999467  16.63104
  2       0.001  1.00  21.87105  0.06811382  16.56096
  2       0.010  0.25  21.43759  0.07654309  16.35808
  2       0.010  0.50  21.15262  0.08873545  16.19559
  2       0.010  1.00  20.92091  0.10530840  16.02413
  2       0.100  0.25  20.70990  0.12990995  15.36270
  2       0.100  0.50  20.78891  0.12727668  15.37977
  2       0.100  1.00  20.82641  0.12572356  15.39074
  3       0.001  0.25  22.16784  0.07096377  16.65761
  3       0.001  0.50  21.93801  0.06972008  16.58081
  3       0.001  1.00  21.72239  0.06792263  16.52339
  3       0.010  0.25  21.11330  0.10097240  16.10497
  3       0.010  0.50  20.83713  0.11862895  15.89105
  3       0.010  1.00  20.69222  0.13259183  15.64504
  3       0.100  0.25  20.39511  0.15678631  14.76920
  3       0.100  0.50  20.34664  0.15965691  14.67890
  3       0.100  1.00  20.31024  0.16147033  14.63213

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 87 of 224 using max hudgins svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1836 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1654, 1653, 1651, 1652, 1652, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.25271  0.04231859  16.64229
  1       0.001  0.50  22.21050  0.04070495  16.58612
  1       0.001  1.00  22.11135  0.03929394  16.52868
  1       0.010  0.25  21.96050  0.04429948  16.40810
  1       0.010  0.50  21.84939  0.05212844  16.29200
  1       0.010  1.00  21.63076  0.06252748  16.15396
  1       0.100  0.25  21.50950  0.07029379  16.00162
  1       0.100  0.50  21.49259  0.07172344  15.95694
  1       0.100  1.00  21.48729  0.07275343  15.92140
  2       0.001  0.25  22.20112  0.04264200  16.57212
  2       0.001  0.50  22.08655  0.04223510  16.50217
  2       0.001  1.00  21.94117  0.04797527  16.39632
  2       0.010  0.25  21.31156  0.10380020  15.83307
  2       0.010  0.50  21.10633  0.10935339  15.63194
  2       0.010  1.00  21.04038  0.11022783  15.52790
  2       0.100  0.25  20.80842  0.12887165  15.08553
  2       0.100  0.50  20.73593  0.13345915  15.00230
  2       0.100  1.00  20.69647  0.13671667  14.96086
  3       0.001  0.25  22.12168  0.04429523  16.51764
  3       0.001  0.50  21.96523  0.04943982  16.40779
  3       0.001  1.00  21.79905  0.06150412  16.25197
  3       0.010  0.25  21.08736  0.11056456  15.59855
  3       0.010  0.50  20.97856  0.11568901  15.42506
  3       0.010  1.00  20.85345  0.12289775  15.24733
  3       0.100  0.25  20.89894  0.13965111  14.87882
  3       0.100  0.50  21.11136  0.13545722  14.96267
  3       0.100  1.00  21.53656  0.12691648  15.09944

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 88 of 224 using same hudgins svmPoly 3 
Support Vector Machines with Polynomial Kernel 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 825, 827, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.12566  0.04908359  16.69753
  1       0.001  0.50  22.05098  0.04520351  16.61163
  1       0.001  1.00  21.95083  0.04279505  16.52533
  1       0.010  0.25  21.83073  0.04535630  16.41544
  1       0.010  0.50  21.70310  0.05314850  16.32831
  1       0.010  1.00  21.54728  0.06442118  16.23721
  1       0.100  0.25  21.31273  0.07782610  16.09908
  1       0.100  0.50  21.26654  0.08288352  16.01887
  1       0.100  1.00  21.26523  0.08443366  16.00620
  2       0.001  0.25  22.04354  0.04591304  16.60255
  2       0.001  0.50  21.93496  0.04403375  16.50606
  2       0.001  1.00  21.83437  0.04749962  16.41725
  2       0.010  0.25  21.33540  0.08658603  16.06568
  2       0.010  0.50  21.04533  0.09997181  15.89610
  2       0.010  1.00  20.94335  0.10493837  15.79258
  2       0.100  0.25  20.68135  0.12566077  15.43185
  2       0.100  0.50  20.57239  0.13360708  15.27062
  2       0.100  1.00  20.52525  0.13805301  15.18170
  3       0.001  0.25  21.96615  0.04541322  16.53098
  3       0.001  0.50  21.85504  0.04805271  16.43371
  3       0.001  1.00  21.73242  0.05456202  16.34617
  3       0.010  0.25  21.06348  0.10035176  15.90657
  3       0.010  0.50  20.93871  0.10658859  15.78808
  3       0.010  1.00  20.82668  0.11496542  15.65760
  3       0.100  0.25  22.48120  0.10611676  15.93376
  3       0.100  0.50  23.39947  0.10145733  16.15208
  3       0.100  1.00  25.42275  0.08828524  16.67982

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 89 of 224 using max all ranger 3 
Random Forest 

1836 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1652, 1651, 1652, 1653, 1653, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.52602  0.4393806  11.41121
   2    extratrees  16.94914  0.4246468  12.08530
  13    variance    16.43401  0.4364902  10.97164
  13    extratrees  16.37194  0.4485272  11.22379
  24    variance    16.65521  0.4203442  11.13432
  24    extratrees  16.36692  0.4462251  11.15928

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 90 of 224 using same all ranger 3 
Random Forest 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 826, 826, 827, 827, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.65686  0.3488531  12.51331
   2    extratrees  17.83475  0.3474795  13.01226
  13    variance    17.85162  0.3303672  12.39468
  13    extratrees  17.55200  0.3561646  12.40443
  24    variance    18.09446  0.3125213  12.59179
  24    extratrees  17.59184  0.3515893  12.36589

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 91 of 224 using max du ranger 3 
Random Forest 

1836 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1652, 1654, 1652, 1651, 1653, 1652, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.90641  0.4133223  11.83498
   2    extratrees  17.52485  0.3797044  12.63523
  10    variance    16.52187  0.4301583  11.08984
  10    extratrees  16.77155  0.4217213  11.62851
  18    variance    16.65740  0.4198145  11.14909
  18    extratrees  16.70169  0.4237771  11.51303

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 92 of 224 using same du ranger 3 
Random Forest 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 828, 825, 825, 826, 827, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.90261  0.3308911  12.84462
   2    extratrees  18.21997  0.3158083  13.38570
  10    variance    17.93377  0.3245639  12.50069
  10    extratrees  17.83031  0.3360314  12.72872
  18    variance    18.13645  0.3102417  12.62762
  18    extratrees  17.81075  0.3361865  12.66329

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 93 of 224 using max rms ranger 3 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1836 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1653, 1653, 1652, 1652, 1653, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.44742  0.4336464  10.96728
  2     extratrees  16.51295  0.4386116  11.53425
  3     variance    16.51268  0.4291973  10.85395
  3     extratrees  16.48156  0.4365660  11.38849

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 94 of 224 using same rms ranger 3 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 826, 828, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.71309  0.3402665  12.18842
  2     extratrees  17.44604  0.3652587  12.49173
  3     variance    17.78698  0.3364706  12.11606
  3     extratrees  17.49007  0.3587854  12.41917

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 95 of 224 using max hudgins ranger 3 
Random Forest 

1836 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1653, 1653, 1652, 1653, 1653, 1652, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.79124  0.3459148  12.71008
   2    extratrees  18.27834  0.3169823  13.36350
   7    variance    17.25637  0.3774149  11.83316
   7    extratrees  17.58626  0.3606922  12.42720
  12    variance    17.24889  0.3769124  11.68598
  12    extratrees  17.44154  0.3691058  12.22469

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 96 of 224 using same hudgins ranger 3 
Random Forest 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 826, 826, 826, 827, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.51247  0.2821270  13.45813
   2    extratrees  18.79504  0.2651160  13.97947
   7    variance    18.36304  0.2908558  12.93194
   7    extratrees  18.36154  0.2938454  13.28538
  12    variance    18.52325  0.2809712  12.94076
  12    extratrees  18.29872  0.2975004  13.15315

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 97 of 224 using max all lm 4 
Linear Regression 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 827, 825, 827, 827, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.98346  0.1040437  16.30362

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 98 of 224 using same all lm 4 
Linear Regression 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 827, 826, 825, 826, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  20.33709  0.109958  15.90665

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 99 of 224 using max du lm 4 
Linear Regression 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 828, 827, 826, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.96903  0.1004982  16.21345

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 100 of 224 using same du lm 4 
Linear Regression 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 828, 825, 826, 827, 825, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.33476  0.1064982  15.85768

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 101 of 224 using max rms lm 4 
Linear Regression 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 826, 827, 825, 827, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.30431  0.07046952  16.97578

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 102 of 224 using same rms lm 4 
Linear Regression 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 827, 827, 826, 825, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.65492  0.07551868  16.51459

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 103 of 224 using max hudgins lm 4 
Linear Regression 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 826, 827, 825, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE   
  21.09385  0.0893403  16.567

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 104 of 224 using same hudgins lm 4 
Linear Regression 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 828, 825, 826, 826, 826, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.41449  0.1004939  16.15281

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 105 of 224 using max all knn 4 
k-Nearest Neighbors 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 828, 827, 826, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.49935  0.3161834  12.38210
  7  18.33517  0.3193680  12.56204
  9  18.39820  0.3112070  12.71196

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 106 of 224 using same all knn 4 
k-Nearest Neighbors 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 827, 827, 826, 824, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.73591  0.3355127  11.74151
  7  17.51031  0.3430246  11.87955
  9  17.49884  0.3409971  12.02572

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 107 of 224 using max du knn 4 
k-Nearest Neighbors 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 827, 825, 826, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.60806  0.3078228  12.44323
  7  18.36148  0.3158058  12.59702
  9  18.38514  0.3106219  12.74089

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 108 of 224 using same du knn 4 
k-Nearest Neighbors 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 827, 825, 827, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.62071  0.3429750  11.68239
  7  17.43274  0.3480654  11.85910
  9  17.39811  0.3479174  11.97567

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 109 of 224 using max rms knn 4 
k-Nearest Neighbors 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 827, 826, 828, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.17619  0.3347392  12.22132
  7  18.16339  0.3291829  12.52357
  9  18.36045  0.3127471  12.79626

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 110 of 224 using same rms knn 4 
k-Nearest Neighbors 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 827, 827, 826, 828, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.28185  0.3603398  11.71078
  7  17.31790  0.3529905  11.82496
  9  17.42510  0.3430965  12.00975

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 111 of 224 using max hudgins knn 4 
k-Nearest Neighbors 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 826, 826, 825, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.88507  0.1554591  15.17051
  7  20.43326  0.1712086  14.82187
  9  20.34354  0.1696454  14.77521

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 112 of 224 using same hudgins knn 4 
k-Nearest Neighbors 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 826, 825, 825, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.95091  0.1795423  14.34692
  7  19.55391  0.1929659  14.12320
  9  19.44402  0.1948690  14.03613

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 113 of 224 using max all svmPoly 4 
Support Vector Machines with Polynomial Kernel 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 826, 826, 825, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.41760  0.06200572  16.67765
  1       0.001  0.50  22.25497  0.06360031  16.62680
  1       0.001  1.00  22.13976  0.06722658  16.54442
  1       0.010  0.25  21.90945  0.07498416  16.34830
  1       0.010  0.50  21.66185  0.08546591  16.11652
  1       0.010  1.00  21.45848  0.09385480  15.90553
  1       0.100  0.25  21.36475  0.09797863  15.73019
  1       0.100  0.50  21.36277  0.09818118  15.67949
  1       0.100  1.00  21.37916  0.09918623  15.64221
  2       0.001  0.25  22.20935  0.07111700  16.57681
  2       0.001  0.50  22.05369  0.07858505  16.45719
  2       0.001  1.00  21.79660  0.09285766  16.22993
  2       0.010  0.25  20.96096  0.14246338  15.31718
  2       0.010  0.50  20.89620  0.14436202  15.16948
  2       0.010  1.00  20.90538  0.14532578  15.07186
  2       0.100  0.25  20.56734  0.16801869  14.61268
  2       0.100  0.50  20.53203  0.17145287  14.60378
  2       0.100  1.00  20.55032  0.17173333  14.65012
  3       0.001  0.25  22.05981  0.08434513  16.44661
  3       0.001  0.50  21.80553  0.10112704  16.21006
  3       0.001  1.00  21.43389  0.12291866  15.87068
  3       0.010  0.25  20.81909  0.14955017  15.03624
  3       0.010  0.50  20.78285  0.15194531  14.86280
  3       0.010  1.00  20.74568  0.15576474  14.72367
  3       0.100  0.25  24.07925  0.12142713  15.78521
  3       0.100  0.50  26.73329  0.10745741  16.51182
  3       0.100  1.00  29.46587  0.09013535  17.43724

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 114 of 224 using same all svmPoly 4 
Support Vector Machines with Polynomial Kernel 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 826, 828, 826, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.68926  0.06139170  16.28111
  1       0.001  0.50  21.56186  0.06334089  16.22403
  1       0.001  1.00  21.42190  0.06650332  16.15302
  1       0.010  0.25  21.21536  0.07521422  15.95770
  1       0.010  0.50  21.01392  0.08506211  15.78031
  1       0.010  1.00  20.83280  0.09436857  15.60038
  1       0.100  0.25  20.75673  0.10014807  15.41960
  1       0.100  0.50  20.73949  0.10060403  15.38844
  1       0.100  1.00  20.75646  0.10001849  15.38140
  2       0.001  0.25  21.51995  0.06994252  16.17912
  2       0.001  0.50  21.33922  0.07792966  16.06376
  2       0.001  1.00  21.11895  0.09077177  15.85906
  2       0.010  0.25  20.39449  0.14091099  15.01284
  2       0.010  0.50  20.30038  0.14366087  14.85729
  2       0.010  1.00  20.28140  0.14646111  14.74463
  2       0.100  0.25  19.80030  0.17929951  14.16776
  2       0.100  0.50  19.76188  0.18254042  14.15522
  2       0.100  1.00  19.76675  0.18333472  14.16051
  3       0.001  0.25  21.35263  0.08349471  16.05439
  3       0.001  0.50  21.12433  0.09900796  15.84439
  3       0.001  1.00  20.80385  0.11953814  15.54554
  3       0.010  0.25  20.18087  0.15261998  14.69474
  3       0.010  0.50  20.11884  0.15859557  14.48249
  3       0.010  1.00  20.05991  0.16273589  14.35007
  3       0.100  0.25  22.57674  0.13872144  15.07302
  3       0.100  0.50  25.71908  0.12033463  15.74584
  3       0.100  1.00  29.40156  0.10547941  16.59763

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 115 of 224 using max du svmPoly 4 
Support Vector Machines with Polynomial Kernel 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 824, 826, 826, 827, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.45755  0.05408180  16.70333
  1       0.001  0.50  22.36928  0.05553759  16.65594
  1       0.001  1.00  22.26080  0.05901518  16.58653
  1       0.010  0.25  22.08421  0.06730542  16.41834
  1       0.010  0.50  21.86034  0.07995455  16.21837
  1       0.010  1.00  21.61653  0.09069391  16.01669
  1       0.100  0.25  21.37975  0.09847354  15.79130
  1       0.100  0.50  21.34360  0.10001820  15.69931
  1       0.100  1.00  21.35024  0.10027947  15.65852
  2       0.001  0.25  22.35294  0.05905430  16.63675
  2       0.001  0.50  22.22963  0.06469465  16.54682
  2       0.001  1.00  22.06041  0.07450582  16.39380
  2       0.010  0.25  21.29681  0.13193207  15.65598
  2       0.010  0.50  21.07618  0.13301846  15.49090
  2       0.010  1.00  21.02313  0.13435476  15.34534
  2       0.100  0.25  20.70082  0.15792991  14.76901
  2       0.100  0.50  20.65304  0.16149949  14.74105
  2       0.100  1.00  20.63859  0.16383075  14.75137
  3       0.001  0.25  22.25547  0.06664890  16.56066
  3       0.001  0.50  22.08434  0.07764391  16.41005
  3       0.001  1.00  21.82049  0.09746698  16.16507
  3       0.010  0.25  21.07693  0.13424767  15.48948
  3       0.010  0.50  20.94499  0.14152401  15.22643
  3       0.010  1.00  20.87884  0.14655106  15.03497
  3       0.100  0.25  22.04363  0.13002385  15.47698
  3       0.100  0.50  23.71752  0.11514025  15.93356
  3       0.100  1.00  26.57363  0.09700822  16.73609

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 116 of 224 using same du svmPoly 4 
Support Vector Machines with Polynomial Kernel 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 827, 826, 825, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.78351  0.05469375  16.32746
  1       0.001  0.50  21.69411  0.05391747  16.27527
  1       0.001  1.00  21.56563  0.05536602  16.21574
  1       0.010  0.25  21.39021  0.06351171  16.05971
  1       0.010  0.50  21.22671  0.07454501  15.88651
  1       0.010  1.00  21.00013  0.08718731  15.71394
  1       0.100  0.25  20.77124  0.09732087  15.49919
  1       0.100  0.50  20.74253  0.09908091  15.42858
  1       0.100  1.00  20.75757  0.09924099  15.40977
  2       0.001  0.25  21.67731  0.05643302  16.25694
  2       0.001  0.50  21.53104  0.06071869  16.17613
  2       0.001  1.00  21.37521  0.06922547  16.03946
  2       0.010  0.25  20.68616  0.13253237  15.36510
  2       0.010  0.50  20.44382  0.13627522  15.16804
  2       0.010  1.00  20.39427  0.13766409  15.02467
  2       0.100  0.25  20.01841  0.16386868  14.41050
  2       0.100  0.50  19.96557  0.16757438  14.34368
  2       0.100  1.00  19.97263  0.16825709  14.37174
  3       0.001  0.25  21.56352  0.06266808  16.18448
  3       0.001  0.50  21.39425  0.07214360  16.05401
  3       0.001  1.00  21.19040  0.08901735  15.84836
  3       0.010  0.25  20.44408  0.13647557  15.15172
  3       0.010  0.50  20.35726  0.14252796  14.91611
  3       0.010  1.00  20.23598  0.15038437  14.67499
  3       0.100  0.25  21.04416  0.13896983  14.91038
  3       0.100  0.50  22.49904  0.12373311  15.32256
  3       0.100  1.00  26.08509  0.10390499  16.17376

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 117 of 224 using max rms svmPoly 4 
Support Vector Machines with Polynomial Kernel 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 825, 826, 826, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.90450  0.06685125  16.83186
  1       0.001  0.50  22.60075  0.06659694  16.73693
  1       0.001  1.00  22.52346  0.06512873  16.68090
  1       0.010  0.25  22.25491  0.06300916  16.63903
  1       0.010  0.50  22.18745  0.06166390  16.64477
  1       0.010  1.00  22.13659  0.06045613  16.65613
  1       0.100  0.25  22.10856  0.05875891  16.67999
  1       0.100  0.50  22.10249  0.05806134  16.68909
  1       0.100  1.00  22.09781  0.05768261  16.69362
  2       0.001  0.25  22.60031  0.06694854  16.73581
  2       0.001  0.50  22.52111  0.06564973  16.67876
  2       0.001  1.00  22.28229  0.06445926  16.63501
  2       0.010  0.25  22.02897  0.07832982  16.50151
  2       0.010  0.50  21.83172  0.08938488  16.37019
  2       0.010  1.00  21.53990  0.11097641  16.11483
  2       0.100  0.25  20.81044  0.15010810  15.00532
  2       0.100  0.50  20.80057  0.14860918  15.00134
  2       0.100  1.00  20.81475  0.14827632  15.00583
  3       0.001  0.25  22.57585  0.06674595  16.69590
  3       0.001  0.50  22.35738  0.06561404  16.64784
  3       0.001  1.00  22.21142  0.06492179  16.62181
  3       0.010  0.25  21.72435  0.10584466  16.24123
  3       0.010  0.50  21.35310  0.12813550  15.86518
  3       0.010  1.00  21.06154  0.14545377  15.40509
  3       0.100  0.25  20.64634  0.16444014  14.62409
  3       0.100  0.50  20.62920  0.16703697  14.48567
  3       0.100  1.00  20.64240  0.16783985  14.38892

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 118 of 224 using same rms svmPoly 4 
Support Vector Machines with Polynomial Kernel 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 827, 826, 826, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.95047  0.07194765  16.42464
  1       0.001  0.50  21.92119  0.07433659  16.36763
  1       0.001  1.00  21.71668  0.07453418  16.29958
  1       0.010  0.25  21.48923  0.07404659  16.24462
  1       0.010  0.50  21.36178  0.07358541  16.23664
  1       0.010  1.00  21.25738  0.07237240  16.24250
  1       0.100  0.25  21.18157  0.07105873  16.24693
  1       0.100  0.50  21.15823  0.06997380  16.24888
  1       0.100  1.00  21.14140  0.06926185  16.25050
  2       0.001  0.25  21.92083  0.07471025  16.36644
  2       0.001  0.50  21.71445  0.07508837  16.29684
  2       0.001  1.00  21.53288  0.07526212  16.24861
  2       0.010  0.25  21.22139  0.09386547  16.08462
  2       0.010  0.50  21.00818  0.10630823  15.96302
  2       0.010  1.00  20.75233  0.12460810  15.73912
  2       0.100  0.25  20.23690  0.15187586  14.73033
  2       0.100  0.50  20.24611  0.15099766  14.72544
  2       0.100  1.00  20.24890  0.15082048  14.72993
  3       0.001  0.25  21.82709  0.07573926  16.32704
  3       0.001  0.50  21.59840  0.07609298  16.25729
  3       0.001  1.00  21.43356  0.07669340  16.22018
  3       0.010  0.25  20.95154  0.12304465  15.84744
  3       0.010  0.50  20.63021  0.13837016  15.52382
  3       0.010  1.00  20.42916  0.14691462  15.14348
  3       0.100  0.25  19.97119  0.16982737  14.30326
  3       0.100  0.50  19.93527  0.17389018  14.15510
  3       0.100  1.00  19.93339  0.17578352  14.07352

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 119 of 224 using max hudgins svmPoly 4 
Support Vector Machines with Polynomial Kernel 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 825, 827, 828, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.52616  0.04829231  16.77495
  1       0.001  0.50  22.48536  0.04739425  16.72683
  1       0.001  1.00  22.41864  0.04785686  16.68886
  1       0.010  0.25  22.28324  0.05157150  16.59946
  1       0.010  0.50  22.15228  0.05820429  16.47406
  1       0.010  1.00  21.95452  0.06775355  16.32098
  1       0.100  0.25  21.69809  0.07946555  16.08561
  1       0.100  0.50  21.62033  0.08280071  15.98468
  1       0.100  1.00  21.61921  0.08327158  15.93397
  2       0.001  0.25  22.48129  0.04875500  16.71995
  2       0.001  0.50  22.40785  0.05004558  16.67476
  2       0.001  1.00  22.28686  0.05395211  16.59975
  2       0.010  0.25  21.67275  0.11444964  16.00075
  2       0.010  0.50  21.30999  0.12364485  15.73298
  2       0.010  1.00  21.18211  0.12405683  15.56982
  2       0.100  0.25  21.04680  0.13339565  15.15600
  2       0.100  0.50  20.92896  0.14234283  14.99415
  2       0.100  1.00  20.91630  0.14434680  14.98515
  3       0.001  0.25  22.44293  0.05110101  16.68076
  3       0.001  0.50  22.32431  0.05486803  16.61628
  3       0.001  1.00  22.16598  0.06331369  16.47502
  3       0.010  0.25  21.31523  0.12272721  15.74297
  3       0.010  0.50  21.17181  0.12380370  15.56161
  3       0.010  1.00  21.10589  0.12636963  15.41521
  3       0.100  0.25  21.72800  0.12128563  15.54891
  3       0.100  0.50  22.11040  0.11986854  15.66915
  3       0.100  1.00  23.03885  0.11200393  15.93438

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 120 of 224 using same hudgins svmPoly 4 
Support Vector Machines with Polynomial Kernel 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 826, 826, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.84810  0.05025471  16.37672
  1       0.001  0.50  21.76312  0.04800286  16.33457
  1       0.001  1.00  21.68989  0.04511395  16.29709
  1       0.010  0.25  21.58802  0.04486893  16.22140
  1       0.010  0.50  21.48837  0.05057989  16.12219
  1       0.010  1.00  21.31601  0.06058813  15.99614
  1       0.100  0.25  21.06091  0.07377381  15.80838
  1       0.100  0.50  21.01238  0.07825446  15.70600
  1       0.100  1.00  20.97884  0.08056161  15.65225
  2       0.001  0.25  21.75898  0.04894199  16.32825
  2       0.001  0.50  21.67843  0.04694757  16.28229
  2       0.001  1.00  21.58795  0.04801602  16.21654
  2       0.010  0.25  21.02759  0.11271172  15.69086
  2       0.010  0.50  20.73019  0.12043581  15.49983
  2       0.010  1.00  20.56687  0.12382645  15.31200
  2       0.100  0.25  20.41897  0.13854930  14.86701
  2       0.100  0.50  20.33152  0.14450693  14.75861
  2       0.100  1.00  20.25961  0.14973105  14.71112
  3       0.001  0.25  21.70795  0.04892397  16.29433
  3       0.001  0.50  21.60823  0.04967474  16.22750
  3       0.001  1.00  21.48620  0.05734437  16.11801
  3       0.010  0.25  20.74474  0.11816732  15.49398
  3       0.010  0.50  20.57559  0.12310322  15.29443
  3       0.010  1.00  20.48296  0.12694101  15.13043
  3       0.100  0.25  20.73688  0.13629850  15.02014
  3       0.100  0.50  21.13229  0.13206528  15.18423
  3       0.100  1.00  21.97429  0.12799159  15.40343

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 121 of 224 using max all ranger 4 
Random Forest 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 825, 826, 828, 826, 827, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.61712  0.3694079  12.35519
   2    extratrees  17.76650  0.3701888  12.80424
  13    variance    17.75109  0.3550741  12.18962
  13    extratrees  17.48596  0.3786412  12.27115
  24    variance    17.98935  0.3380468  12.39508
  24    extratrees  17.49443  0.3767882  12.24251

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 122 of 224 using same all ranger 4 
Random Forest 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 827, 825, 825, 827, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.70185  0.4047644  11.64185
   2    extratrees  16.96138  0.3966860  12.20263
  13    variance    16.72476  0.3961100  11.37139
  13    extratrees  16.60008  0.4099379  11.52282
  24    variance    16.98035  0.3778180  11.57996
  24    extratrees  16.61448  0.4073332  11.47829

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 123 of 224 using max du ranger 4 
Random Forest 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 828, 826, 827, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.86047  0.3520231  12.65602
   2    extratrees  18.18103  0.3385002  13.21960
  10    variance    17.78988  0.3509323  12.27609
  10    extratrees  17.74514  0.3602626  12.58850
  18    variance    17.93865  0.3398918  12.38908
  18    extratrees  17.77299  0.3565914  12.57296

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = extratrees
 and min.node.size = 5.


Now processing model 124 of 224 using same du ranger 4 
Random Forest 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 827, 828, 825, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.02313  0.3797639  11.96081
   2    extratrees  17.38674  0.3611384  12.60781
  10    variance    16.79379  0.3897316  11.45196
  10    extratrees  16.94328  0.3844747  11.86865
  18    variance    16.94211  0.3785164  11.56153
  18    extratrees  16.91486  0.3847478  11.79926

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 125 of 224 using max rms ranger 4 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 827, 825, 828, 825, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.72681  0.3550818  12.09903
  2     extratrees  17.43131  0.3806380  12.38519
  3     variance    17.86416  0.3476447  12.10592
  3     extratrees  17.52377  0.3707695  12.33895

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 126 of 224 using same rms ranger 4 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 828, 827, 825, 826, 828, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.84341  0.3865735  11.41650
  2     extratrees  16.62153  0.4091525  11.77000
  3     variance    16.93747  0.3811859  11.38998
  3     extratrees  16.68093  0.4016313  11.69123

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 127 of 224 using max hudgins ranger 4 
Random Forest 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 827, 825, 828, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.42228  0.3098366  13.26697
   2    extratrees  18.64703  0.3022381  13.74160
   7    variance    18.11520  0.3286673  12.67912
   7    extratrees  18.19804  0.3280824  13.08140
  12    variance    18.15090  0.3267171  12.59960
  12    extratrees  18.13633  0.3306685  12.96093

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 128 of 224 using same hudgins ranger 4 
Random Forest 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 824, 826, 826, 826, 827, 827, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.55564  0.3396411  12.56999
   2    extratrees  17.89262  0.3215551  13.16498
   7    variance    17.07486  0.3698279  11.80272
   7    extratrees  17.31181  0.3585501  12.33204
  12    variance    17.03068  0.3730539  11.68822
  12    extratrees  17.21660  0.3638263  12.18621

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 129 of 224 using max all lm 5 
Linear Regression 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 825, 827, 826, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.54809  0.1158712  16.02577

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 130 of 224 using same all lm 5 
Linear Regression 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 826, 827, 827, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.66053  0.1250019  16.16457

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 131 of 224 using max du lm 5 
Linear Regression 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 827, 825, 825, 827, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.57235  0.1149376  15.96294

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 132 of 224 using same du lm 5 
Linear Regression 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 827, 825, 826, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE    
  20.70935  0.1210963  16.0832

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 133 of 224 using max rms lm 5 
Linear Regression 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 827, 826, 826, 828, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.92304  0.08107201  16.76763

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 134 of 224 using same rms lm 5 
Linear Regression 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 827, 826, 826, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.15907  0.08059822  16.93351

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 135 of 224 using max hudgins lm 5 
Linear Regression 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 826, 827, 825, 825, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.70299  0.1027119  16.30808

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 136 of 224 using same hudgins lm 5 
Linear Regression 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 827, 827, 827, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.81885  0.1116214  16.44402

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 137 of 224 using max all knn 5 
k-Nearest Neighbors 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 827, 827, 826, 825, 828, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.30003  0.3160392  12.10382
  7  18.14577  0.3179655  12.23485
  9  18.09639  0.3181274  12.38863

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 138 of 224 using same all knn 5 
k-Nearest Neighbors 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 826, 825, 826, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.28091  0.3306010  12.05944
  7  18.07727  0.3367559  12.17103
  9  18.09998  0.3323471  12.41247

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 139 of 224 using max du knn 5 
k-Nearest Neighbors 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 827, 826, 827, 826, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.29556  0.3176262  12.08234
  7  18.11210  0.3196766  12.21766
  9  18.17068  0.3122986  12.44059

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 140 of 224 using same du knn 5 
k-Nearest Neighbors 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 825, 827, 827, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.16205  0.3375314  11.96454
  7  18.03105  0.3389991  12.11569
  9  18.09505  0.3315282  12.41343

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 141 of 224 using max rms knn 5 
k-Nearest Neighbors 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 825, 825, 828, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.05567  0.3268309  12.09023
  7  18.02256  0.3224470  12.34653
  9  18.08296  0.3154878  12.57544

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 142 of 224 using same rms knn 5 
k-Nearest Neighbors 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 827, 825, 825, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.24916  0.3294552  12.04345
  7  18.22013  0.3246867  12.32462
  9  18.27571  0.3168351  12.55116

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 143 of 224 using max hudgins knn 5 
k-Nearest Neighbors 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 828, 826, 826, 827, 826, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.08721  0.1938287  14.44941
  7  19.82467  0.1982706  14.50294
  9  19.77099  0.1963824  14.49245

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 144 of 224 using same hudgins knn 5 
k-Nearest Neighbors 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 827, 827, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.28795  0.1943101  14.50812
  7  20.00531  0.1987818  14.56317
  9  19.75625  0.2098458  14.39798

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 145 of 224 using max all svmPoly 5 
Support Vector Machines with Polynomial Kernel 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 826, 828, 826, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.02607  0.07257905  16.52763
  1       0.001  0.50  21.83701  0.07574003  16.46358
  1       0.001  1.00  21.63846  0.08061965  16.33867
  1       0.010  0.25  21.35742  0.09101297  16.08689
  1       0.010  0.50  21.13373  0.09976419  15.85899
  1       0.010  1.00  21.00806  0.10569032  15.67631
  1       0.100  0.25  20.94260  0.10902585  15.53817
  1       0.100  0.50  20.92454  0.10970007  15.49921
  1       0.100  1.00  20.93803  0.10943521  15.49329
  2       0.001  0.25  21.79350  0.08218879  16.41601
  2       0.001  0.50  21.55364  0.09128939  16.24888
  2       0.001  1.00  21.25115  0.10463459  15.98552
  2       0.010  0.25  20.60483  0.14144685  15.20464
  2       0.010  0.50  20.55271  0.14197391  15.08682
  2       0.010  1.00  20.51596  0.14712347  14.94666
  2       0.100  0.25  20.08019  0.17831297  14.36301
  2       0.100  0.50  19.99517  0.18459392  14.27883
  2       0.100  1.00  20.01708  0.18478531  14.28102
  3       0.001  0.25  21.59410  0.09521278  16.25825
  3       0.001  0.50  21.28184  0.11044251  15.99394
  3       0.001  1.00  20.95558  0.12640120  15.67721
  3       0.010  0.25  20.41442  0.15310700  14.88259
  3       0.010  0.50  20.36741  0.15734479  14.71331
  3       0.010  1.00  20.34920  0.15980789  14.61487
  3       0.100  0.25  21.83493  0.13533003  15.12793
  3       0.100  0.50  23.20261  0.11739672  15.67819
  3       0.100  1.00  25.03530  0.09850708  16.36818

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 146 of 224 using same all svmPoly 5 
Support Vector Machines with Polynomial Kernel 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 827, 826, 826, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.26018  0.07007003  16.74837
  1       0.001  0.50  22.09908  0.07227761  16.68937
  1       0.001  1.00  21.89561  0.07752180  16.57836
  1       0.010  0.25  21.57676  0.09024598  16.34363
  1       0.010  0.50  21.33698  0.10067168  16.13111
  1       0.010  1.00  21.21073  0.10764760  15.96237
  1       0.100  0.25  21.08418  0.11443926  15.79243
  1       0.100  0.50  21.01907  0.11836037  15.70652
  1       0.100  1.00  21.01275  0.12012683  15.67778
  2       0.001  0.25  22.05594  0.07848335  16.64456
  2       0.001  0.50  21.81458  0.08790943  16.48995
  2       0.001  1.00  21.49838  0.10209863  16.25247
  2       0.010  0.25  20.76566  0.14776711  15.37936
  2       0.010  0.50  20.68541  0.15031231  15.24948
  2       0.010  1.00  20.63040  0.15418528  15.13077
  2       0.100  0.25  20.20140  0.18365371  14.51695
  2       0.100  0.50  20.11714  0.19084875  14.39914
  2       0.100  1.00  20.13316  0.19203370  14.41160
  3       0.001  0.25  21.85629  0.09188859  16.49471
  3       0.001  0.50  21.53257  0.10745655  16.25402
  3       0.001  1.00  21.15854  0.12732887  15.91870
  3       0.010  0.25  20.58352  0.15929140  15.06723
  3       0.010  0.50  20.48895  0.16454452  14.89323
  3       0.010  1.00  20.40110  0.17050776  14.74972
  3       0.100  0.25  22.31175  0.14127767  15.33975
  3       0.100  0.50  23.30744  0.12701767  15.75714
  3       0.100  1.00  24.55338  0.10970552  16.41392

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 147 of 224 using max du svmPoly 5 
Support Vector Machines with Polynomial Kernel 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 827, 826, 827, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.17024  0.06563777  16.58582
  1       0.001  0.50  22.00337  0.06536385  16.52797
  1       0.001  1.00  21.84948  0.06762159  16.43618
  1       0.010  0.25  21.60644  0.07782807  16.20711
  1       0.010  0.50  21.38166  0.08938824  16.01754
  1       0.010  1.00  21.12257  0.10187889  15.79150
  1       0.100  0.25  20.94759  0.10885078  15.59051
  1       0.100  0.50  20.93048  0.10945049  15.54144
  1       0.100  1.00  20.95484  0.10846100  15.54093
  2       0.001  0.25  21.98238  0.06829364  16.50690
  2       0.001  0.50  21.81714  0.07239942  16.39591
  2       0.001  1.00  21.59201  0.08228696  16.20026
  2       0.010  0.25  20.79363  0.13736702  15.48632
  2       0.010  0.50  20.68360  0.13838591  15.32569
  2       0.010  1.00  20.64111  0.13851300  15.20361
  2       0.100  0.25  20.27479  0.16540611  14.63267
  2       0.100  0.50  20.22147  0.16963205  14.55175
  2       0.100  1.00  20.19941  0.17288473  14.51833
  3       0.001  0.25  21.85503  0.07420290  16.41843
  3       0.001  0.50  21.63372  0.08381115  16.23520
  3       0.001  1.00  21.34686  0.10216571  15.97585
  3       0.010  0.25  20.66624  0.14004166  15.30613
  3       0.010  0.50  20.55161  0.14524136  15.09968
  3       0.010  1.00  20.49360  0.14953169  14.94200
  3       0.100  0.25  21.25465  0.14150256  15.09345
  3       0.100  0.50  22.89581  0.11828991  15.64321
  3       0.100  1.00  24.83951  0.10499805  16.19411

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 148 of 224 using same du svmPoly 5 
Support Vector Machines with Polynomial Kernel 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 826, 826, 826, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.39582  0.06315795  16.79813
  1       0.001  0.50  22.25590  0.06243278  16.73504
  1       0.001  1.00  22.09085  0.06494825  16.64865
  1       0.010  0.25  21.82905  0.07691513  16.45265
  1       0.010  0.50  21.56211  0.09093591  16.25216
  1       0.010  1.00  21.29227  0.10426566  16.03222
  1       0.100  0.25  21.09000  0.11391341  15.82054
  1       0.100  0.50  21.01675  0.11726926  15.72388
  1       0.100  1.00  20.97229  0.11960889  15.66496
  2       0.001  0.25  22.24081  0.06500806  16.71616
  2       0.001  0.50  22.05849  0.06977907  16.61267
  2       0.001  1.00  21.83465  0.08091471  16.44755
  2       0.010  0.25  20.96798  0.14198680  15.66553
  2       0.010  0.50  20.82804  0.14473372  15.48276
  2       0.010  1.00  20.71392  0.14686926  15.34112
  2       0.100  0.25  20.34983  0.17173703  14.76795
  2       0.100  0.50  20.27959  0.17773921  14.65835
  2       0.100  1.00  20.27285  0.18002984  14.63036
  3       0.001  0.25  22.12715  0.07087785  16.63472
  3       0.001  0.50  21.88116  0.08202525  16.47421
  3       0.001  1.00  21.56674  0.10158959  16.22882
  3       0.010  0.25  20.83674  0.14437830  15.48318
  3       0.010  0.50  20.67838  0.15069712  15.25157
  3       0.010  1.00  20.57869  0.15815666  15.06637
  3       0.100  0.25  21.51342  0.13946601  15.40917
  3       0.100  0.50  22.69673  0.12363027  15.72435
  3       0.100  1.00  24.35933  0.10404288  16.28718

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 149 of 224 using max rms svmPoly 5 
Support Vector Machines with Polynomial Kernel 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 825, 827, 825, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.53064  0.07593938  16.73863
  1       0.001  0.50  22.34795  0.07624234  16.65798
  1       0.001  1.00  22.14123  0.07550001  16.57022
  1       0.010  0.25  21.82168  0.07523378  16.50773
  1       0.010  0.50  21.67642  0.07353973  16.51530
  1       0.010  1.00  21.60188  0.07031183  16.53431
  1       0.100  0.25  21.54924  0.06727156  16.55826
  1       0.100  0.50  21.52170  0.06642618  16.56119
  1       0.100  1.00  21.51037  0.06600902  16.56462
  2       0.001  0.25  22.34743  0.07665694  16.65676
  2       0.001  0.50  22.13801  0.07608396  16.56754
  2       0.001  1.00  21.86865  0.07625260  16.50753
  2       0.010  0.25  21.51740  0.09189100  16.36237
  2       0.010  0.50  21.29013  0.10300703  16.22206
  2       0.010  1.00  21.00801  0.12147712  15.97526
  2       0.100  0.25  20.59044  0.14609514  14.97474
  2       0.100  0.50  20.62286  0.14384210  14.97994
  2       0.100  1.00  20.64389  0.14284599  14.98926
  3       0.001  0.25  22.28796  0.07673682  16.60173
  3       0.001  0.50  21.95782  0.07647322  16.52367
  3       0.001  1.00  21.75989  0.07718342  16.49030
  3       0.010  0.25  21.20039  0.11825518  16.08988
  3       0.010  0.50  20.85783  0.13756091  15.73516
  3       0.010  1.00  20.66970  0.14917618  15.33954
  3       0.100  0.25  20.39913  0.16226192  14.55466
  3       0.100  0.50  20.37819  0.16470220  14.41163
  3       0.100  1.00  20.41227  0.16459474  14.35209

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 150 of 224 using same rms svmPoly 5 
Support Vector Machines with Polynomial Kernel 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 826, 825, 827, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.59852  0.07817337  16.94937
  1       0.001  0.50  22.52882  0.07904821  16.87773
  1       0.001  1.00  22.27931  0.07764333  16.77774
  1       0.010  0.25  21.98205  0.07744817  16.70303
  1       0.010  0.50  21.80776  0.07603243  16.68965
  1       0.010  1.00  21.71298  0.07267314  16.69027
  1       0.100  0.25  21.63972  0.06950588  16.68336
  1       0.100  0.50  21.61342  0.06919123  16.67984
  1       0.100  1.00  21.60168  0.06887400  16.67991
  2       0.001  0.25  22.52840  0.07940999  16.87655
  2       0.001  0.50  22.27699  0.07816905  16.77507
  2       0.001  1.00  22.05156  0.07838270  16.70908
  2       0.010  0.25  21.66609  0.09372467  16.53933
  2       0.010  0.50  21.42207  0.10471492  16.38269
  2       0.010  1.00  21.12293  0.12288584  16.13521
  2       0.100  0.25  20.64844  0.15700788  15.12997
  2       0.100  0.50  20.63581  0.15733679  15.12181
  2       0.100  1.00  20.65361  0.15656562  15.13240
  3       0.001  0.25  22.38465  0.07891250  16.81545
  3       0.001  0.50  22.13856  0.07886809  16.72824
  3       0.001  1.00  21.91159  0.07975037  16.68020
  3       0.010  0.25  21.35244  0.11834688  16.25939
  3       0.010  0.50  21.01835  0.13890225  15.91478
  3       0.010  1.00  20.73213  0.15366639  15.48715
  3       0.100  0.25  20.43550  0.17412463  14.71512
  3       0.100  0.50  20.40635  0.17711575  14.57715
  3       0.100  1.00  20.42771  0.17787859  14.51050

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 151 of 224 using max hudgins svmPoly 5 
Support Vector Machines with Polynomial Kernel 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 827, 827, 826, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.23482  0.05879970  16.66216
  1       0.001  0.50  22.15596  0.05793117  16.59813
  1       0.001  1.00  22.02642  0.05705469  16.54587
  1       0.010  0.25  21.83824  0.05999400  16.42770
  1       0.010  0.50  21.67824  0.06817843  16.27778
  1       0.010  1.00  21.48329  0.07772437  16.11056
  1       0.100  0.25  21.25524  0.08724079  15.88742
  1       0.100  0.50  21.20518  0.08992662  15.79091
  1       0.100  1.00  21.18073  0.09168310  15.73437
  2       0.001  0.25  22.14957  0.05905962  16.59129
  2       0.001  0.50  22.01166  0.05916260  16.53123
  2       0.001  1.00  21.87638  0.06099156  16.44271
  2       0.010  0.25  21.21089  0.11718482  15.84280
  2       0.010  0.50  20.83253  0.12905753  15.54792
  2       0.010  1.00  20.72655  0.13158642  15.38374
  2       0.100  0.25  20.61111  0.14197728  15.01770
  2       0.100  0.50  20.53449  0.14724334  14.91244
  2       0.100  1.00  20.47465  0.15133127  14.85919
  3       0.001  0.25  22.06295  0.06069385  16.54480
  3       0.001  0.50  21.92364  0.06209165  16.46702
  3       0.001  1.00  21.70237  0.07205289  16.29785
  3       0.010  0.25  20.81682  0.13004815  15.55307
  3       0.010  0.50  20.71927  0.13206946  15.38728
  3       0.010  1.00  20.64671  0.13535195  15.25728
  3       0.100  0.25  20.95452  0.14017836  15.10323
  3       0.100  0.50  21.07519  0.14025310  15.16144
  3       0.100  1.00  21.75329  0.12627600  15.47976

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 152 of 224 using same hudgins svmPoly 5 
Support Vector Machines with Polynomial Kernel 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 826, 826, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.47941  0.05462200  16.87945
  1       0.001  0.50  22.41151  0.05338810  16.82147
  1       0.001  1.00  22.29355  0.05218599  16.77119
  1       0.010  0.25  22.11349  0.05432781  16.68320
  1       0.010  0.50  21.93129  0.06401017  16.55673
  1       0.010  1.00  21.66824  0.07541701  16.40265
  1       0.100  0.25  21.44136  0.08788404  16.17797
  1       0.100  0.50  21.33762  0.09351827  16.06157
  1       0.100  1.00  21.32625  0.09565804  16.01721
  2       0.001  0.25  22.40864  0.05423202  16.81540
  2       0.001  0.50  22.28296  0.05379262  16.75796
  2       0.001  1.00  22.13340  0.05612376  16.68393
  2       0.010  0.25  21.43146  0.11492815  16.09747
  2       0.010  0.50  21.08087  0.12866128  15.79199
  2       0.010  1.00  20.90162  0.13487267  15.58522
  2       0.100  0.25  20.74075  0.14415474  15.22892
  2       0.100  0.50  20.70817  0.14832563  15.14190
  2       0.100  1.00  20.70781  0.15111662  15.11599
  3       0.001  0.25  22.32977  0.05508809  16.77236
  3       0.001  0.50  22.17891  0.05723881  16.70151
  3       0.001  1.00  21.99225  0.06533204  16.57089
  3       0.010  0.25  21.04937  0.13194290  15.75743
  3       0.010  0.50  20.92042  0.13303120  15.59143
  3       0.010  1.00  20.82127  0.13661479  15.45334
  3       0.100  0.25  21.36851  0.13272559  15.54968
  3       0.100  0.50  21.80023  0.12405324  15.75185
  3       0.100  1.00  22.45594  0.11509244  15.93337

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 153 of 224 using max all ranger 5 
Random Forest 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 827, 826, 825, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.96927  0.4040576  11.85414
   2    extratrees  17.23989  0.3973448  12.43728
  13    variance    17.05748  0.3900991  11.63375
  13    extratrees  16.84895  0.4113119  11.74103
  24    variance    17.32997  0.3706269  11.83105
  24    extratrees  16.86055  0.4086082  11.69255

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 154 of 224 using same all ranger 5 
Random Forest 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 826, 827, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.01228  0.4137567  11.86632
   2    extratrees  17.32914  0.4047543  12.44167
  13    variance    17.11369  0.3980600  11.64266
  13    extratrees  16.89533  0.4205955  11.70969
  24    variance    17.38313  0.3788109  11.84255
  24    extratrees  16.91562  0.4168225  11.66028

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 155 of 224 using max du ranger 5 
Random Forest 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 825, 826, 828, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.23799  0.3851100  12.16346
   2    extratrees  17.65870  0.3645055  12.84932
  10    variance    17.12268  0.3856387  11.74823
  10    extratrees  17.16249  0.3893794  12.07641
  18    variance    17.31475  0.3718476  11.86683
  18    extratrees  17.16995  0.3865805  12.02352

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 156 of 224 using same du ranger 5 
Random Forest 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 826, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.31505  0.3939393  12.19784
   2    extratrees  17.75724  0.3721746  12.87228
  10    variance    17.19697  0.3941754  11.76227
  10    extratrees  17.22590  0.3991106  12.08100
  18    variance    17.38778  0.3804821  11.86070
  18    extratrees  17.19895  0.3990164  11.99109

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 157 of 224 using max rms ranger 5 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 826, 826, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.93811  0.3987483  11.57033
  2     extratrees  16.82299  0.4170737  11.98841
  3     variance    17.06147  0.3907905  11.55883
  3     extratrees  16.82401  0.4125000  11.89403

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 158 of 224 using same rms ranger 5 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 826, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.97761  0.4079364  11.53574
  2     extratrees  16.82269  0.4283063  11.88059
  3     variance    17.10980  0.3997540  11.50613
  3     extratrees  16.85481  0.4214874  11.80107

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 159 of 224 using max hudgins ranger 5 
Random Forest 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 826, 827, 825, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.84585  0.3369361  12.75200
   2    extratrees  18.16120  0.3227062  13.39000
   7    variance    17.58279  0.3517808  12.18176
   7    extratrees  17.66687  0.3512951  12.60490
  12    variance    17.61928  0.3495903  12.10624
  12    extratrees  17.58123  0.3560638  12.45234

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 160 of 224 using same hudgins ranger 5 
Random Forest 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 827, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.03194  0.3380468  12.91405
   2    extratrees  18.33021  0.3251112  13.50052
   7    variance    17.68052  0.3593210  12.27136
   7    extratrees  17.77397  0.3579153  12.67614
  12    variance    17.71493  0.3572675  12.18463
  12    extratrees  17.68430  0.3623967  12.51694

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 161 of 224 using max all lm 6 
Linear Regression 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 826, 827, 825, 827, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.81832  0.09932547  16.26601

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 162 of 224 using same all lm 6 
Linear Regression 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 828, 827, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.79123  0.1030037  16.28553

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 163 of 224 using max du lm 6 
Linear Regression 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 827, 825, 826, 827, ... 
Resampling results:

  RMSE      Rsquared    MAE   
  20.81816  0.09984439  16.183

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 164 of 224 using same du lm 6 
Linear Regression 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 827, 825, 827, 826, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.81732  0.09860071  16.24142

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 165 of 224 using max rms lm 6 
Linear Regression 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 825, 827, 825, 826, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.09004  0.07103538  16.80417

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 166 of 224 using same rms lm 6 
Linear Regression 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 826, 826, 826, 827, ... 
Resampling results:

  RMSE     Rsquared    MAE     
  21.1189  0.06915242  16.88738

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 167 of 224 using max hudgins lm 6 
Linear Regression 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 826, 827, 825, 827, ... 
Resampling results:

  RMSE     Rsquared    MAE     
  20.8617  0.09444868  16.45466

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 168 of 224 using same hudgins lm 6 
Linear Regression 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 826, 826, 826, 827, ... 
Resampling results:

  RMSE     Rsquared    MAE     
  20.9022  0.08817569  16.61774

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 169 of 224 using max all knn 6 
k-Nearest Neighbors 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 826, 827, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.02499  0.3385082  12.03306
  7  18.11751  0.3230695  12.31291
  9  18.06410  0.3228578  12.46609

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 170 of 224 using same all knn 6 
k-Nearest Neighbors 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 825, 827, 826, 826, 828, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.92159  0.3443218  11.92136
  7  17.95341  0.3349412  12.26280
  9  18.12050  0.3198088  12.58946

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 171 of 224 using max du knn 6 
k-Nearest Neighbors 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 827, 826, 826, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.03706  0.3385668  12.07859
  7  18.17643  0.3208604  12.36853
  9  18.12301  0.3204755  12.50408

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 172 of 224 using same du knn 6 
k-Nearest Neighbors 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 828, 825, 825, 826, 827, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.83020  0.3505063  11.85474
  7  17.93671  0.3356903  12.25705
  9  18.06761  0.3227907  12.58634

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 173 of 224 using max rms knn 6 
k-Nearest Neighbors 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 826, 827, 827, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.45453  0.3709698  11.64149
  7  17.65063  0.3529129  12.06968
  9  18.06707  0.3211213  12.55278

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 174 of 224 using same rms knn 6 
k-Nearest Neighbors 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 827, 826, 826, 827, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.38307  0.3765722  11.68892
  7  17.67508  0.3518882  12.20299
  9  17.96989  0.3283375  12.61597

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 175 of 224 using max hudgins knn 6 
k-Nearest Neighbors 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 828, 826, 826, 826, 828, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.51663  0.1682387  14.75363
  7  19.93196  0.1905914  14.52698
  9  20.08997  0.1747438  14.73828

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 176 of 224 using same hudgins knn 6 
k-Nearest Neighbors 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 827, 826, 826, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.12856  0.1889877  14.59000
  7  19.82885  0.1953487  14.59478
  9  19.94787  0.1821590  14.78611

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 177 of 224 using max all svmPoly 6 
Support Vector Machines with Polynomial Kernel 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 828, 825, 827, 827, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.14927  0.06612464  16.49492
  1       0.001  0.50  22.03192  0.06757179  16.43278
  1       0.001  1.00  21.87908  0.07050245  16.33873
  1       0.010  0.25  21.64973  0.08140371  16.11743
  1       0.010  0.50  21.47882  0.08838720  15.94468
  1       0.010  1.00  21.33649  0.09279511  15.80649
  1       0.100  0.25  21.23322  0.09477142  15.66610
  1       0.100  0.50  21.23360  0.09413438  15.63870
  1       0.100  1.00  21.25955  0.09342566  15.63176
  2       0.001  0.25  21.99956  0.07306309  16.39406
  2       0.001  0.50  21.80002  0.07974287  16.25315
  2       0.001  1.00  21.57491  0.09123629  16.04691
  2       0.010  0.25  20.92763  0.12384651  15.41782
  2       0.010  0.50  20.84486  0.12604647  15.24892
  2       0.010  1.00  20.74745  0.13282840  15.05272
  2       0.100  0.25  20.41693  0.16015488  14.46809
  2       0.100  0.50  20.31489  0.16937306  14.35123
  2       0.100  1.00  20.27185  0.17691036  14.25996
  3       0.001  0.25  21.83232  0.08316280  16.26175
  3       0.001  0.50  21.59407  0.09601961  16.04329
  3       0.001  1.00  21.35367  0.10684632  15.81207
  3       0.010  0.25  20.67061  0.13870039  15.00550
  3       0.010  0.50  20.58421  0.14487219  14.78626
  3       0.010  1.00  20.52163  0.14963877  14.61572
  3       0.100  0.25  21.85416  0.13988996  15.17433
  3       0.100  0.50  22.97238  0.12510320  15.76978
  3       0.100  1.00  24.98298  0.10909828  16.50573

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 178 of 224 using same all svmPoly 6 
Support Vector Machines with Polynomial Kernel 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 825, 826, 825, 826, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.17264  0.05876599  16.61352
  1       0.001  0.50  22.00330  0.05940094  16.55814
  1       0.001  1.00  21.86274  0.06072199  16.47513
  1       0.010  0.25  21.66168  0.07020746  16.27578
  1       0.010  0.50  21.49007  0.07976199  16.11084
  1       0.010  1.00  21.31221  0.08711907  15.96375
  1       0.100  0.25  21.23200  0.09303424  15.79480
  1       0.100  0.50  21.19115  0.09580657  15.72952
  1       0.100  1.00  21.19833  0.09689782  15.72611
  2       0.001  0.25  21.96035  0.06491675  16.51594
  2       0.001  0.50  21.78555  0.07029090  16.38812
  2       0.001  1.00  21.55969  0.08402118  16.17567
  2       0.010  0.25  20.85937  0.13067188  15.44295
  2       0.010  0.50  20.79870  0.13213499  15.31225
  2       0.010  1.00  20.72734  0.13713596  15.15752
  2       0.100  0.25  20.19416  0.17533754  14.49278
  2       0.100  0.50  20.11293  0.18261492  14.39465
  2       0.100  1.00  20.13060  0.18416573  14.37186
  3       0.001  0.25  21.80347  0.07444093  16.38838
  3       0.001  0.50  21.56260  0.09208188  16.15386
  3       0.001  1.00  21.28528  0.10983558  15.92072
  3       0.010  0.25  20.66661  0.14066942  15.13457
  3       0.010  0.50  20.60393  0.14597686  14.94340
  3       0.010  1.00  20.50311  0.15293368  14.77415
  3       0.100  0.25  22.61297  0.13813942  15.39240
  3       0.100  0.50  24.15128  0.12216474  15.95616
  3       0.100  1.00  25.43652  0.11156283  16.39745

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 179 of 224 using max du svmPoly 6 
Support Vector Machines with Polynomial Kernel 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 826, 825, 826, 828, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.15898  0.06164932  16.51952
  1       0.001  0.50  22.09887  0.06244045  16.44999
  1       0.001  1.00  21.99225  0.06253594  16.36354
  1       0.010  0.25  21.81408  0.06987192  16.17963
  1       0.010  0.50  21.62180  0.08090829  15.99898
  1       0.010  1.00  21.43418  0.09001943  15.84729
  1       0.100  0.25  21.20735  0.09559090  15.70289
  1       0.100  0.50  21.18446  0.09592111  15.63972
  1       0.100  1.00  21.22337  0.09364092  15.61713
  2       0.001  0.25  22.08867  0.06510409  16.43076
  2       0.001  0.50  21.97045  0.06635566  16.33012
  2       0.001  1.00  21.80850  0.07359860  16.17381
  2       0.010  0.25  21.16020  0.12024469  15.66938
  2       0.010  0.50  20.90434  0.12626310  15.47117
  2       0.010  1.00  20.82552  0.12851682  15.30499
  2       0.100  0.25  20.56225  0.14984200  14.73161
  2       0.100  0.50  20.50217  0.15661418  14.64605
  2       0.100  1.00  20.53607  0.15861240  14.62422
  3       0.001  0.25  22.00936  0.06848380  16.35248
  3       0.001  0.50  21.85392  0.07495406  16.20671
  3       0.001  1.00  21.60844  0.09007010  16.00274
  3       0.010  0.25  20.83882  0.13072822  15.38228
  3       0.010  0.50  20.73294  0.13412482  15.18532
  3       0.010  1.00  20.61877  0.14156108  14.96340
  3       0.100  0.25  21.54013  0.12912419  15.15715
  3       0.100  0.50  22.33559  0.11814311  15.52670
  3       0.100  1.00  23.54258  0.10610517  16.01388

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 180 of 224 using same du svmPoly 6 
Support Vector Machines with Polynomial Kernel 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 827, 827, 825, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.23205  0.05252710  16.64773
  1       0.001  0.50  22.14514  0.05108774  16.59752
  1       0.001  1.00  22.00736  0.05097759  16.53001
  1       0.010  0.25  21.81854  0.06035596  16.34868
  1       0.010  0.50  21.65753  0.07183423  16.18463
  1       0.010  1.00  21.43978  0.08086071  16.03537
  1       0.100  0.25  21.21652  0.08972962  15.84004
  1       0.100  0.50  21.16577  0.09223169  15.76703
  1       0.100  1.00  21.19440  0.09099849  15.75923
  2       0.001  0.25  22.12885  0.05380180  16.57908
  2       0.001  0.50  21.97590  0.05520541  16.49341
  2       0.001  1.00  21.80864  0.06482139  16.32960
  2       0.010  0.25  21.11202  0.12220772  15.72060
  2       0.010  0.50  20.91182  0.12580578  15.55336
  2       0.010  1.00  20.83310  0.12802454  15.40907
  2       0.100  0.25  20.41598  0.15790128  14.76754
  2       0.100  0.50  20.31548  0.16593236  14.64616
  2       0.100  1.00  20.31885  0.16709356  14.64755
  3       0.001  0.25  22.01928  0.05714834  16.51382
  3       0.001  0.50  21.83628  0.06609164  16.34845
  3       0.001  1.00  21.61215  0.08414314  16.14247
  3       0.010  0.25  20.91700  0.12660685  15.53787
  3       0.010  0.50  20.79984  0.13083358  15.32835
  3       0.010  1.00  20.75669  0.13534670  15.16449
  3       0.100  0.25  21.32677  0.13991825  15.13436
  3       0.100  0.50  22.14464  0.12982792  15.48458
  3       0.100  1.00  23.56531  0.11687711  16.04065

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 181 of 224 using max rms svmPoly 6 
Support Vector Machines with Polynomial Kernel 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 826, 826, 826, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.71920  0.06887625  16.68538
  1       0.001  0.50  22.49343  0.06970337  16.60851
  1       0.001  1.00  22.28965  0.06958925  16.54246
  1       0.010  0.25  22.02505  0.06909242  16.49297
  1       0.010  0.50  21.91855  0.06853048  16.48164
  1       0.010  1.00  21.83405  0.06701062  16.49311
  1       0.100  0.25  21.72047  0.06552543  16.50260
  1       0.100  0.50  21.67273  0.06467497  16.50527
  1       0.100  1.00  21.64995  0.06448904  16.50604
  2       0.001  0.25  22.49161  0.07015543  16.60711
  2       0.001  0.50  22.28817  0.07020569  16.53994
  2       0.001  1.00  22.09656  0.07014754  16.50096
  2       0.010  0.25  21.76385  0.08792701  16.34330
  2       0.010  0.50  21.55121  0.09899073  16.21811
  2       0.010  1.00  21.27205  0.11657497  15.99135
  2       0.100  0.25  20.88625  0.12992903  15.07353
  2       0.100  0.50  20.93120  0.12919335  15.06197
  2       0.100  1.00  20.93789  0.12931850  15.05223
  3       0.001  0.25  22.32613  0.07087380  16.55414
  3       0.001  0.50  22.19947  0.07093982  16.51353
  3       0.001  1.00  21.96874  0.07150377  16.46950
  3       0.010  0.25  21.47724  0.11267023  16.10470
  3       0.010  0.50  21.15190  0.12959983  15.78510
  3       0.010  1.00  20.96890  0.13143227  15.44988
  3       0.100  0.25  20.59659  0.15290294  14.54304
  3       0.100  0.50  20.55301  0.15764552  14.39753
  3       0.100  1.00  20.57703  0.15811983  14.31587

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 182 of 224 using same rms svmPoly 6 
Support Vector Machines with Polynomial Kernel 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 827, 827, 827, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.62501  0.06740517  16.78728
  1       0.001  0.50  22.37258  0.06721821  16.70932
  1       0.001  1.00  22.29371  0.06576720  16.64863
  1       0.010  0.25  21.98436  0.06383367  16.60258
  1       0.010  0.50  21.84152  0.06263372  16.60969
  1       0.010  1.00  21.76928  0.05998773  16.62601
  1       0.100  0.25  21.72831  0.05752240  16.64274
  1       0.100  0.50  21.71190  0.05679117  16.64907
  1       0.100  1.00  21.70373  0.05645200  16.65096
  2       0.001  0.25  22.37213  0.06759949  16.70819
  2       0.001  0.50  22.29101  0.06633664  16.64628
  2       0.001  1.00  22.02340  0.06526492  16.59969
  2       0.010  0.25  21.69059  0.08101528  16.46741
  2       0.010  0.50  21.48833  0.09307373  16.33223
  2       0.010  1.00  21.25372  0.11148732  16.10615
  2       0.100  0.25  20.78030  0.13651194  15.16613
  2       0.100  0.50  20.80989  0.13446958  15.18321
  2       0.100  1.00  20.83875  0.13310640  15.20317
  3       0.001  0.25  22.34092  0.06721710  16.66420
  3       0.001  0.50  22.10848  0.06633147  16.61229
  3       0.001  1.00  21.92376  0.06595804  16.58443
  3       0.010  0.25  21.40926  0.10860670  16.20875
  3       0.010  0.50  21.11467  0.12670021  15.88315
  3       0.010  1.00  20.94207  0.13410072  15.51021
  3       0.100  0.25  20.55984  0.15419364  14.68474
  3       0.100  0.50  20.54316  0.15694590  14.57323
  3       0.100  1.00  20.52712  0.15926037  14.50206

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 183 of 224 using max hudgins svmPoly 6 
Support Vector Machines with Polynomial Kernel 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 828, 827, 825, 826, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.50570  0.05268566  16.60573
  1       0.001  0.50  22.20129  0.05298723  16.52743
  1       0.001  1.00  22.15361  0.05126950  16.46718
  1       0.010  0.25  22.07824  0.04946725  16.36438
  1       0.010  0.50  21.95309  0.05566211  16.24599
  1       0.010  1.00  21.79241  0.06406668  16.12612
  1       0.100  0.25  21.56240  0.07381214  15.95692
  1       0.100  0.50  21.46693  0.07857143  15.83149
  1       0.100  1.00  21.45635  0.07964903  15.79334
  2       0.001  0.25  22.19681  0.05409136  16.52020
  2       0.001  0.50  22.14778  0.05280858  16.45470
  2       0.001  1.00  22.08219  0.05167656  16.37088
  2       0.010  0.25  21.56771  0.09199302  15.93677
  2       0.010  0.50  21.27740  0.10387564  15.70672
  2       0.010  1.00  21.05221  0.11235215  15.53286
  2       0.100  0.25  20.95613  0.11949514  15.17672
  2       0.100  0.50  20.95778  0.12246356  15.08925
  2       0.100  1.00  20.92081  0.12765393  15.01655
  3       0.001  0.25  22.16568  0.05488386  16.47349
  3       0.001  0.50  22.10351  0.05379261  16.39429
  3       0.001  1.00  21.98643  0.05889170  16.25421
  3       0.010  0.25  21.22551  0.10923826  15.68198
  3       0.010  0.50  20.98775  0.11789127  15.46409
  3       0.010  1.00  20.87251  0.12142883  15.29274
  3       0.100  0.25  21.11878  0.13135101  15.08311
  3       0.100  0.50  21.27597  0.12959425  15.17760
  3       0.100  1.00  21.62802  0.12133039  15.40513

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 184 of 224 using same hudgins svmPoly 6 
Support Vector Machines with Polynomial Kernel 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 825, 825, 828, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.27076  0.04832983  16.71225
  1       0.001  0.50  22.25875  0.04748414  16.65303
  1       0.001  1.00  22.18006  0.04549527  16.61520
  1       0.010  0.25  22.07004  0.04546187  16.54556
  1       0.010  0.50  21.95684  0.05123389  16.45084
  1       0.010  1.00  21.81293  0.05773724  16.33806
  1       0.100  0.25  21.59394  0.06885743  16.16031
  1       0.100  0.50  21.53235  0.07300556  16.05865
  1       0.100  1.00  21.48618  0.07557383  15.99932
  2       0.001  0.25  22.25451  0.04842322  16.64658
  2       0.001  0.50  22.17013  0.04693285  16.60355
  2       0.001  1.00  22.07702  0.04711559  16.54511
  2       0.010  0.25  21.55269  0.09614172  16.06118
  2       0.010  0.50  21.25309  0.10552104  15.85251
  2       0.010  1.00  21.08660  0.11091135  15.68672
  2       0.100  0.25  20.92570  0.12634113  15.31714
  2       0.100  0.50  20.79638  0.13539728  15.18604
  2       0.100  1.00  20.71457  0.14176133  15.08803
  3       0.001  0.25  22.20729  0.04870639  16.61465
  3       0.001  0.50  22.10458  0.04813652  16.55875
  3       0.001  1.00  21.98100  0.05373733  16.44178
  3       0.010  0.25  21.21649  0.10826895  15.83944
  3       0.010  0.50  21.08857  0.11190639  15.68865
  3       0.010  1.00  21.01159  0.11636237  15.53655
  3       0.100  0.25  20.93524  0.14033854  15.16051
  3       0.100  0.50  21.05257  0.14269916  15.15659
  3       0.100  1.00  21.49640  0.13786836  15.34060

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 185 of 224 using max all ranger 6 
Random Forest 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 827, 825, 826, 827, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.77510  0.4248544  11.59793
   2    extratrees  17.03242  0.4237183  12.19599
  13    variance    16.92269  0.4046597  11.41920
  13    extratrees  16.58485  0.4367985  11.41466
  24    variance    17.11289  0.3908374  11.53700
  24    extratrees  16.58540  0.4338259  11.35914

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 186 of 224 using same all ranger 6 
Random Forest 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 827, 828, 828, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.86099  0.4183209  11.73431
   2    extratrees  17.10149  0.4190291  12.32678
  13    variance    16.91592  0.4046937  11.45929
  13    extratrees  16.71997  0.4274085  11.60802
  24    variance    17.15773  0.3874398  11.62975
  24    extratrees  16.70911  0.4251959  11.53841

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 187 of 224 using max du ranger 6 
Random Forest 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 825, 827, 826, 825, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.08286  0.4018974  11.90789
   2    extratrees  17.49703  0.3860085  12.63058
  10    variance    16.91850  0.4031941  11.47039
  10    extratrees  16.89214  0.4145251  11.73099
  18    variance    17.04142  0.3939530  11.49054
  18    extratrees  16.85731  0.4135698  11.62796

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 188 of 224 using same du ranger 6 
Random Forest 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 825, 826, 826, 827, 825, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.17285  0.3956986  12.05474
   2    extratrees  17.56475  0.3808582  12.77328
  10    variance    17.03731  0.3953396  11.59714
  10    extratrees  17.00953  0.4073050  11.95294
  18    variance    17.20536  0.3828439  11.66699
  18    extratrees  17.01213  0.4042035  11.87838

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = extratrees
 and min.node.size = 5.


Now processing model 189 of 224 using max rms ranger 6 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 826, 826, 827, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.78307  0.4124766  11.46920
  2     extratrees  16.62082  0.4359634  11.73049
  3     variance    16.89122  0.4050500  11.45315
  3     extratrees  16.68469  0.4260952  11.66663

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 190 of 224 using same rms ranger 6 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 827, 825, 826, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.83718  0.4079513  11.54044
  2     extratrees  16.66033  0.4322792  11.83784
  3     variance    16.91263  0.4030858  11.48475
  3     extratrees  16.70219  0.4247130  11.77406

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 191 of 224 using max hudgins ranger 6 
Random Forest 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 826, 827, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.74957  0.3529442  12.63830
   2    extratrees  18.12703  0.3364523  13.31686
   7    variance    17.32896  0.3760892  11.97617
   7    extratrees  17.50308  0.3720873  12.46252
  12    variance    17.31966  0.3764317  11.88517
  12    extratrees  17.39448  0.3773554  12.28309

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 192 of 224 using same hudgins ranger 6 
Random Forest 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 826, 826, 827, 827, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.78600  0.3493658  12.69985
   2    extratrees  18.16463  0.3316598  13.39721
   7    variance    17.36452  0.3725697  11.98001
   7    extratrees  17.59392  0.3637870  12.58408
  12    variance    17.40902  0.3692647  11.90707
  12    extratrees  17.52054  0.3659749  12.43371

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 193 of 224 using max all lm 7 
Linear Regression 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 826, 825, 827, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.66101  0.1022147  16.19824

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 194 of 224 using same all lm 7 
Linear Regression 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 827, 825, 827, 826, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.00075  0.1004016  16.55801

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 195 of 224 using max du lm 7 
Linear Regression 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 828, 825, 828, 826, ... 
Resampling results:

  RMSE      Rsquared    MAE  
  20.71889  0.09847931  16.14

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 196 of 224 using same du lm 7 
Linear Regression 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 825, 827, 827, 824, ... 
Resampling results:

  RMSE      Rsquared  MAE    
  21.05713  0.101989  16.4703

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 197 of 224 using max rms lm 7 
Linear Regression 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 824, 827, 826, 826, 827, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.03528  0.0675477  16.65295

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 198 of 224 using same rms lm 7 
Linear Regression 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 827, 826, 826, 827, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.31669  0.07133795  16.97531

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 199 of 224 using max hudgins lm 7 
Linear Regression 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 826, 827, 827, 827, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.81905  0.0889623  16.46922

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 200 of 224 using same hudgins lm 7 
Linear Regression 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 825, 826, 827, 824, 827, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.05344  0.09472327  16.69509

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 201 of 224 using max all knn 7 
k-Nearest Neighbors 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 825, 828, 825, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.93092  0.3353531  11.95305
  7  17.82579  0.3348858  12.23107
  9  17.88891  0.3281705  12.45779

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 202 of 224 using same all knn 7 
k-Nearest Neighbors 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 824, 827, 827, 826, 827, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.39544  0.3217844  12.44242
  7  18.40942  0.3130340  12.74279
  9  18.38112  0.3115253  12.95381

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 203 of 224 using max du knn 7 
k-Nearest Neighbors 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 827, 825, 828, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.93839  0.3344388  12.00437
  7  17.83269  0.3341248  12.26960
  9  17.90952  0.3257045  12.48992

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 204 of 224 using same du knn 7 
k-Nearest Neighbors 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 825, 826, 827, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.40026  0.3213594  12.44913
  7  18.33705  0.3181234  12.72335
  9  18.35619  0.3136404  12.92561

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 205 of 224 using max rms knn 7 
k-Nearest Neighbors 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 826, 828, 826, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.16854  0.3206960  12.22280
  7  18.05871  0.3195097  12.48195
  9  18.17412  0.3075619  12.74534

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 206 of 224 using same rms knn 7 
k-Nearest Neighbors 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 826, 827, 826, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.44695  0.3229504  12.62457
  7  18.39819  0.3158553  12.92286
  9  18.56260  0.3009520  13.16518

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 207 of 224 using max hudgins knn 7 
k-Nearest Neighbors 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 827, 827, 825, 825, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.44530  0.1674175  14.62810
  7  20.21517  0.1703652  14.59593
  9  19.98721  0.1757462  14.57157

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 208 of 224 using same hudgins knn 7 
k-Nearest Neighbors 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 827, 827, 825, 826, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.47904  0.1835066  14.85876
  7  20.31019  0.1822857  14.79830
  9  20.26583  0.1786526  14.90543

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 209 of 224 using max all svmPoly 7 
Support Vector Machines with Polynomial Kernel 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 828, 827, 825, 825, 825, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.01252  0.05669053  16.43999
  1       0.001  0.50  21.84028  0.05544662  16.34488
  1       0.001  1.00  21.70508  0.05727329  16.24436
  1       0.010  0.25  21.50537  0.06841734  16.07364
  1       0.010  0.50  21.38988  0.07630808  15.93903
  1       0.010  1.00  21.22846  0.08313001  15.81916
  1       0.100  0.25  21.11025  0.08974959  15.69909
  1       0.100  0.50  21.09911  0.09125011  15.67916
  1       0.100  1.00  21.10083  0.09144548  15.67939
  2       0.001  0.25  21.80400  0.05946788  16.30078
  2       0.001  0.50  21.61082  0.06583381  16.16265
  2       0.001  1.00  21.40640  0.07771588  15.98838
  2       0.010  0.25  20.86317  0.11084814  15.54142
  2       0.010  0.50  20.82220  0.11193994  15.47034
  2       0.010  1.00  20.73035  0.12107565  15.31374
  2       0.100  0.25  20.14492  0.17026652  14.38536
  2       0.100  0.50  20.04159  0.17792177  14.27048
  2       0.100  1.00  19.95149  0.18665446  14.15641
  3       0.001  0.25  21.64482  0.06884303  16.17507
  3       0.001  0.50  21.40271  0.08092171  15.98924
  3       0.001  1.00  21.17501  0.09520186  15.79326
  3       0.010  0.25  20.58795  0.13216598  15.13242
  3       0.010  0.50  20.46199  0.14199447  14.90332
  3       0.010  1.00  20.37189  0.15019790  14.73154
  3       0.100  0.25  21.85087  0.13754819  15.07557
  3       0.100  0.50  22.63790  0.12429725  15.44013
  3       0.100  1.00  23.61021  0.11295414  15.98463

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 210 of 224 using same all svmPoly 7 
Support Vector Machines with Polynomial Kernel 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 827, 826, 825, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.41355  0.05730648  16.72561
  1       0.001  0.50  22.22310  0.05619228  16.64146
  1       0.001  1.00  22.04542  0.05866988  16.53632
  1       0.010  0.25  21.86493  0.06803711  16.35319
  1       0.010  0.50  21.74850  0.07547827  16.21602
  1       0.010  1.00  21.55858  0.08389890  16.08742
  1       0.100  0.25  21.43398  0.08977349  15.98127
  1       0.100  0.50  21.40059  0.09209723  15.94553
  1       0.100  1.00  21.42580  0.09187228  15.94432
  2       0.001  0.25  22.18742  0.06085983  16.60040
  2       0.001  0.50  21.97800  0.06609113  16.46458
  2       0.001  1.00  21.77911  0.07793045  16.26878
  2       0.010  0.25  21.20389  0.11629609  15.76222
  2       0.010  0.50  21.10901  0.12031130  15.65192
  2       0.010  1.00  20.98735  0.12858772  15.48550
  2       0.100  0.25  20.34115  0.18053866  14.54998
  2       0.100  0.50  20.30884  0.18481225  14.48983
  2       0.100  1.00  20.40039  0.18473096  14.49365
  3       0.001  0.25  22.01503  0.07024229  16.47288
  3       0.001  0.50  21.78363  0.08194028  16.26995
  3       0.001  1.00  21.57377  0.09648278  16.07589
  3       0.010  0.25  20.83433  0.13996583  15.30633
  3       0.010  0.50  20.67917  0.15044718  15.06477
  3       0.010  1.00  20.63253  0.15516945  14.95268
  3       0.100  0.25  21.83082  0.13582333  15.45682
  3       0.100  0.50  23.12805  0.11467098  16.08446
  3       0.100  1.00  26.04747  0.09623780  17.12827

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 211 of 224 using max du svmPoly 7 
Support Vector Machines with Polynomial Kernel 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 825, 825, 826, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.05884  0.05311327  16.48572
  1       0.001  0.50  21.94566  0.05023486  16.38685
  1       0.001  1.00  21.82895  0.05038738  16.28634
  1       0.010  0.25  21.66094  0.06062185  16.13054
  1       0.010  0.50  21.49485  0.07119796  15.98013
  1       0.010  1.00  21.32402  0.08059707  15.85136
  1       0.100  0.25  21.12620  0.08939758  15.72766
  1       0.100  0.50  21.06233  0.09210239  15.68232
  1       0.100  1.00  21.04850  0.09243768  15.67727
  2       0.001  0.25  21.93251  0.05198673  16.36611
  2       0.001  0.50  21.78763  0.05415107  16.24284
  2       0.001  1.00  21.62921  0.06446808  16.10442
  2       0.010  0.25  21.05077  0.10409223  15.71818
  2       0.010  0.50  20.88732  0.10706396  15.61646
  2       0.010  1.00  20.83939  0.11159911  15.51964
  2       0.100  0.25  20.33460  0.15135896  14.75046
  2       0.100  0.50  20.18724  0.16324239  14.55417
  2       0.100  1.00  20.18420  0.16537484  14.50698
  3       0.001  0.25  21.83029  0.05542491  16.27117
  3       0.001  0.50  21.65708  0.06463069  16.12786
  3       0.001  1.00  21.42887  0.07920175  15.93952
  3       0.010  0.25  20.83483  0.11395698  15.51984
  3       0.010  0.50  20.68896  0.12464274  15.29006
  3       0.010  1.00  20.53310  0.13475239  15.07053
  3       0.100  0.25  21.18988  0.13782150  15.02675
  3       0.100  0.50  22.00305  0.12588318  15.32971
  3       0.100  1.00  23.37595  0.10796787  15.78970

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 212 of 224 using same du svmPoly 7 
Support Vector Machines with Polynomial Kernel 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 827, 826, 824, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.44711  0.05674709  16.75459
  1       0.001  0.50  22.33799  0.05507129  16.68047
  1       0.001  1.00  22.18719  0.05634105  16.58666
  1       0.010  0.25  22.00006  0.06606896  16.41401
  1       0.010  0.50  21.85765  0.07636483  16.26044
  1       0.010  1.00  21.69334  0.08544120  16.14191
  1       0.100  0.25  21.45040  0.09510750  16.02114
  1       0.100  0.50  21.40712  0.09544282  16.01267
  1       0.100  1.00  21.41413  0.09457270  16.00317
  2       0.001  0.25  22.32423  0.05718121  16.66111
  2       0.001  0.50  22.15739  0.05993709  16.55295
  2       0.001  1.00  21.98873  0.06882525  16.40348
  2       0.010  0.25  21.41380  0.10979325  15.96714
  2       0.010  0.50  21.18779  0.11543274  15.80863
  2       0.010  1.00  21.10369  0.12013418  15.69396
  2       0.100  0.25  20.52483  0.16280009  14.90301
  2       0.100  0.50  20.42100  0.17252946  14.77804
  2       0.100  1.00  20.44350  0.17400330  14.74821
  3       0.001  0.25  22.20718  0.06177027  16.57661
  3       0.001  0.50  22.02105  0.06920018  16.42628
  3       0.001  1.00  21.81297  0.08420866  16.23562
  3       0.010  0.25  21.09796  0.12381144  15.68337
  3       0.010  0.50  20.93510  0.13328782  15.47849
  3       0.010  1.00  20.75596  0.14394693  15.25912
  3       0.100  0.25  21.60047  0.13400549  15.44764
  3       0.100  0.50  22.03525  0.12729961  15.62618
  3       0.100  1.00  23.41676  0.10867975  16.21644

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 213 of 224 using max rms svmPoly 7 
Support Vector Machines with Polynomial Kernel 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 827, 827, 826, 824, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.54286  0.06809878  16.69541
  1       0.001  0.50  22.24182  0.06871747  16.61184
  1       0.001  1.00  22.11531  0.06743487  16.52610
  1       0.010  0.25  21.84151  0.06374755  16.44915
  1       0.010  0.50  21.55183  0.06042924  16.39123
  1       0.010  1.00  21.44521  0.05914636  16.35839
  1       0.100  0.25  21.39428  0.05765949  16.34146
  1       0.100  0.50  21.37685  0.05756564  16.33256
  1       0.100  1.00  21.37185  0.05750778  16.33008
  2       0.001  0.25  22.24149  0.06914043  16.61016
  2       0.001  0.50  22.11378  0.06795778  16.52363
  2       0.001  1.00  21.91364  0.06579841  16.45461
  2       0.010  0.25  21.45031  0.07293300  16.26482
  2       0.010  0.50  21.23315  0.08238761  16.10827
  2       0.010  1.00  21.06236  0.09638045  15.95924
  2       0.100  0.25  20.92724  0.11385263  15.36780
  2       0.100  0.50  20.99546  0.11175713  15.37002
  2       0.100  1.00  21.04287  0.10997102  15.38193
  3       0.001  0.25  22.18668  0.06887745  16.55849
  3       0.001  0.50  21.98634  0.06723668  16.47647
  3       0.001  1.00  21.75085  0.06456217  16.42483
  3       0.010  0.25  21.20140  0.09449153  16.01931
  3       0.010  0.50  20.98536  0.10958290  15.81963
  3       0.010  1.00  20.81886  0.12031505  15.59421
  3       0.100  0.25  20.56509  0.14172371  14.77936
  3       0.100  0.50  20.48650  0.14769264  14.64058
  3       0.100  1.00  20.43514  0.15166949  14.58470

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 214 of 224 using same rms svmPoly 7 
Support Vector Machines with Polynomial Kernel 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 826, 827, 825, 827, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.96554  0.06847241  16.94054
  1       0.001  0.50  22.61870  0.06949419  16.86178
  1       0.001  1.00  22.50837  0.06956887  16.78234
  1       0.010  0.25  22.22484  0.06800062  16.71430
  1       0.010  0.50  21.94233  0.06551341  16.69650
  1       0.010  1.00  21.78852  0.06434745  16.65766
  1       0.100  0.25  21.73760  0.06290927  16.65373
  1       0.100  0.50  21.72330  0.06227929  16.65307
  1       0.100  1.00  21.71447  0.06210480  16.65067
  2       0.001  0.25  22.61835  0.06988359  16.86029
  2       0.001  0.50  22.50691  0.07008994  16.77987
  2       0.001  1.00  22.27414  0.06920762  16.71521
  2       0.010  0.25  21.84154  0.07836682  16.57423
  2       0.010  0.50  21.59146  0.08692200  16.40836
  2       0.010  1.00  21.39100  0.10240590  16.22757
  2       0.100  0.25  21.22650  0.11964782  15.50991
  2       0.100  0.50  21.26770  0.11849230  15.50660
  2       0.100  1.00  21.28852  0.11838771  15.51336
  3       0.001  0.25  22.57769  0.07025878  16.81487
  3       0.001  0.50  22.35231  0.07025957  16.73836
  3       0.001  1.00  22.14379  0.06960399  16.69451
  3       0.010  0.25  21.57348  0.09870775  16.31886
  3       0.010  0.50  21.32810  0.11503596  16.08417
  3       0.010  1.00  21.16244  0.12394420  15.82615
  3       0.100  0.25  20.79206  0.14999491  14.92037
  3       0.100  0.50  20.72233  0.15491669  14.79034
  3       0.100  1.00  20.69188  0.15781148  14.75183

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 215 of 224 using max hudgins svmPoly 7 
Support Vector Machines with Polynomial Kernel 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 825, 825, 826, 828, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.13198  0.04916507  16.58739
  1       0.001  0.50  22.08093  0.04583178  16.49923
  1       0.001  1.00  21.98902  0.04211781  16.42541
  1       0.010  0.25  21.87774  0.04207290  16.33094
  1       0.010  0.50  21.76981  0.04910458  16.25458
  1       0.010  1.00  21.65173  0.05711370  16.15187
  1       0.100  0.25  21.46199  0.06796194  16.01639
  1       0.100  0.50  21.40640  0.07207818  15.94995
  1       0.100  1.00  21.39805  0.07457537  15.91560
  2       0.001  0.25  22.07515  0.04637086  16.49106
  2       0.001  0.50  21.98143  0.04314140  16.41191
  2       0.001  1.00  21.87065  0.04330907  16.32267
  2       0.010  0.25  21.41509  0.08000564  15.95217
  2       0.010  0.50  21.13481  0.09254305  15.79675
  2       0.010  1.00  21.00410  0.09871507  15.67986
  2       0.100  0.25  20.90987  0.11418551  15.38623
  2       0.100  0.50  20.89092  0.11931324  15.28794
  2       0.100  1.00  20.76622  0.12850598  15.13670
  3       0.001  0.25  22.01020  0.04478807  16.42785
  3       0.001  0.50  21.90125  0.04379479  16.34083
  3       0.001  1.00  21.76638  0.05120172  16.24498
  3       0.010  0.25  21.15501  0.09319147  15.80876
  3       0.010  0.50  21.00515  0.10047616  15.66936
  3       0.010  1.00  20.91234  0.10761489  15.55138
  3       0.100  0.25  20.98771  0.12879614  15.12933
  3       0.100  0.50  21.53710  0.12446826  15.28183
  3       0.100  1.00  22.47377  0.11854188  15.54686

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 216 of 224 using same hudgins svmPoly 7 
Support Vector Machines with Polynomial Kernel 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 827, 825, 827, 827, 826, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.50620  0.05100422  16.84967
  1       0.001  0.50  22.45588  0.05063511  16.77772
  1       0.001  1.00  22.36284  0.04941095  16.71744
  1       0.010  0.25  22.23364  0.05006144  16.61878
  1       0.010  0.50  22.11895  0.05550209  16.52601
  1       0.010  1.00  21.98970  0.06269798  16.42594
  1       0.100  0.25  21.75611  0.07397427  16.27306
  1       0.100  0.50  21.69049  0.07766159  16.20412
  1       0.100  1.00  21.65181  0.07994220  16.16289
  2       0.001  0.25  22.45188  0.05136605  16.77070
  2       0.001  0.50  22.35218  0.05049555  16.70527
  2       0.001  1.00  22.23725  0.05157256  16.62006
  2       0.010  0.25  21.80171  0.08692683  16.22823
  2       0.010  0.50  21.51716  0.09728944  16.06435
  2       0.010  1.00  21.31475  0.10373956  15.90659
  2       0.100  0.25  21.09003  0.12162494  15.54114
  2       0.100  0.50  21.01402  0.12962526  15.44250
  2       0.100  1.00  20.97151  0.13514930  15.36591
  3       0.001  0.25  22.39606  0.05189139  16.72028
  3       0.001  0.50  22.26926  0.05227635  16.64423
  3       0.001  1.00  22.13814  0.05675380  16.52501
  3       0.010  0.25  21.49009  0.09874148  16.03868
  3       0.010  0.50  21.24791  0.10892335  15.83636
  3       0.010  1.00  21.08482  0.11716590  15.66658
  3       0.100  0.25  21.26871  0.13006693  15.44739
  3       0.100  0.50  21.54880  0.12667384  15.59400
  3       0.100  1.00  22.10908  0.11732661  15.86237

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 217 of 224 using max all ranger 7 
Random Forest 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 828, 826, 825, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.01385  0.3992199  11.85520
   2    extratrees  17.23466  0.4006679  12.35423
  13    variance    17.23712  0.3750082  11.75392
  13    extratrees  16.87923  0.4070141  11.70956
  24    variance    17.52402  0.3546737  11.95053
  24    extratrees  16.91747  0.4018521  11.66926

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 218 of 224 using same all ranger 7 
Random Forest 

918 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 827, 827, 826, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.13907  0.4092020  12.09470
   2    extratrees  17.47188  0.4023614  12.59787
  13    variance    17.31681  0.3872495  11.98896
  13    extratrees  16.99423  0.4177714  11.91168
  24    variance    17.60670  0.3673124  12.19813
  24    extratrees  17.01124  0.4137403  11.89339

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 219 of 224 using max du ranger 7 
Random Forest 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 827, 826, 827, 827, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.36818  0.3727872  12.18359
   2    extratrees  17.64799  0.3670136  12.75740
  10    variance    17.32668  0.3682140  11.82978
  10    extratrees  17.20411  0.3840746  11.99850
  18    variance    17.54259  0.3536483  11.95438
  18    extratrees  17.18917  0.3831744  11.92937

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 220 of 224 using same du ranger 7 
Random Forest 

918 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 826, 827, 826, 827, 825, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.53128  0.3816704  12.47834
   2    extratrees  17.87725  0.3694173  13.01973
  10    variance    17.53282  0.3745907  12.17959
  10    extratrees  17.32667  0.3959762  12.26130
  18    variance    17.72986  0.3616971  12.27189
  18    extratrees  17.31259  0.3943046  12.19813

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 221 of 224 using max rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 824, 827, 826, 828, 828, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.05758  0.3878461  11.72789
  2     extratrees  17.04606  0.3969287  12.07015
  3     variance    17.13093  0.3834882  11.66594
  3     extratrees  17.08314  0.3898793  12.01864

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 222 of 224 using same rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

918 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 826, 826, 825, 827, 825, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.23944  0.3929680  11.97101
  2     extratrees  17.33594  0.3946065  12.37076
  3     variance    17.27871  0.3907638  11.87562
  3     extratrees  17.36665  0.3884988  12.31867

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 223 of 224 using max hudgins ranger 7 
Random Forest 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 825, 826, 828, 827, 826, 825, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.01803  0.3202049  12.85264
   2    extratrees  18.24332  0.3162782  13.39341
   7    variance    17.75557  0.3361346  12.26379
   7    extratrees  17.78684  0.3392126  12.61176
  12    variance    17.86940  0.3294216  12.26812
  12    extratrees  17.72533  0.3417931  12.46742

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 224 of 224 using same hudgins ranger 7 
Random Forest 

918 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 827, 827, 827, 825, 825, 826, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.15691  0.3332614  13.09322
   2    extratrees  18.48432  0.3211932  13.65095
   7    variance    17.87295  0.3500166  12.55237
   7    extratrees  17.88002  0.3563041  12.82782
  12    variance    17.96493  0.3445937  12.56044
  12    extratrees  17.77869  0.3614268  12.67445

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.
