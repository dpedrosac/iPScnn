Processing  36 subjects



Now processing model 1 of 224 using max all lm 1 
Linear Regression 

6949 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6252, 6254, 6253, 6256, 6253, 6255, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.44888  0.08992466  16.15198

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 2 of 224 using same all lm 1 
Linear Regression 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 894, 894, 892, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.34925  0.08679961  16.04892

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 3 of 224 using max du lm 1 
Linear Regression 

6949 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6254, 6254, 6253, 6254, 6252, 6253, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.51164  0.0843206  16.22072

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 4 of 224 using same du lm 1 
Linear Regression 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 893, 895, 894, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.44577  0.07583752  16.22813

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 5 of 224 using max rms lm 1 
Linear Regression 

6949 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6254, 6254, 6255, 6254, 6255, 6253, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.88405  0.05079276  16.68899

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 6 of 224 using same rms lm 1 
Linear Regression 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 893, 893, 894, 895, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.59578  0.06114248  16.47183

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 7 of 224 using max hudgins lm 1 
Linear Regression 

6949 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6255, 6255, 6254, 6255, 6253, 6255, ... 
Resampling results:

  RMSE     Rsquared    MAE    
  20.7265  0.06508754  16.5511

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 8 of 224 using same hudgins lm 1 
Linear Regression 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 894, 893, 893, 895, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.65069  0.05869178  16.53467

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 9 of 224 using max all knn 1 
k-Nearest Neighbors 

6949 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6254, 6255, 6254, 6254, 6254, 6255, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.13049  0.3053851  12.24849
  7  17.86558  0.3141168  12.32111
  9  17.77910  0.3160994  12.44060

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 10 of 224 using same all knn 1 
k-Nearest Neighbors 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 893, 893, 894, 895, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.08492  0.1613177  14.75335
  7  19.92449  0.1581153  14.74457
  9  19.76533  0.1578896  14.76676

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 11 of 224 using max du knn 1 
k-Nearest Neighbors 

6949 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6255, 6253, 6254, 6253, 6256, 6253, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.18242  0.3025888  12.27611
  7  17.93801  0.3094374  12.35482
  9  17.81267  0.3137210  12.44415

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 12 of 224 using same du knn 1 
k-Nearest Neighbors 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 894, 893, 895, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.97851  0.1691291  14.67372
  7  19.88118  0.1621945  14.71156
  9  19.68857  0.1641324  14.67548

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 13 of 224 using max rms knn 1 
k-Nearest Neighbors 

6949 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6253, 6254, 6255, 6253, 6255, 6254, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.61392  0.3401299  11.80180
  7  17.43581  0.3452192  11.89892
  9  17.37876  0.3456794  12.03184

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 14 of 224 using same rms knn 1 
k-Nearest Neighbors 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 894, 893, 894, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.68089  0.1809321  14.50106
  7  19.37401  0.1891574  14.44776
  9  19.42950  0.1795145  14.55086

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 15 of 224 using max hudgins knn 1 
k-Nearest Neighbors 

6949 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6253, 6254, 6253, 6253, 6255, 6253, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared    MAE     
  5  21.36796  0.08612552  15.78497
  7  20.85217  0.09868610  15.53243
  9  20.58917  0.10672854  15.38126

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 16 of 224 using same hudgins knn 1 
k-Nearest Neighbors 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 893, 894, 892, 893, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared    MAE     
  5  21.45471  0.06849810  16.11311
  7  21.03930  0.07459401  15.93944
  9  20.87349  0.07601302  15.93367

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 17 of 224 using max all svmPoly 1 
Support Vector Machines with Polynomial Kernel 

6949 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6255, 6254, 6254, 6254, 6253, 6254, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.60295  0.04118383  16.15790
  1       0.001  0.50  21.50210  0.04735792  16.09226
  1       0.001  1.00  21.43776  0.05412662  16.02518
  1       0.010  0.25  21.35201  0.06216144  15.92469
  1       0.010  0.50  21.30184  0.06627786  15.87123
  1       0.010  1.00  21.26900  0.06966644  15.82051
  1       0.100  0.25  21.24508  0.07195126  15.77663
  1       0.100  0.50  21.21145  0.07328094  15.75425
  1       0.100  1.00  21.17814  0.07377338  15.74595
  2       0.001  0.25  21.36581  0.06395987  15.95366
  2       0.001  0.50  21.19169  0.07859899  15.83286
  2       0.001  1.00  21.02395  0.08886223  15.72323
  2       0.010  0.25  20.54944  0.10990123  15.32169
  2       0.010  0.50  20.44108  0.11735936  15.15706
  2       0.010  1.00  20.33771  0.12492753  14.99697
  2       0.100  0.25  19.89799  0.15704343  14.50926
  2       0.100  0.50  19.82817  0.16247538  14.42280
  2       0.100  1.00  19.76390  0.16685636  14.35176
  3       0.001  0.25  21.15641  0.08441631  15.81362
  3       0.001  0.50  20.98288  0.09309162  15.70221
  3       0.001  1.00  20.81614  0.09788093  15.59158
  3       0.010  0.25  20.31681  0.12528728  14.95765
  3       0.010  0.50  20.21608  0.13284405  14.83851
  3       0.010  1.00  20.10577  0.14141266  14.71759
  3       0.100  0.25  19.78832  0.16914825  14.26515
  3       0.100  0.50  19.94477  0.16502824  14.27167
  3       0.100  1.00  20.24006  0.15795048  14.34372

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 18 of 224 using same all svmPoly 1 
Support Vector Machines with Polynomial Kernel 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 893, 893, 893, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.51922  0.04689538  16.13815
  1       0.001  0.50  21.46989  0.04263531  16.08056
  1       0.001  1.00  21.42878  0.04395083  16.02686
  1       0.010  0.25  21.33230  0.04808467  15.97600
  1       0.010  0.50  21.25311  0.05333864  15.92760
  1       0.010  1.00  21.21260  0.05721350  15.89910
  1       0.100  0.25  21.18586  0.05928839  15.86724
  1       0.100  0.50  21.18197  0.05803145  15.87900
  1       0.100  1.00  21.15201  0.05913317  15.88041
  2       0.001  0.25  21.45556  0.04523210  16.04833
  2       0.001  0.50  21.37579  0.05000261  15.98339
  2       0.001  1.00  21.25106  0.05751912  15.91669
  2       0.010  0.25  20.88233  0.07861915  15.73457
  2       0.010  0.50  20.80815  0.08001832  15.70807
  2       0.010  1.00  20.75420  0.08211191  15.66586
  2       0.100  0.25  20.50750  0.10675577  15.33016
  2       0.100  0.50  20.77688  0.10339698  15.42970
  2       0.100  1.00  21.35306  0.09898247  15.58348
  3       0.001  0.25  21.37922  0.05205072  15.97986
  3       0.001  0.50  21.24478  0.06062156  15.90602
  3       0.001  1.00  21.12065  0.06817070  15.82536
  3       0.010  0.25  20.77620  0.08754453  15.60613
  3       0.010  0.50  20.68408  0.09138157  15.51710
  3       0.010  1.00  20.57782  0.09616670  15.43768
  3       0.100  0.25  27.14809  0.06119203  17.44613
  3       0.100  0.50  31.01108  0.04844303  18.54398
  3       0.100  1.00  36.21644  0.03879223  19.86764

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 19 of 224 using max du svmPoly 1 
Support Vector Machines with Polynomial Kernel 

6949 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6253, 6254, 6253, 6252, 6254, 6256, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.67696  0.03653416  16.18178
  1       0.001  0.50  21.55574  0.04291520  16.11777
  1       0.001  1.00  21.46415  0.05085899  16.05197
  1       0.010  0.25  21.38198  0.05851403  15.97020
  1       0.010  0.50  21.34354  0.06177023  15.92916
  1       0.010  1.00  21.31516  0.06308258  15.90076
  1       0.100  0.25  21.33149  0.06247934  15.88648
  1       0.100  0.50  21.35474  0.06131230  15.88621
  1       0.100  1.00  21.37442  0.06053913  15.88440
  2       0.001  0.25  21.47484  0.05155443  16.04821
  2       0.001  0.50  21.34967  0.06355136  15.94188
  2       0.001  1.00  21.20760  0.07592805  15.84708
  2       0.010  0.25  20.72704  0.10134185  15.55381
  2       0.010  0.50  20.58329  0.10673986  15.44343
  2       0.010  1.00  20.46934  0.11303151  15.32165
  2       0.100  0.25  20.16949  0.13513305  14.97013
  2       0.100  0.50  20.10305  0.13984431  14.91130
  2       0.100  1.00  20.03722  0.14466661  14.84961
  3       0.001  0.25  21.34231  0.06787143  15.93232
  3       0.001  0.50  21.17804  0.08097724  15.82965
  3       0.001  1.00  21.01601  0.09086720  15.74100
  3       0.010  0.25  20.40859  0.11824952  15.21518
  3       0.010  0.50  20.33106  0.12361990  15.10093
  3       0.010  1.00  20.25287  0.12904526  15.01077
  3       0.100  0.25  20.05860  0.14893072  14.66535
  3       0.100  0.50  20.09281  0.14969562  14.63148
  3       0.100  1.00  20.23499  0.14574241  14.64141

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 20 of 224 using same du svmPoly 1 
Support Vector Machines with Polynomial Kernel 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 894, 893, 893, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.52447  0.04654511  16.14331
  1       0.001  0.50  21.49105  0.04116509  16.08769
  1       0.001  1.00  21.46424  0.04194371  16.03318
  1       0.010  0.25  21.38539  0.04620935  15.97100
  1       0.010  0.50  21.28561  0.05201453  15.92146
  1       0.010  1.00  21.20983  0.05726344  15.87649
  1       0.100  0.25  21.14486  0.06068801  15.84478
  1       0.100  0.50  21.15670  0.05879934  15.85568
  1       0.100  1.00  21.19618  0.05580573  15.87703
  2       0.001  0.25  21.48648  0.04201484  16.06932
  2       0.001  0.50  21.45103  0.04450559  16.01228
  2       0.001  1.00  21.35741  0.05048255  15.95013
  2       0.010  0.25  21.03938  0.07150174  15.77471
  2       0.010  0.50  20.93426  0.07287733  15.74914
  2       0.010  1.00  20.86567  0.07304231  15.73522
  2       0.100  0.25  20.60540  0.08954159  15.57933
  2       0.100  0.50  20.75005  0.08853886  15.68378
  2       0.100  1.00  21.01573  0.08511762  15.84295
  3       0.001  0.25  21.45889  0.04511457  16.02023
  3       0.001  0.50  21.37477  0.05143458  15.95580
  3       0.001  1.00  21.22546  0.05920605  15.87771
  3       0.010  0.25  20.95853  0.07206694  15.74830
  3       0.010  0.50  20.82018  0.07720083  15.69052
  3       0.010  1.00  20.69399  0.08212534  15.64116
  3       0.100  0.25  24.29395  0.05399893  16.93138
  3       0.100  0.50  27.24856  0.04525269  17.67952
  3       0.100  1.00  33.46418  0.03457264  19.11335

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 21 of 224 using max rms svmPoly 1 
Support Vector Machines with Polynomial Kernel 

6949 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6255, 6254, 6253, 6255, 6255, 6254, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.85789  0.04904044  16.34997
  1       0.001  0.50  21.74257  0.04781695  16.32592
  1       0.001  1.00  21.66679  0.04611310  16.31202
  1       0.010  0.25  21.55862  0.04328994  16.30371
  1       0.010  0.50  21.50454  0.04182075  16.30049
  1       0.010  1.00  21.47682  0.04117670  16.29851
  1       0.100  0.25  21.46142  0.04085608  16.29667
  1       0.100  0.50  21.45665  0.04076840  16.29629
  1       0.100  1.00  21.45469  0.04071477  16.29619
  2       0.001  0.25  21.73678  0.04886180  16.31902
  2       0.001  0.50  21.65350  0.04804504  16.29916
  2       0.001  1.00  21.55497  0.04752249  16.27940
  2       0.010  0.25  21.13256  0.09161402  15.88006
  2       0.010  0.50  20.99085  0.09510146  15.71287
  2       0.010  1.00  20.85408  0.09197716  15.57991
  2       0.100  0.25  20.82498  0.08944591  15.48989
  2       0.100  0.50  20.82541  0.08945769  15.48986
  2       0.100  1.00  20.82629  0.08943638  15.49022
  3       0.001  0.25  21.68030  0.04974195  16.29673
  3       0.001  0.50  21.57609  0.05030887  16.26965
  3       0.001  1.00  21.47336  0.05302963  16.22870
  3       0.010  0.25  20.89809  0.09345068  15.62012
  3       0.010  0.50  20.83804  0.09159040  15.53561
  3       0.010  1.00  20.81963  0.09104104  15.49102
  3       0.100  0.25  20.56698  0.11005524  14.98149
  3       0.100  0.50  20.57530  0.11033679  14.97208
  3       0.100  1.00  20.58464  0.11017068  14.97253

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.25.


Now processing model 22 of 224 using same rms svmPoly 1 
Support Vector Machines with Polynomial Kernel 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 895, 893, 893, 894, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.96903  0.05867675  16.30328
  1       0.001  0.50  21.70229  0.05940938  16.24772
  1       0.001  1.00  21.61348  0.05821041  16.20478
  1       0.010  0.25  21.47082  0.05379006  16.16996
  1       0.010  0.50  21.36760  0.05153751  16.14377
  1       0.010  1.00  21.27098  0.04934951  16.13655
  1       0.100  0.25  21.19804  0.04808513  16.13250
  1       0.100  0.50  21.17749  0.04745996  16.13319
  1       0.100  1.00  21.16682  0.04708930  16.13228
  2       0.001  0.25  21.70166  0.05966348  16.24647
  2       0.001  0.50  21.61218  0.05851175  16.20279
  2       0.001  1.00  21.50670  0.05519493  16.17589
  2       0.010  0.25  21.30568  0.05882522  16.06953
  2       0.010  0.50  21.17178  0.06229741  16.00197
  2       0.010  1.00  21.04259  0.07097897  15.89994
  2       0.100  0.25  20.67723  0.08430744  15.54869
  2       0.100  0.50  20.72633  0.08138755  15.56658
  2       0.100  1.00  20.75913  0.07958370  15.57585
  3       0.001  0.25  21.66334  0.05971916  16.21457
  3       0.001  0.50  21.54662  0.05679982  16.18252
  3       0.001  1.00  21.43546  0.05426504  16.15100
  3       0.010  0.25  21.15361  0.06962091  15.94100
  3       0.010  0.50  20.98464  0.07914347  15.82107
  3       0.010  1.00  20.80608  0.08531592  15.73817
  3       0.100  0.25  20.50211  0.09741900  15.22859
  3       0.100  0.50  20.45106  0.10159436  15.12893
  3       0.100  1.00  20.40064  0.10638194  15.05493

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 23 of 224 using max hudgins svmPoly 1 
Support Vector Machines with Polynomial Kernel 

6949 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6255, 6253, 6254, 6254, 6254, 6254, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.81454  0.02746248  16.28367
  1       0.001  0.50  21.77502  0.02734791  16.25109
  1       0.001  1.00  21.72915  0.03033277  16.22152
  1       0.010  0.25  21.63441  0.03499025  16.19271
  1       0.010  0.50  21.57082  0.03849813  16.17945
  1       0.010  1.00  21.51843  0.04154242  16.16883
  1       0.100  0.25  21.47512  0.04402276  16.16274
  1       0.100  0.50  21.46319  0.04453446  16.16538
  1       0.100  1.00  21.45566  0.04482158  16.16753
  2       0.001  0.25  21.74137  0.03044532  16.21916
  2       0.001  0.50  21.64846  0.03658773  16.15964
  2       0.001  1.00  21.52711  0.04436573  16.08687
  2       0.010  0.25  21.01872  0.08483463  15.76890
  2       0.010  0.50  20.87588  0.08680756  15.71257
  2       0.010  1.00  20.78487  0.08970109  15.64414
  2       0.100  0.25  20.54075  0.10827996  15.33118
  2       0.100  0.50  20.51911  0.10987173  15.28639
  2       0.100  1.00  20.50406  0.11077398  15.24904
  3       0.001  0.25  21.64823  0.03830980  16.14385
  3       0.001  0.50  21.50560  0.04828737  16.05608
  3       0.001  1.00  21.34335  0.06072503  15.96888
  3       0.010  0.25  20.73742  0.09689908  15.54587
  3       0.010  0.50  20.61274  0.10394734  15.40014
  3       0.010  1.00  20.52645  0.10948439  15.27382
  3       0.100  0.25  20.25242  0.13102723  14.86522
  3       0.100  0.50  20.25562  0.13239717  14.83369
  3       0.100  1.00  20.30868  0.13132210  14.82979

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.25.


Now processing model 24 of 224 using same hudgins svmPoly 1 
Support Vector Machines with Polynomial Kernel 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 893, 894, 894, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.59914  0.04034816  16.23638
  1       0.001  0.50  21.56161  0.03670626  16.17092
  1       0.001  1.00  21.53589  0.03243254  16.11220
  1       0.010  0.25  21.56631  0.03036419  16.08800
  1       0.010  0.50  21.55025  0.03156111  16.07481
  1       0.010  1.00  21.49250  0.03418995  16.06726
  1       0.100  0.25  21.40458  0.03726588  16.08651
  1       0.100  0.50  21.36665  0.03902685  16.10498
  1       0.100  1.00  21.34844  0.03944573  16.12716
  2       0.001  0.25  21.55808  0.03691431  16.16368
  2       0.001  0.50  21.52969  0.03325307  16.09943
  2       0.001  1.00  21.54812  0.03267864  16.07703
  2       0.010  0.25  21.28559  0.05162443  15.90650
  2       0.010  0.50  21.22042  0.05418232  15.89774
  2       0.010  1.00  21.12847  0.05612401  15.90473
  2       0.100  0.25  21.02202  0.06799070  15.82717
  2       0.100  0.50  21.02631  0.07086030  15.82619
  2       0.100  1.00  21.06055  0.07196710  15.85710
  3       0.001  0.25  21.53318  0.03443012  16.11495
  3       0.001  0.50  21.53412  0.03427258  16.07277
  3       0.001  1.00  21.50112  0.03639486  16.03814
  3       0.010  0.25  21.23876  0.05380027  15.91262
  3       0.010  0.50  21.13833  0.05722698  15.90212
  3       0.010  1.00  21.05086  0.06146172  15.88274
  3       0.100  0.25  24.50763  0.06319529  16.53623
  3       0.100  0.50  25.15891  0.05950881  16.78616
  3       0.100  1.00  25.69112  0.05362301  17.14648

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 25 of 224 using max all ranger 1 
Random Forest 

6949 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6253, 6254, 6254, 6255, 6254, 6255, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.58774  0.4137710  11.83484
   2    extratrees  17.09175  0.3906051  12.58452
  13    variance    16.34407  0.4205168  11.17090
  13    extratrees  16.32761  0.4288997  11.47328
  24    variance    16.51811  0.4072162  11.30504
  24    extratrees  16.28699  0.4297980  11.37528

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 26 of 224 using same all ranger 1 
Random Forest 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 893, 893, 894, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.90807  0.2124847  14.26409
   2    extratrees  18.87722  0.2205077  14.46053
  13    variance    19.17844  0.1925050  14.31408
  13    extratrees  18.91375  0.2124786  14.18151
  24    variance    19.31510  0.1817838  14.44793
  24    extratrees  18.99458  0.2063607  14.23458

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 27 of 224 using max du ranger 1 
Random Forest 

6949 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6254, 6255, 6254, 6254, 6255, 6254, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.13105  0.3759658  12.46886
   2    extratrees  17.82451  0.3305077  13.30299
  10    variance    16.44457  0.4142360  11.35005
  10    extratrees  16.79766  0.3966525  12.00227
  18    variance    16.51574  0.4075052  11.31259
  18    extratrees  16.69046  0.4022088  11.83931

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 28 of 224 using same du ranger 1 
Random Forest 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 894, 895, 894, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    19.08641  0.1975437  14.49593
   2    extratrees  19.13301  0.1969278  14.72146
  10    variance    19.17802  0.1929306  14.35390
  10    extratrees  19.10073  0.1971132  14.40998
  18    variance    19.29818  0.1846031  14.41834
  18    extratrees  19.19123  0.1900517  14.45273

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 29 of 224 using max rms ranger 1 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

6949 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6254, 6255, 6253, 6254, 6254, 6255, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.81263  0.3852437  11.50952
  2     extratrees  16.75899  0.3934990  11.94058
  3     variance    16.87434  0.3816737  11.43975
  3     extratrees  16.72310  0.3937262  11.81285

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 30 of 224 using same rms ranger 1 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 893, 894, 894, 893, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    19.14895  0.1972154  14.13881
  2     extratrees  18.82195  0.2174445  14.17185
  3     variance    19.34000  0.1867937  14.24889
  3     extratrees  18.91571  0.2104742  14.17071

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 31 of 224 using max hudgins ranger 1 
Random Forest 

6949 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 6252, 6254, 6254, 6255, 6254, 6254, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.98405  0.3069516  13.35992
   2    extratrees  18.50694  0.2704010  13.98630
   7    variance    17.15136  0.3615042  12.16253
   7    extratrees  17.53834  0.3395372  12.81468
  12    variance    17.08877  0.3648545  11.96404
  12    extratrees  17.33783  0.3524873  12.55773

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 32 of 224 using same hudgins ranger 1 
Random Forest 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 894, 894, 893, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    19.49799  0.1605455  14.94263
   2    extratrees  19.48391  0.1640451  15.06806
   7    variance    19.54250  0.1619954  14.78683
   7    extratrees  19.52579  0.1596638  14.87454
  12    variance    19.58469  0.1624840  14.76439
  12    extratrees  19.57071  0.1571566  14.85658

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 33 of 224 using max all lm 2 
Linear Regression 

2978 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2681, 2680, 2680, 2679, 2680, 2681, ... 
Resampling results:

  RMSE     Rsquared    MAE    
  20.3821  0.09260896  16.0658

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 34 of 224 using same all lm 2 
Linear Regression 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 895, 894, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.16605  0.08589727  15.85126

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 35 of 224 using max du lm 2 
Linear Regression 

2978 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2681, 2679, 2680, 2681, 2680, 2679, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.44203  0.08671653  16.13742

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 36 of 224 using same du lm 2 
Linear Regression 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 894, 893, 894, 894, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.20367  0.08344586  15.91531

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 37 of 224 using max rms lm 2 
Linear Regression 

2978 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2680, 2681, 2680, 2679, 2680, 2679, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.81439  0.05289288  16.68114

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 38 of 224 using same rms lm 2 
Linear Regression 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 893, 895, 893, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE    
  20.42516  0.05918013  16.3759

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 39 of 224 using max hudgins lm 2 
Linear Regression 

2978 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2680, 2680, 2680, 2681, 2681, 2679, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.66284  0.06664516  16.53808

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 40 of 224 using same hudgins lm 2 
Linear Regression 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 894, 893, 895, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.33767  0.06744151  16.29201

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 41 of 224 using max all knn 2 
k-Nearest Neighbors 

2978 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2680, 2681, 2679, 2681, 2681, 2681, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.65405  0.3328223  11.76021
  7  17.52064  0.3343040  11.88182
  9  17.59962  0.3254934  12.08740

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 42 of 224 using same all knn 2 
k-Nearest Neighbors 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 895, 893, 893, 892, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.06565  0.2131546  13.71691
  7  18.86690  0.2142109  13.72456
  9  18.75976  0.2163529  13.78175

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 43 of 224 using max du knn 2 
k-Nearest Neighbors 

2978 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2679, 2679, 2680, 2681, 2681, 2680, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.60626  0.3363599  11.74615
  7  17.47446  0.3377561  11.87208
  9  17.55432  0.3287944  12.07553

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 44 of 224 using same du knn 2 
k-Nearest Neighbors 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 895, 894, 893, 892, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.16251  0.2066072  13.77159
  7  18.95699  0.2081902  13.81518
  9  18.84214  0.2095132  13.87336

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 45 of 224 using max rms knn 2 
k-Nearest Neighbors 

2978 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2679, 2681, 2681, 2680, 2680, 2680, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.64874  0.3339770  11.80971
  7  17.47777  0.3380638  11.87422
  9  17.46870  0.3354715  12.04131

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 46 of 224 using same rms knn 2 
k-Nearest Neighbors 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 893, 893, 896, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.37525  0.2587268  13.10454
  7  18.48709  0.2404321  13.49030
  9  18.60050  0.2267076  13.64664

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 47 of 224 using max hudgins knn 2 
k-Nearest Neighbors 

2978 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2680, 2681, 2681, 2680, 2680, 2679, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.01807  0.1032700  15.57671
  7  20.62426  0.1111007  15.41666
  9  20.44621  0.1155690  15.33167

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 48 of 224 using same hudgins knn 2 
k-Nearest Neighbors 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 894, 894, 894, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared    MAE     
  5  20.67041  0.09992331  15.61836
  7  20.28150  0.10954914  15.48350
  9  20.03614  0.11786762  15.32119

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 49 of 224 using max all svmPoly 2 
Support Vector Machines with Polynomial Kernel 

2978 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2680, 2680, 2680, 2681, 2679, 2679, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.63256  0.04555308  16.22416
  1       0.001  0.50  21.53230  0.04769602  16.15339
  1       0.001  1.00  21.44055  0.05142682  16.07139
  1       0.010  0.25  21.32952  0.05863385  15.95230
  1       0.010  0.50  21.25668  0.06484116  15.83691
  1       0.010  1.00  21.19941  0.06823649  15.75267
  1       0.100  0.25  21.19352  0.07045616  15.68050
  1       0.100  0.50  21.18908  0.07217026  15.64222
  1       0.100  1.00  21.17365  0.07346370  15.61226
  2       0.001  0.25  21.43665  0.05974114  16.05380
  2       0.001  0.50  21.25284  0.07170514  15.90925
  2       0.001  1.00  21.05794  0.08581661  15.74345
  2       0.010  0.25  20.57064  0.10763735  15.30002
  2       0.010  0.50  20.48556  0.11487145  15.12249
  2       0.010  1.00  20.37735  0.12373499  14.91234
  2       0.100  0.25  19.96870  0.15650393  14.33608
  2       0.100  0.50  19.86781  0.16487434  14.22679
  2       0.100  1.00  19.79164  0.17050797  14.13213
  3       0.001  0.25  21.24489  0.08111424  15.87659
  3       0.001  0.50  21.02992  0.09274009  15.71475
  3       0.001  1.00  20.87673  0.09843877  15.55049
  3       0.010  0.25  20.25559  0.13079232  14.78107
  3       0.010  0.50  20.20240  0.13606301  14.65497
  3       0.010  1.00  20.15546  0.14185682  14.53633
  3       0.100  0.25  20.33277  0.15032207  14.42490
  3       0.100  0.50  20.75620  0.14302209  14.54663
  3       0.100  1.00  21.50482  0.13003480  14.83327

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 50 of 224 using same all svmPoly 2 
Support Vector Machines with Polynomial Kernel 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 894, 894, 894, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.34852  0.05221304  16.05310
  1       0.001  0.50  21.26168  0.05185098  16.02178
  1       0.001  1.00  21.17692  0.05234851  15.98445
  1       0.010  0.25  21.10407  0.05535857  15.89535
  1       0.010  0.50  21.02471  0.05987676  15.81672
  1       0.010  1.00  20.94256  0.06522944  15.73923
  1       0.100  0.25  20.82830  0.07247044  15.62313
  1       0.100  0.50  20.79059  0.07535644  15.58120
  1       0.100  1.00  20.80192  0.07575615  15.56911
  2       0.001  0.25  21.23310  0.05665652  15.99074
  2       0.001  0.50  21.12000  0.06143629  15.92320
  2       0.001  1.00  21.01400  0.06966532  15.81522
  2       0.010  0.25  20.36461  0.10673441  15.33421
  2       0.010  0.50  20.22443  0.10986213  15.25632
  2       0.010  1.00  20.13309  0.11381425  15.16862
  2       0.100  0.25  19.71585  0.14405835  14.56725
  2       0.100  0.50  19.63208  0.15110667  14.48403
  2       0.100  1.00  19.62259  0.15337616  14.46429
  3       0.001  0.25  21.11994  0.06620021  15.90761
  3       0.001  0.50  20.99129  0.07725354  15.78175
  3       0.001  1.00  20.79769  0.08783901  15.64687
  3       0.010  0.25  20.08132  0.11724234  15.12325
  3       0.010  0.50  20.01361  0.12147580  14.94187
  3       0.010  1.00  19.99031  0.12327510  14.83496
  3       0.100  0.25  21.81938  0.09304355  15.78164
  3       0.100  0.50  23.80864  0.07611107  16.48589
  3       0.100  1.00  26.32969  0.06504314  17.24904

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 51 of 224 using max du svmPoly 2 
Support Vector Machines with Polynomial Kernel 

2978 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2679, 2681, 2680, 2680, 2680, 2681, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.70912  0.04040861  16.24460
  1       0.001  0.50  21.62106  0.04148523  16.18158
  1       0.001  1.00  21.50974  0.04615278  16.10104
  1       0.010  0.25  21.37114  0.05487148  15.99105
  1       0.010  0.50  21.28797  0.06109800  15.89588
  1       0.010  1.00  21.22715  0.06433079  15.82061
  1       0.100  0.25  21.22171  0.06390638  15.76518
  1       0.100  0.50  21.25965  0.06287301  15.74370
  1       0.100  1.00  21.28914  0.06212848  15.74487
  2       0.001  0.25  21.57492  0.04672686  16.13674
  2       0.001  0.50  21.42738  0.05569399  16.02647
  2       0.001  1.00  21.24458  0.06859046  15.89170
  2       0.010  0.25  20.74038  0.10187180  15.49982
  2       0.010  0.50  20.61315  0.10495600  15.39887
  2       0.010  1.00  20.53711  0.10949651  15.26684
  2       0.100  0.25  20.19520  0.13665028  14.70359
  2       0.100  0.50  20.16092  0.14018909  14.66910
  2       0.100  1.00  20.12579  0.14326180  14.65531
  3       0.001  0.25  21.43514  0.05935282  16.01814
  3       0.001  0.50  21.24654  0.07390954  15.88679
  3       0.001  1.00  21.05559  0.08690287  15.74587
  3       0.010  0.25  20.39766  0.12060615  15.13155
  3       0.010  0.50  20.32196  0.12579825  14.96201
  3       0.010  1.00  20.26468  0.13074882  14.82788
  3       0.100  0.25  20.45737  0.13446524  14.73158
  3       0.100  0.50  20.69058  0.13139829  14.79778
  3       0.100  1.00  21.26019  0.11989581  15.00094

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 52 of 224 using same du svmPoly 2 
Support Vector Machines with Polynomial Kernel 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 893, 893, 894, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.39242  0.04910583  16.06010
  1       0.001  0.50  21.30899  0.04890094  16.03063
  1       0.001  1.00  21.21861  0.04925383  15.99122
  1       0.010  0.25  21.09886  0.05161244  15.90607
  1       0.010  0.50  20.99454  0.05751446  15.81694
  1       0.010  1.00  20.90065  0.06302258  15.75223
  1       0.100  0.25  20.76898  0.06916550  15.66525
  1       0.100  0.50  20.74549  0.07065899  15.63250
  1       0.100  1.00  20.79239  0.06859694  15.64235
  2       0.001  0.25  21.29548  0.05108019  16.01605
  2       0.001  0.50  21.19255  0.05318583  15.96254
  2       0.001  1.00  21.08296  0.05723878  15.88310
  2       0.010  0.25  20.62194  0.09605062  15.51386
  2       0.010  0.50  20.44103  0.10054884  15.43217
  2       0.010  1.00  20.32138  0.10139000  15.40580
  2       0.100  0.25  20.00236  0.12112317  15.02106
  2       0.100  0.50  19.91208  0.12993190  14.88324
  2       0.100  1.00  19.88349  0.13346082  14.84520
  3       0.001  0.25  21.21273  0.05535000  15.96440
  3       0.001  0.50  21.09471  0.06047389  15.88065
  3       0.001  1.00  20.95401  0.07047027  15.76327
  3       0.010  0.25  20.41456  0.09977847  15.44738
  3       0.010  0.50  20.22443  0.10559272  15.32780
  3       0.010  1.00  20.11760  0.11128193  15.18597
  3       0.100  0.25  21.56672  0.09535251  15.66324
  3       0.100  0.50  23.11042  0.08094218  16.29580
  3       0.100  1.00  26.25987  0.06395153  17.36675

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 53 of 224 using max rms svmPoly 2 
Support Vector Machines with Polynomial Kernel 

2978 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2680, 2681, 2679, 2681, 2680, 2681, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.97607  0.04987257  16.36601
  1       0.001  0.50  21.86427  0.04964839  16.33799
  1       0.001  1.00  21.70054  0.04897189  16.32178
  1       0.010  0.25  21.62072  0.04826478  16.32025
  1       0.010  0.50  21.56503  0.04772410  16.32347
  1       0.010  1.00  21.52758  0.04728025  16.32617
  1       0.100  0.25  21.50205  0.04677880  16.32961
  1       0.100  0.50  21.49296  0.04661126  16.33106
  1       0.100  1.00  21.48740  0.04651018  16.33174
  2       0.001  0.25  21.86066  0.05038347  16.33459
  2       0.001  0.50  21.69196  0.05019136  16.31460
  2       0.001  1.00  21.61975  0.05071780  16.30394
  2       0.010  0.25  21.25952  0.09175544  16.00329
  2       0.010  0.50  21.03269  0.10438127  15.76994
  2       0.010  1.00  20.81781  0.10445776  15.49213
  2       0.100  0.25  20.61409  0.10431269  15.23901
  2       0.100  0.50  20.62328  0.10397706  15.23876
  2       0.100  1.00  20.62889  0.10384428  15.23895
  3       0.001  0.25  21.73981  0.05113005  16.31667
  3       0.001  0.50  21.63435  0.05216567  16.29681
  3       0.001  1.00  21.55467  0.05482686  16.27491
  3       0.010  0.25  20.94522  0.10517438  15.61162
  3       0.010  0.50  20.72518  0.10463332  15.38456
  3       0.010  1.00  20.64736  0.10397884  15.29216
  3       0.100  0.25  20.37470  0.12676988  14.66702
  3       0.100  0.50  20.37150  0.12803037  14.63504
  3       0.100  1.00  20.37734  0.12839196  14.62710

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 54 of 224 using same rms svmPoly 2 
Support Vector Machines with Polynomial Kernel 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 894, 893, 894, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.67408  0.05406284  16.17568
  1       0.001  0.50  21.57869  0.05482747  16.11870
  1       0.001  1.00  21.49864  0.05504712  16.07763
  1       0.010  0.25  21.28022  0.05367571  16.05924
  1       0.010  0.50  21.20950  0.05239119  16.06008
  1       0.010  1.00  21.16214  0.05128255  16.07409
  1       0.100  0.25  21.12814  0.04994507  16.09377
  1       0.100  0.50  21.11693  0.04947867  16.10223
  1       0.100  1.00  21.11075  0.04937491  16.10433
  2       0.001  0.25  21.57810  0.05510063  16.11772
  2       0.001  0.50  21.49713  0.05543838  16.07601
  2       0.001  1.00  21.31223  0.05460011  16.05948
  2       0.010  0.25  21.10564  0.06745859  15.96368
  2       0.010  0.50  20.97723  0.07884728  15.88232
  2       0.010  1.00  20.80584  0.09389530  15.73557
  2       0.100  0.25  20.20418  0.11033176  15.07098
  2       0.100  0.50  20.19969  0.11001008  15.07923
  2       0.100  1.00  20.19968  0.10999209  15.08372
  3       0.001  0.25  21.54206  0.05572849  16.09146
  3       0.001  0.50  21.37596  0.05528159  16.06507
  3       0.001  1.00  21.24539  0.05514206  16.04550
  3       0.010  0.25  20.92711  0.09043666  15.79850
  3       0.010  0.50  20.69325  0.10377508  15.59624
  3       0.010  1.00  20.44967  0.10833260  15.32713
  3       0.100  0.25  19.94692  0.12877808  14.71633
  3       0.100  0.50  19.94185  0.12982637  14.60705
  3       0.100  1.00  19.95707  0.12986241  14.54932

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 55 of 224 using max hudgins svmPoly 2 
Support Vector Machines with Polynomial Kernel 

2978 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2680, 2680, 2681, 2679, 2681, 2681, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.82348  0.03435956  16.32001
  1       0.001  0.50  21.74769  0.03167769  16.27965
  1       0.001  1.00  21.70488  0.03015243  16.23825
  1       0.010  0.25  21.65529  0.03409395  16.17979
  1       0.010  0.50  21.62933  0.03625098  16.14808
  1       0.010  1.00  21.55974  0.03960726  16.13354
  1       0.100  0.25  21.47549  0.04326213  16.11788
  1       0.100  0.50  21.43456  0.04514841  16.11488
  1       0.100  1.00  21.40894  0.04611694  16.10993
  2       0.001  0.25  21.72928  0.03387319  16.26088
  2       0.001  0.50  21.66873  0.03416323  16.19994
  2       0.001  1.00  21.59005  0.03997950  16.12529
  2       0.010  0.25  21.07542  0.08806902  15.68778
  2       0.010  0.50  20.90917  0.08929788  15.61293
  2       0.010  1.00  20.82404  0.08995761  15.55523
  2       0.100  0.25  20.58010  0.10697788  15.20767
  2       0.100  0.50  20.54413  0.11107831  15.14282
  2       0.100  1.00  20.51444  0.11424392  15.09268
  3       0.001  0.25  21.66698  0.03639855  16.20103
  3       0.001  0.50  21.57511  0.04236437  16.11361
  3       0.001  1.00  21.46189  0.05289603  16.01128
  3       0.010  0.25  20.82907  0.09442868  15.52678
  3       0.010  0.50  20.67901  0.09982870  15.37530
  3       0.010  1.00  20.54773  0.10743042  15.18959
  3       0.100  0.25  20.40472  0.12707187  14.81126
  3       0.100  0.50  20.49400  0.12611470  14.81772
  3       0.100  1.00  20.59123  0.12468523  14.85074

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.25.


Now processing model 56 of 224 using same hudgins svmPoly 2 
Support Vector Machines with Polynomial Kernel 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 895, 892, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.45276  0.04157106  16.12243
  1       0.001  0.50  21.41775  0.03886005  16.09925
  1       0.001  1.00  21.36803  0.03578252  16.08386
  1       0.010  0.25  21.33747  0.03200120  16.07534
  1       0.010  0.50  21.31756  0.03257733  16.03536
  1       0.010  1.00  21.29798  0.03586999  15.98804
  1       0.100  0.25  21.22336  0.04000036  15.95069
  1       0.100  0.50  21.19011  0.04211009  15.95399
  1       0.100  1.00  21.16376  0.04399827  15.95099
  2       0.001  0.25  21.41426  0.03949637  16.09451
  2       0.001  0.50  21.35887  0.03721876  16.07289
  2       0.001  1.00  21.32411  0.03539963  16.05714
  2       0.010  0.25  21.02595  0.07016901  15.75197
  2       0.010  0.50  20.86295  0.07986374  15.65590
  2       0.010  1.00  20.66175  0.08685751  15.57511
  2       0.100  0.25  20.41501  0.09541579  15.40272
  2       0.100  0.50  20.44042  0.09568294  15.38075
  2       0.100  1.00  20.43748  0.09767817  15.37343
  3       0.001  0.25  21.37496  0.03903666  16.07621
  3       0.001  0.50  21.32413  0.03830587  16.04670
  3       0.001  1.00  21.26967  0.03928584  16.00316
  3       0.010  0.25  20.79007  0.08566297  15.59980
  3       0.010  0.50  20.60270  0.08765882  15.55986
  3       0.010  1.00  20.43880  0.09208693  15.48257
  3       0.100  0.25  20.94335  0.09027857  15.43303
  3       0.100  0.50  21.29245  0.08775314  15.60411
  3       0.100  1.00  22.13102  0.07855208  15.93067

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 57 of 224 using max all ranger 2 
Random Forest 

2978 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2680, 2681, 2680, 2680, 2680, 2680, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.63441  0.4064153  11.84334
   2    extratrees  17.08820  0.3873794  12.47072
  13    variance    16.40388  0.4146089  11.38720
  13    extratrees  16.44616  0.4178394  11.52420
  24    variance    16.57251  0.4017281  11.54945
  24    extratrees  16.44740  0.4154650  11.46888

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = variance
 and min.node.size = 5.


Now processing model 58 of 224 using same all ranger 2 
Random Forest 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 893, 895, 893, 895, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.47576  0.3201080  12.88935
   2    extratrees  17.75679  0.3069173  13.38550
  13    variance    17.50058  0.3118691  12.77429
  13    extratrees  17.37366  0.3266077  12.72626
  24    variance    17.71726  0.2942952  12.99999
  24    extratrees  17.38598  0.3240247  12.71109

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 59 of 224 using max du ranger 2 
Random Forest 

2978 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2681, 2681, 2680, 2681, 2678, 2681, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.10653  0.3727636  12.35317
   2    extratrees  17.73035  0.3336205  13.08142
  10    variance    16.52002  0.4072379  11.53547
  10    extratrees  16.90839  0.3849269  11.99992
  18    variance    16.56871  0.4026337  11.54471
  18    extratrees  16.83888  0.3879263  11.88328

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 60 of 224 using same du ranger 2 
Random Forest 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 895, 894, 893, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.79792  0.2946151  13.26709
   2    extratrees  18.13614  0.2731625  13.75490
  10    variance    17.62353  0.3014562  12.93479
  10    extratrees  17.74392  0.2965318  13.13743
  18    variance    17.79170  0.2876574  13.05148
  18    extratrees  17.74125  0.2951634  13.09245

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 61 of 224 using max rms ranger 2 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

2978 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2681, 2681, 2680, 2679, 2681, 2679, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.89764  0.3768902  11.55296
  2     extratrees  16.79330  0.3894813  11.96912
  3     variance    16.95498  0.3737826  11.49700
  3     extratrees  16.78814  0.3866887  11.85912

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 62 of 224 using same rms ranger 2 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 894, 893, 894, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.67163  0.2985663  12.67170
  2     extratrees  17.59595  0.3079157  13.01419
  3     variance    17.76529  0.2930826  12.69336
  3     extratrees  17.63993  0.3015666  12.96200

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 63 of 224 using max hudgins ranger 2 
Random Forest 

2978 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2680, 2678, 2681, 2681, 2681, 2680, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.94488  0.3040069  13.22231
   2    extratrees  18.42775  0.2711117  13.78826
   7    variance    17.25646  0.3511353  12.29118
   7    extratrees  17.68145  0.3243075  12.85937
  12    variance    17.20943  0.3535884  12.17103
  12    extratrees  17.51702  0.3355312  12.65074

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 64 of 224 using same hudgins ranger 2 
Random Forest 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 893, 896, 893, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.32874  0.2466163  13.84202
   2    extratrees  18.56372  0.2316771  14.20444
   7    variance    17.95050  0.2746918  13.29629
   7    extratrees  18.15378  0.2603558  13.65289
  12    variance    17.92018  0.2771650  13.21409
  12    extratrees  18.11319  0.2627973  13.54917

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 65 of 224 using max all lm 3 
Linear Regression 

1985 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1788, 1786, 1786, 1787, 1787, 1787, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.36327  0.09931704  16.05277

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 66 of 224 using same all lm 3 
Linear Regression 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 892, 894, 893, 892, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.51427  0.08811832  16.20675

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 67 of 224 using max du lm 3 
Linear Regression 

1985 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1786, 1785, 1787, 1787, 1787, 1785, ... 
Resampling results:

  RMSE     Rsquared    MAE     
  20.4362  0.09325764  16.05243

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 68 of 224 using same du lm 3 
Linear Regression 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 895, 893, 894, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE    
  20.63325  0.08245576  16.3132

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 69 of 224 using max rms lm 3 
Linear Regression 

1985 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1785, 1786, 1787, 1786, 1786, 1787, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.83638  0.05635958  16.59923

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 70 of 224 using same rms lm 3 
Linear Regression 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 896, 893, 893, 892, 895, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.84938  0.05399082  16.75362

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 71 of 224 using max hudgins lm 3 
Linear Regression 

1985 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1786, 1788, 1787, 1787, 1786, 1786, ... 
Resampling results:

  RMSE      Rsquared    MAE    
  20.67895  0.07176208  16.4745

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 72 of 224 using same hudgins lm 3 
Linear Regression 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 893, 894, 894, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.74343  0.06853452  16.59599

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 73 of 224 using max all knn 3 
k-Nearest Neighbors 

1985 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1787, 1786, 1787, 1786, 1786, 1787, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.86417  0.3238154  11.88509
  7  17.69153  0.3268588  12.12632
  9  17.73188  0.3199640  12.37987

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 74 of 224 using same all knn 3 
k-Nearest Neighbors 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 892, 894, 893, 893, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.72041  0.1938721  13.96766
  7  19.47225  0.1976073  13.96962
  9  19.33385  0.2009434  13.94932

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 75 of 224 using max du knn 3 
k-Nearest Neighbors 

1985 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1787, 1787, 1786, 1787, 1787, 1787, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.82662  0.3260184  11.87605
  7  17.65601  0.3292420  12.08361
  9  17.69434  0.3224061  12.34545

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 76 of 224 using same du knn 3 
k-Nearest Neighbors 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 892, 893, 894, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.75750  0.1912306  14.01395
  7  19.51080  0.1955111  14.02044
  9  19.38457  0.1979332  14.02583

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 77 of 224 using max rms knn 3 
k-Nearest Neighbors 

1985 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1785, 1787, 1787, 1786, 1786, 1787, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.65325  0.3370940  11.86999
  7  17.44434  0.3437939  11.98018
  9  17.53420  0.3342815  12.24291

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 78 of 224 using same rms knn 3 
k-Nearest Neighbors 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 893, 893, 893, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.24638  0.2273853  13.41635
  7  18.95987  0.2327379  13.53938
  9  18.87556  0.2329988  13.61520

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 79 of 224 using max hudgins knn 3 
k-Nearest Neighbors 

1985 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1785, 1787, 1786, 1786, 1787, 1787, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.73339  0.1278834  15.22389
  7  20.41103  0.1315496  15.03079
  9  20.22171  0.1358945  14.98563

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 80 of 224 using same hudgins knn 3 
k-Nearest Neighbors 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 895, 893, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.21278  0.0978984  15.94882
  7  20.81028  0.1030673  15.71614
  9  20.64082  0.1053582  15.64815

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 81 of 224 using max all svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1985 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1787, 1786, 1786, 1788, 1785, 1786, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.69994  0.04758007  16.19957
  1       0.001  0.50  21.59568  0.04736755  16.13149
  1       0.001  1.00  21.48856  0.05086640  16.04221
  1       0.010  0.25  21.31650  0.05936822  15.92126
  1       0.010  0.50  21.21786  0.06593290  15.82516
  1       0.010  1.00  21.09716  0.07287424  15.71220
  1       0.100  0.25  21.02870  0.07712889  15.62474
  1       0.100  0.50  21.02383  0.07860565  15.59187
  1       0.100  1.00  21.01389  0.07931252  15.56828
  2       0.001  0.25  21.51428  0.05520577  16.05951
  2       0.001  0.50  21.34714  0.06695161  15.92001
  2       0.001  1.00  21.10401  0.08213059  15.75457
  2       0.010  0.25  20.56729  0.11057842  15.28073
  2       0.010  0.50  20.48522  0.11644450  15.14703
  2       0.010  1.00  20.40008  0.12311201  14.99715
  2       0.100  0.25  19.95067  0.15932836  14.38244
  2       0.100  0.50  19.88523  0.16642662  14.28879
  2       0.100  1.00  19.80220  0.17330965  14.16171
  3       0.001  0.25  21.33437  0.07334937  15.89762
  3       0.001  0.50  21.08910  0.08936428  15.73502
  3       0.001  1.00  20.85252  0.09899195  15.57777
  3       0.010  0.25  20.27859  0.13098499  14.84322
  3       0.010  0.50  20.20812  0.13703124  14.69674
  3       0.010  1.00  20.17027  0.14226388  14.59319
  3       0.100  0.25  21.04557  0.13843040  14.61750
  3       0.100  0.50  22.47683  0.12780278  14.88023
  3       0.100  1.00  24.46506  0.11564783  15.25041

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 82 of 224 using same all svmPoly 3 
Support Vector Machines with Polynomial Kernel 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 895, 893, 893, 893, 892, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.79249  0.04481860  16.44473
  1       0.001  0.50  21.68510  0.04341832  16.39901
  1       0.001  1.00  21.59011  0.04368068  16.32593
  1       0.010  0.25  21.46383  0.04932975  16.22674
  1       0.010  0.50  21.34883  0.05532282  16.15234
  1       0.010  1.00  21.23704  0.06270273  16.06961
  1       0.100  0.25  21.05099  0.07219218  15.95428
  1       0.100  0.50  21.00111  0.07531962  15.90033
  1       0.100  1.00  21.02690  0.07581006  15.88841
  2       0.001  0.25  21.65392  0.04787522  16.36295
  2       0.001  0.50  21.52916  0.05057912  16.25716
  2       0.001  1.00  21.35769  0.05997081  16.12705
  2       0.010  0.25  20.71667  0.10023073  15.65695
  2       0.010  0.50  20.68573  0.10122849  15.59507
  2       0.010  1.00  20.64495  0.10461162  15.49737
  2       0.100  0.25  20.33532  0.13103929  15.00311
  2       0.100  0.50  20.25123  0.13908971  14.88426
  2       0.100  1.00  20.23184  0.14406199  14.82090
  3       0.001  0.25  21.53633  0.05405912  16.24845
  3       0.001  0.50  21.35112  0.06530415  16.10465
  3       0.001  1.00  21.10979  0.07951877  15.98533
  3       0.010  0.25  20.51321  0.11294193  15.34096
  3       0.010  0.50  20.45684  0.11754169  15.18567
  3       0.010  1.00  20.44976  0.12106679  15.06905
  3       0.100  0.25  21.85540  0.09943379  15.83482
  3       0.100  0.50  23.21177  0.08489614  16.40792
  3       0.100  1.00  25.72360  0.06994111  17.40052

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 83 of 224 using max du svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1985 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1786, 1787, 1787, 1784, 1787, 1787, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.78285  0.04330905  16.22515
  1       0.001  0.50  21.68302  0.04261024  16.15944
  1       0.001  1.00  21.58017  0.04574427  16.07917
  1       0.010  0.25  21.39175  0.05590446  15.95753
  1       0.010  0.50  21.27360  0.06303887  15.87169
  1       0.010  1.00  21.14658  0.07054352  15.76306
  1       0.100  0.25  21.06370  0.07466723  15.67980
  1       0.100  0.50  21.07330  0.07445497  15.66299
  1       0.100  1.00  21.11794  0.07206364  15.66430
  2       0.001  0.25  21.65058  0.04582840  16.12606
  2       0.001  0.50  21.50551  0.05357766  16.02044
  2       0.001  1.00  21.31863  0.06552805  15.89560
  2       0.010  0.25  20.73408  0.10512866  15.48248
  2       0.010  0.50  20.57986  0.10820594  15.37630
  2       0.010  1.00  20.51064  0.11268061  15.26893
  2       0.100  0.25  20.14731  0.14238586  14.71962
  2       0.100  0.50  20.09372  0.14857661  14.63932
  2       0.100  1.00  20.09220  0.15103965  14.60756
  3       0.001  0.25  21.52456  0.05525076  16.02431
  3       0.001  0.50  21.32115  0.06986240  15.88699
  3       0.001  1.00  21.09651  0.08518727  15.74533
  3       0.010  0.25  20.44530  0.11953486  15.18905
  3       0.010  0.50  20.33495  0.12655477  15.01842
  3       0.010  1.00  20.25648  0.13302994  14.87060
  3       0.100  0.25  20.80629  0.12912183  14.89017
  3       0.100  0.50  21.69488  0.11554437  15.10389
  3       0.100  1.00  23.13840  0.10254842  15.38203

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 84 of 224 using same du svmPoly 3 
Support Vector Machines with Polynomial Kernel 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 894, 894, 892, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.84709  0.04199754  16.47171
  1       0.001  0.50  21.76782  0.03884029  16.43130
  1       0.001  1.00  21.68814  0.03763719  16.37332
  1       0.010  0.25  21.54729  0.04383767  16.26836
  1       0.010  0.50  21.40273  0.05132793  16.18385
  1       0.010  1.00  21.32234  0.05825495  16.12749
  1       0.100  0.25  21.14907  0.06818699  16.02938
  1       0.100  0.50  21.10791  0.06990996  16.02192
  1       0.100  1.00  21.12584  0.06884955  16.03008
  2       0.001  0.25  21.75305  0.04077610  16.41455
  2       0.001  0.50  21.66380  0.04052265  16.34123
  2       0.001  1.00  21.53141  0.04740627  16.23386
  2       0.010  0.25  20.92457  0.09772334  15.83985
  2       0.010  0.50  20.78426  0.09879260  15.77628
  2       0.010  1.00  20.70627  0.10061844  15.71640
  2       0.100  0.25  20.57244  0.11484180  15.40763
  2       0.100  0.50  20.52066  0.12067646  15.33583
  2       0.100  1.00  20.47912  0.12573131  15.27218
  3       0.001  0.25  21.67907  0.04232744  16.35293
  3       0.001  0.50  21.54537  0.04893856  16.22807
  3       0.001  1.00  21.33563  0.06243471  16.11127
  3       0.010  0.25  20.74004  0.10190876  15.72915
  3       0.010  0.50  20.59993  0.10962342  15.55029
  3       0.010  1.00  20.54812  0.11345324  15.40868
  3       0.100  0.25  21.92631  0.10097288  15.84560
  3       0.100  0.50  23.24058  0.08594428  16.41354
  3       0.100  1.00  25.76749  0.06751546  17.36280

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 85 of 224 using max rms svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1985 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1787, 1786, 1787, 1786, 1787, 1786, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.05059  0.05446630  16.36255
  1       0.001  0.50  21.98475  0.05485089  16.31660
  1       0.001  1.00  21.77174  0.05407161  16.28859
  1       0.010  0.25  21.62260  0.05304266  16.27363
  1       0.010  0.50  21.51809  0.05209173  16.27063
  1       0.010  1.00  21.44717  0.05072893  16.27241
  1       0.100  0.25  21.39911  0.04958719  16.27275
  1       0.100  0.50  21.37976  0.04927272  16.27247
  1       0.100  1.00  21.37331  0.04921837  16.27281
  2       0.001  0.25  21.98305  0.05537244  16.31444
  2       0.001  0.50  21.76661  0.05488405  16.28389
  2       0.001  1.00  21.64690  0.05471367  16.26578
  2       0.010  0.25  21.31987  0.08308503  16.04749
  2       0.010  0.50  21.10988  0.09792620  15.86710
  2       0.010  1.00  20.88458  0.10801527  15.59553
  2       0.100  0.25  20.62524  0.10733884  15.18141
  2       0.100  0.50  20.63412  0.10707227  15.18584
  2       0.100  1.00  20.63911  0.10693035  15.18824
  3       0.001  0.25  21.84420  0.05579035  16.29356
  3       0.001  0.50  21.68617  0.05584755  16.26547
  3       0.001  1.00  21.56133  0.05702748  16.24198
  3       0.010  0.25  21.01622  0.10762661  15.71123
  3       0.010  0.50  20.78953  0.10842115  15.43280
  3       0.010  1.00  20.68162  0.10711659  15.27390
  3       0.100  0.25  20.31178  0.13322788  14.62758
  3       0.100  0.50  20.30586  0.13425270  14.59285
  3       0.100  1.00  20.29774  0.13553424  14.56629

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 86 of 224 using same rms svmPoly 3 
Support Vector Machines with Polynomial Kernel 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 894, 894, 893, 894, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.18166  0.05422362  16.57513
  1       0.001  0.50  22.00223  0.05529006  16.52195
  1       0.001  1.00  21.90659  0.05552621  16.47868
  1       0.010  0.25  21.67871  0.05380842  16.45204
  1       0.010  0.50  21.54676  0.05185093  16.44506
  1       0.010  1.00  21.44856  0.04927086  16.44708
  1       0.100  0.25  21.37534  0.04700028  16.44311
  1       0.100  0.50  21.35344  0.04586552  16.44101
  1       0.100  1.00  21.33929  0.04589160  16.43886
  2       0.001  0.25  22.00141  0.05559065  16.52083
  2       0.001  0.50  21.90470  0.05596565  16.47660
  2       0.001  1.00  21.70904  0.05496229  16.45125
  2       0.010  0.25  21.45728  0.06771631  16.32782
  2       0.010  0.50  21.30050  0.07513057  16.23977
  2       0.010  1.00  21.10360  0.09041777  16.05881
  2       0.100  0.25  20.69739  0.10596425  15.36456
  2       0.100  0.50  20.71608  0.10534192  15.36860
  2       0.100  1.00  20.73860  0.10475952  15.38330
  3       0.001  0.25  21.95606  0.05643067  16.49161
  3       0.001  0.50  21.75944  0.05597682  16.45813
  3       0.001  1.00  21.62964  0.05548407  16.43432
  3       0.010  0.25  21.24911  0.08705451  16.13299
  3       0.010  0.50  20.99409  0.10440858  15.89309
  3       0.010  1.00  20.74154  0.10808087  15.63528
  3       0.100  0.25  20.45280  0.12376392  14.96625
  3       0.100  0.50  20.45318  0.12547520  14.86906
  3       0.100  1.00  20.49322  0.12433733  14.85991

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.25.


Now processing model 87 of 224 using max hudgins svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1985 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1786, 1787, 1786, 1787, 1786, 1787, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.88653  0.03751890  16.30474
  1       0.001  0.50  21.81381  0.03409329  16.26305
  1       0.001  1.00  21.74975  0.03144574  16.21766
  1       0.010  0.25  21.69241  0.03316295  16.15563
  1       0.010  0.50  21.66039  0.03593873  16.11679
  1       0.010  1.00  21.55677  0.04036830  16.08772
  1       0.100  0.25  21.43844  0.04507701  16.06362
  1       0.100  0.50  21.37104  0.04800021  16.05029
  1       0.100  1.00  21.34032  0.04945076  16.03818
  2       0.001  0.25  21.80351  0.03550381  16.25108
  2       0.001  0.50  21.73093  0.03369260  16.19226
  2       0.001  1.00  21.65464  0.03744482  16.12506
  2       0.010  0.25  21.11978  0.08547597  15.73195
  2       0.010  0.50  20.91657  0.09040082  15.61761
  2       0.010  1.00  20.82708  0.09102672  15.55937
  2       0.100  0.25  20.58325  0.10992952  15.18222
  2       0.100  0.50  20.53178  0.11381584  15.13601
  2       0.100  1.00  20.49346  0.11741797  15.10910
  3       0.001  0.25  21.73916  0.03561386  16.20002
  3       0.001  0.50  21.65133  0.03961198  16.11915
  3       0.001  1.00  21.55409  0.04680314  16.03149
  3       0.010  0.25  20.89972  0.09224913  15.58257
  3       0.010  0.50  20.76899  0.09648641  15.46258
  3       0.010  1.00  20.66776  0.10284480  15.30954
  3       0.100  0.25  20.54166  0.12605652  14.96660
  3       0.100  0.50  20.73943  0.12070309  15.02758
  3       0.100  1.00  21.20871  0.10939449  15.18215

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 88 of 224 using same hudgins svmPoly 3 
Support Vector Machines with Polynomial Kernel 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 892, 894, 893, 894, 895, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.89291  0.03931225  16.52457
  1       0.001  0.50  21.84715  0.03797081  16.48558
  1       0.001  1.00  21.77808  0.03569079  16.44630
  1       0.010  0.25  21.72163  0.03357581  16.39207
  1       0.010  0.50  21.66459  0.03613347  16.34389
  1       0.010  1.00  21.60764  0.03940911  16.31275
  1       0.100  0.25  21.48157  0.04564762  16.29072
  1       0.100  0.50  21.38799  0.05036069  16.28823
  1       0.100  1.00  21.32505  0.05362913  16.26317
  2       0.001  0.25  21.84366  0.03893486  16.47831
  2       0.001  0.50  21.76980  0.03701127  16.43554
  2       0.001  1.00  21.71412  0.03534746  16.38426
  2       0.010  0.25  21.33927  0.06961258  16.08456
  2       0.010  0.50  21.08830  0.08111509  15.94435
  2       0.010  1.00  20.91165  0.08302865  15.89291
  2       0.100  0.25  20.92071  0.08745084  15.77553
  2       0.100  0.50  20.88548  0.09223226  15.69151
  2       0.100  1.00  20.92485  0.09246957  15.73181
  3       0.001  0.25  21.79395  0.03901746  16.44511
  3       0.001  0.50  21.72271  0.03672791  16.39343
  3       0.001  1.00  21.65067  0.03903061  16.31968
  3       0.010  0.25  21.10122  0.08109565  15.93618
  3       0.010  0.50  20.92431  0.08347154  15.87212
  3       0.010  1.00  20.85691  0.08715106  15.78565
  3       0.100  0.25  21.09630  0.09921678  15.65827
  3       0.100  0.50  21.51713  0.09131766  15.89908
  3       0.100  1.00  22.20096  0.08198912  16.29963

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 89 of 224 using max all ranger 3 
Random Forest 

1985 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1787, 1787, 1787, 1788, 1785, 1786, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.83327  0.3945404  11.97803
   2    extratrees  17.10067  0.3903684  12.48677
  13    variance    16.83885  0.3858672  11.65054
  13    extratrees  16.59522  0.4108096  11.67841
  24    variance    17.05110  0.3696543  11.81934
  24    extratrees  16.60492  0.4079231  11.62677

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 90 of 224 using same all ranger 3 
Random Forest 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 893, 894, 893, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.23989  0.2818240  13.36656
   2    extratrees  18.41372  0.2777836  13.75276
  13    variance    18.39289  0.2686750  13.28395
  13    extratrees  18.19153  0.2858707  13.19870
  24    variance    18.53410  0.2581954  13.45557
  24    extratrees  18.18172  0.2855017  13.18048

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 91 of 224 using max du ranger 3 
Random Forest 

1985 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1787, 1785, 1787, 1787, 1786, 1786, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.15172  0.3721839  12.36166
   2    extratrees  17.63189  0.3469696  12.97748
  10    variance    16.85352  0.3853144  11.74114
  10    extratrees  16.96257  0.3853094  12.06644
  18    variance    16.96434  0.3762994  11.78130
  18    extratrees  16.91406  0.3865300  11.97307

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 92 of 224 using same du ranger 3 
Random Forest 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 893, 893, 893, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.39287  0.2710768  13.62976
   2    extratrees  18.73339  0.2501277  14.07818
  10    variance    18.41132  0.2669131  13.40396
  10    extratrees  18.41653  0.2698441  13.49595
  18    variance    18.53971  0.2575595  13.51452
  18    extratrees  18.39791  0.2699455  13.45168

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = variance
 and min.node.size = 5.


Now processing model 93 of 224 using max rms ranger 3 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1985 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1787, 1788, 1785, 1786, 1786, 1787, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.98297  0.3732170  11.72866
  2     extratrees  16.83674  0.3915710  12.06214
  3     variance    17.06922  0.3678964  11.64861
  3     extratrees  16.80066  0.3905137  11.92650

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 94 of 224 using same rms ranger 3 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 892, 894, 893, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    18.28296  0.2777642  13.12544
  2     extratrees  18.14642  0.2887258  13.36173
  3     variance    18.42588  0.2704073  13.13525
  3     extratrees  18.18996  0.2829128  13.27798

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 95 of 224 using max hudgins ranger 3 
Random Forest 

1985 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1786, 1787, 1787, 1787, 1786, 1785, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.02084  0.3003348  13.20837
   2    extratrees  18.37819  0.2792351  13.68257
   7    variance    17.61204  0.3269319  12.50917
   7    extratrees  17.79162  0.3181679  12.89676
  12    variance    17.61154  0.3269062  12.38363
  12    extratrees  17.70105  0.3233600  12.75495

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 96 of 224 using same hudgins ranger 3 
Random Forest 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 893, 893, 894, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.99688  0.2214078  14.28996
   2    extratrees  19.20092  0.2080491  14.58234
   7    variance    18.87204  0.2323617  13.97678
   7    extratrees  18.96755  0.2237935  14.13976
  12    variance    18.92966  0.2309928  13.94616
  12    extratrees  18.97082  0.2228408  14.08039

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 97 of 224 using max all lm 4 
Linear Regression 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 894, 894, 894, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.32781  0.09883472  15.88691

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 98 of 224 using same all lm 4 
Linear Regression 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 895, 895, 893, 895, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.42175  0.09515654  16.00066

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 99 of 224 using max du lm 4 
Linear Regression 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 893, 895, 893, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.37457  0.09399044  15.89933

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 100 of 224 using same du lm 4 
Linear Regression 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 893, 894, 895, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.42933  0.09139236  15.98012

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 101 of 224 using max rms lm 4 
Linear Regression 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 893, 895, 894, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.65973  0.06532916  16.49024

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 102 of 224 using same rms lm 4 
Linear Regression 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 895, 893, 894, 894, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.74495  0.06314032  16.51241

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 103 of 224 using max hudgins lm 4 
Linear Regression 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 894, 894, 894, 893, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.52614  0.07915504  16.28944

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 104 of 224 using same hudgins lm 4 
Linear Regression 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 892, 894, 894, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.58949  0.07806119  16.31862

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 105 of 224 using max all knn 4 
k-Nearest Neighbors 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 895, 891, 893, 894, 895, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.05891  0.3057132  12.21385
  7  17.85185  0.3095024  12.40922
  9  17.98268  0.2962754  12.63455

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 106 of 224 using same all knn 4 
k-Nearest Neighbors 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 894, 896, 892, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.95429  0.3092408  12.33313
  7  17.91491  0.3066955  12.54497
  9  17.98996  0.2979659  12.69398

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 107 of 224 using max du knn 4 
k-Nearest Neighbors 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 895, 892, 895, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.03052  0.3080408  12.20341
  7  17.89190  0.3075701  12.44519
  9  18.04486  0.2929713  12.67153

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 108 of 224 using same du knn 4 
k-Nearest Neighbors 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 895, 893, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.88371  0.3142890  12.32758
  7  17.85727  0.3102136  12.53025
  9  17.95716  0.3001606  12.69832

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 109 of 224 using max rms knn 4 
k-Nearest Neighbors 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 892, 894, 894, 893, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.88784  0.3115433  12.26632
  7  17.88069  0.3053878  12.45657
  9  18.14460  0.2829425  12.78115

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 110 of 224 using same rms knn 4 
k-Nearest Neighbors 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 894, 894, 894, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.12941  0.2951186  12.55865
  7  17.84199  0.3094858  12.59141
  9  18.13692  0.2858430  12.87759

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 111 of 224 using max hudgins knn 4 
k-Nearest Neighbors 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 895, 893, 894, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.45465  0.1353341  15.06215
  7  20.03207  0.1490772  14.71957
  9  19.88332  0.1518551  14.65788

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 112 of 224 using same hudgins knn 4 
k-Nearest Neighbors 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 894, 894, 894, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.80489  0.1157701  15.34764
  7  20.37120  0.1282624  15.00646
  9  20.22050  0.1313039  14.94601

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 113 of 224 using max all svmPoly 4 
Support Vector Machines with Polynomial Kernel 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 895, 893, 894, 892, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.65967  0.05599105  16.20571
  1       0.001  0.50  21.53406  0.05462224  16.15866
  1       0.001  1.00  21.42632  0.05570301  16.08079
  1       0.010  0.25  21.21151  0.06585575  15.91293
  1       0.010  0.50  21.06006  0.07363573  15.81313
  1       0.010  1.00  20.92328  0.08003772  15.68844
  1       0.100  0.25  20.81247  0.08590591  15.53865
  1       0.100  0.50  20.80472  0.08781563  15.47764
  1       0.100  1.00  20.83493  0.08695439  15.47002
  2       0.001  0.25  21.49349  0.06017230  16.11744
  2       0.001  0.50  21.33670  0.06717767  15.99218
  2       0.001  1.00  21.10139  0.07961336  15.82482
  2       0.010  0.25  20.40530  0.12298155  15.19500
  2       0.010  0.50  20.37872  0.12342560  15.08926
  2       0.010  1.00  20.36424  0.12563386  14.97250
  2       0.100  0.25  19.97296  0.15587863  14.41824
  2       0.100  0.50  19.91921  0.16118126  14.37014
  2       0.100  1.00  19.95436  0.16042917  14.42886
  3       0.001  0.25  21.34459  0.07240820  15.98863
  3       0.001  0.50  21.10580  0.08715212  15.80779
  3       0.001  1.00  20.80922  0.10456297  15.58039
  3       0.010  0.25  20.25493  0.13249769  14.87963
  3       0.010  0.50  20.20495  0.13701367  14.70780
  3       0.010  1.00  20.21567  0.13867700  14.62895
  3       0.100  0.25  22.23528  0.11035589  15.37518
  3       0.100  0.50  24.71461  0.08986248  16.02862
  3       0.100  1.00  28.06327  0.07223703  16.91102

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 114 of 224 using same all svmPoly 4 
Support Vector Machines with Polynomial Kernel 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 894, 893, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.72302  0.05342100  16.21764
  1       0.001  0.50  21.57011  0.05531376  16.16619
  1       0.001  1.00  21.49493  0.05629912  16.12193
  1       0.010  0.25  21.32605  0.06118222  15.98971
  1       0.010  0.50  21.16027  0.06833599  15.87186
  1       0.010  1.00  21.00552  0.07618886  15.75081
  1       0.100  0.25  20.90193  0.08271454  15.57954
  1       0.100  0.50  20.89674  0.08417099  15.50811
  1       0.100  1.00  20.91252  0.08433140  15.47859
  2       0.001  0.25  21.53432  0.06105525  16.12956
  2       0.001  0.50  21.42631  0.06513771  16.04806
  2       0.001  1.00  21.22749  0.07611272  15.88553
  2       0.010  0.25  20.61077  0.10921168  15.28017
  2       0.010  0.50  20.61406  0.10985248  15.19707
  2       0.010  1.00  20.64754  0.10957063  15.10906
  2       0.100  0.25  20.23171  0.13796120  14.59166
  2       0.100  0.50  20.17816  0.14221004  14.56495
  2       0.100  1.00  20.16046  0.14523660  14.55302
  3       0.001  0.25  21.42307  0.06996983  16.03742
  3       0.001  0.50  21.21958  0.08391468  15.85922
  3       0.001  1.00  20.96025  0.09900111  15.62048
  3       0.010  0.25  20.51103  0.11630847  15.02060
  3       0.010  0.50  20.47578  0.12017090  14.85359
  3       0.010  1.00  20.47964  0.12294775  14.75094
  3       0.100  0.25  22.18079  0.10376820  15.52595
  3       0.100  0.50  24.53379  0.08354645  16.26852
  3       0.100  1.00  28.67588  0.06343495  17.46264

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 115 of 224 using max du svmPoly 4 
Support Vector Machines with Polynomial Kernel 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 894, 895, 892, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.72356  0.05301202  16.22519
  1       0.001  0.50  21.60095  0.05091966  16.17676
  1       0.001  1.00  21.50170  0.05006669  16.10183
  1       0.010  0.25  21.31498  0.05961867  15.94667
  1       0.010  0.50  21.15656  0.06829678  15.84203
  1       0.010  1.00  20.98490  0.07714781  15.71850
  1       0.100  0.25  20.82099  0.08320202  15.58384
  1       0.100  0.50  20.76427  0.08589350  15.50248
  1       0.100  1.00  20.76446  0.08571591  15.47828
  2       0.001  0.25  21.58292  0.05356065  16.15765
  2       0.001  0.50  21.46460  0.05533583  16.06206
  2       0.001  1.00  21.29540  0.06495922  15.92880
  2       0.010  0.25  20.60431  0.11732465  15.42024
  2       0.010  0.50  20.39472  0.11967377  15.28699
  2       0.010  1.00  20.38051  0.12070895  15.16498
  2       0.100  0.25  20.17802  0.14014797  14.72705
  2       0.100  0.50  20.11597  0.14485026  14.66064
  2       0.100  1.00  20.07895  0.14829058  14.65235
  3       0.001  0.25  21.49368  0.05766017  16.07894
  3       0.001  0.50  21.32085  0.06739662  15.94321
  3       0.001  1.00  21.08658  0.08147897  15.77514
  3       0.010  0.25  20.37885  0.12240686  15.23313
  3       0.010  0.50  20.32697  0.12756411  15.04215
  3       0.010  1.00  20.28169  0.13192268  14.86131
  3       0.100  0.25  21.38411  0.11174109  15.36546
  3       0.100  0.50  23.03989  0.10048114  15.88188
  3       0.100  1.00  26.23717  0.08215377  16.77938

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 116 of 224 using same du svmPoly 4 
Support Vector Machines with Polynomial Kernel 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 893, 893, 893, 895, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.77282  0.04724073  16.24195
  1       0.001  0.50  21.68374  0.04658902  16.20793
  1       0.001  1.00  21.58990  0.04617191  16.17209
  1       0.010  0.25  21.44388  0.05217261  16.05352
  1       0.010  0.50  21.27896  0.06085300  15.93625
  1       0.010  1.00  21.10414  0.06941951  15.81128
  1       0.100  0.25  20.94172  0.07671540  15.64562
  1       0.100  0.50  20.88912  0.07872995  15.56476
  1       0.100  1.00  20.88764  0.07861733  15.52404
  2       0.001  0.25  21.66866  0.04884824  16.19256
  2       0.001  0.50  21.55918  0.05029466  16.13795
  2       0.001  1.00  21.42394  0.05819208  16.01990
  2       0.010  0.25  20.81190  0.10281587  15.47616
  2       0.010  0.50  20.63405  0.10294877  15.37888
  2       0.010  1.00  20.62070  0.10324081  15.27956
  2       0.100  0.25  20.50804  0.11678720  14.91282
  2       0.100  0.50  20.41413  0.12280511  14.84975
  2       0.100  1.00  20.35304  0.12677547  14.85327
  3       0.001  0.25  21.57822  0.05237274  16.14030
  3       0.001  0.50  21.45209  0.06083609  16.02644
  3       0.001  1.00  21.22649  0.07582312  15.85439
  3       0.010  0.25  20.63094  0.10459791  15.35285
  3       0.010  0.50  20.59636  0.10914750  15.16453
  3       0.010  1.00  20.60218  0.11058441  15.02386
  3       0.100  0.25  21.17961  0.10891132  15.28651
  3       0.100  0.50  22.20921  0.09535055  15.70133
  3       0.100  1.00  24.39458  0.07761124  16.49943

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 117 of 224 using max rms svmPoly 4 
Support Vector Machines with Polynomial Kernel 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 892, 894, 893, 895, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.07179  0.05995079  16.35801
  1       0.001  0.50  21.91017  0.06112938  16.29543
  1       0.001  1.00  21.78420  0.06126970  16.24695
  1       0.010  0.25  21.53345  0.05999278  16.20717
  1       0.010  0.50  21.43177  0.05881976  16.20654
  1       0.010  1.00  21.36445  0.05740360  16.22409
  1       0.100  0.25  21.31363  0.05508046  16.24813
  1       0.100  0.50  21.29215  0.05438243  16.25582
  1       0.100  1.00  21.27984  0.05420108  16.25976
  2       0.001  0.25  21.90939  0.06151036  16.29429
  2       0.001  0.50  21.78165  0.06184354  16.24430
  2       0.001  1.00  21.56626  0.06131471  16.20784
  2       0.010  0.25  21.28867  0.07823844  16.07599
  2       0.010  0.50  21.10524  0.09075052  15.96883
  2       0.010  1.00  20.88138  0.10962413  15.75515
  2       0.100  0.25  20.32192  0.12824127  14.91266
  2       0.100  0.50  20.35106  0.12678432  14.91941
  2       0.100  1.00  20.36489  0.12615816  14.92516
  3       0.001  0.25  21.86304  0.06231000  16.26281
  3       0.001  0.50  21.61993  0.06225036  16.21309
  3       0.001  1.00  21.48374  0.06222768  16.18909
  3       0.010  0.25  21.02618  0.10706354  15.84663
  3       0.010  0.50  20.73687  0.12237023  15.55403
  3       0.010  1.00  20.49000  0.12768964  15.18156
  3       0.100  0.25  20.08225  0.14756023  14.49808
  3       0.100  0.50  20.04022  0.15061973  14.36853
  3       0.100  1.00  20.04453  0.15106081  14.32418

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 118 of 224 using same rms svmPoly 4 
Support Vector Machines with Polynomial Kernel 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 892, 896, 896, 892, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.18418  0.05757487  16.35721
  1       0.001  0.50  21.92570  0.05786070  16.28726
  1       0.001  1.00  21.86433  0.05686378  16.24732
  1       0.010  0.25  21.60296  0.05434761  16.20961
  1       0.010  0.50  21.54861  0.05304188  16.21379
  1       0.010  1.00  21.52705  0.05114164  16.22944
  1       0.100  0.25  21.51250  0.04925148  16.24436
  1       0.100  0.50  21.50699  0.04834176  16.25224
  1       0.100  1.00  21.50265  0.04797704  16.25763
  2       0.001  0.25  21.92508  0.05817907  16.28618
  2       0.001  0.50  21.86208  0.05730081  16.24553
  2       0.001  1.00  21.63486  0.05556889  16.21146
  2       0.010  0.25  21.41134  0.06784917  16.09838
  2       0.010  0.50  21.27499  0.07799852  15.99871
  2       0.010  1.00  21.06058  0.09473704  15.79045
  2       0.100  0.25  20.49330  0.11707447  15.00599
  2       0.100  0.50  20.51197  0.11686952  15.00732
  2       0.100  1.00  20.53181  0.11615992  15.01828
  3       0.001  0.25  21.90757  0.05825548  16.25903
  3       0.001  0.50  21.71364  0.05671440  16.22263
  3       0.001  1.00  21.56398  0.05612273  16.19344
  3       0.010  0.25  21.17516  0.09234897  15.88427
  3       0.010  0.50  20.90269  0.10804492  15.59516
  3       0.010  1.00  20.63849  0.11368209  15.25008
  3       0.100  0.25  20.23699  0.13806202  14.58225
  3       0.100  0.50  20.18886  0.14187992  14.46334
  3       0.100  1.00  20.19843  0.14211616  14.42877

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 119 of 224 using max hudgins svmPoly 4 
Support Vector Machines with Polynomial Kernel 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 894, 893, 894, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.76603  0.04821044  16.28305
  1       0.001  0.50  21.71336  0.04555134  16.24015
  1       0.001  1.00  21.61911  0.04196933  16.19996
  1       0.010  0.25  21.53935  0.04135094  16.12424
  1       0.010  0.50  21.45125  0.04673247  16.05482
  1       0.010  1.00  21.34455  0.05257253  15.99718
  1       0.100  0.25  21.17214  0.05962515  15.92961
  1       0.100  0.50  21.07006  0.06517143  15.85584
  1       0.100  1.00  21.04350  0.06729578  15.81344
  2       0.001  0.25  21.70851  0.04660306  16.23312
  2       0.001  0.50  21.60495  0.04365499  16.18408
  2       0.001  1.00  21.53117  0.04399434  16.11510
  2       0.010  0.25  20.96702  0.09679200  15.65323
  2       0.010  0.50  20.69920  0.10541354  15.50924
  2       0.010  1.00  20.52576  0.10842796  15.37663
  2       0.100  0.25  20.58476  0.11518396  15.11732
  2       0.100  0.50  20.54240  0.11870581  15.05456
  2       0.100  1.00  20.47058  0.12401541  15.00914
  3       0.001  0.25  21.63458  0.04623481  16.19345
  3       0.001  0.50  21.54100  0.04567835  16.12368
  3       0.001  1.00  21.42821  0.05270011  16.02566
  3       0.010  0.25  20.67340  0.10727968  15.48044
  3       0.010  0.50  20.51634  0.11097629  15.33455
  3       0.010  1.00  20.53902  0.11191552  15.23079
  3       0.100  0.25  20.69373  0.11925448  15.18721
  3       0.100  0.50  21.11717  0.10895136  15.34861
  3       0.100  1.00  22.01203  0.09598412  15.70899

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 120 of 224 using same hudgins svmPoly 4 
Support Vector Machines with Polynomial Kernel 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 892, 894, 893, 896, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.80458  0.04420137  16.28930
  1       0.001  0.50  21.78853  0.04294533  16.24888
  1       0.001  1.00  21.71857  0.04176958  16.22319
  1       0.010  0.25  21.62954  0.03964488  16.16879
  1       0.010  0.50  21.56716  0.04295037  16.11489
  1       0.010  1.00  21.46078  0.04792670  16.05575
  1       0.100  0.25  21.28282  0.05492514  15.99564
  1       0.100  0.50  21.16925  0.05962861  15.92194
  1       0.100  1.00  21.14306  0.06149083  15.86877
  2       0.001  0.25  21.78424  0.04420393  16.24078
  2       0.001  0.50  21.70684  0.04349617  16.20880
  2       0.001  1.00  21.62616  0.04251937  16.15963
  2       0.010  0.25  21.16284  0.08949169  15.72179
  2       0.010  0.50  20.92130  0.09426869  15.57425
  2       0.010  1.00  20.76311  0.09355684  15.46074
  2       0.100  0.25  20.83849  0.09618532  15.22298
  2       0.100  0.50  20.79038  0.09945322  15.16507
  2       0.100  1.00  20.69708  0.10459351  15.12845
  3       0.001  0.25  21.74086  0.04530159  16.21216
  3       0.001  0.50  21.63794  0.04507599  16.16380
  3       0.001  1.00  21.54034  0.04961072  16.08003
  3       0.010  0.25  20.93718  0.09172754  15.57714
  3       0.010  0.50  20.78391  0.09319240  15.44695
  3       0.010  1.00  20.82383  0.09284941  15.34500
  3       0.100  0.25  21.04940  0.10013883  15.34393
  3       0.100  0.50  21.31948  0.09734412  15.53265
  3       0.100  1.00  22.03604  0.08663901  15.81928

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 121 of 224 using max all ranger 4 
Random Forest 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 894, 894, 893, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.30613  0.3522983  12.37032
   2    extratrees  17.49148  0.3503463  12.82550
  13    variance    17.53831  0.3280713  12.32181
  13    extratrees  17.16460  0.3620691  12.18772
  24    variance    17.77434  0.3098978  12.53090
  24    extratrees  17.16296  0.3605147  12.14332

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 122 of 224 using same all ranger 4 
Random Forest 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 893, 893, 893, 896, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.40377  0.3503098  12.62792
   2    extratrees  17.60320  0.3467484  12.99679
  13    variance    17.55515  0.3308868  12.56384
  13    extratrees  17.20522  0.3645806  12.41219
  24    variance    17.81910  0.3101900  12.77175
  24    extratrees  17.20141  0.3628843  12.38558

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 123 of 224 using max du ranger 4 
Random Forest 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 893, 894, 893, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.60597  0.3295863  12.68109
   2    extratrees  17.90975  0.3143795  13.21884
  10    variance    17.66614  0.3183107  12.46433
  10    extratrees  17.51352  0.3356496  12.54485
  18    variance    17.86317  0.3030321  12.60981
  18    extratrees  17.51234  0.3340249  12.48942

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 124 of 224 using same du ranger 4 
Random Forest 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 892, 894, 895, 892, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.74634  0.3230226  12.95410
   2    extratrees  18.06744  0.3055538  13.40274
  10    variance    17.71402  0.3183383  12.74414
  10    extratrees  17.62001  0.3317176  12.78640
  18    variance    17.87009  0.3059124  12.83671
  18    extratrees  17.61236  0.3300362  12.76298

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 125 of 224 using max rms ranger 4 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 895, 894, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.66130  0.3180662  12.34268
  2     extratrees  17.24538  0.3552715  12.48374
  3     variance    17.79850  0.3099734  12.35707
  3     extratrees  17.31966  0.3456043  12.43060

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 126 of 224 using same rms ranger 4 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 895, 894, 894, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.47696  0.3354822  12.37257
  2     extratrees  17.19535  0.3648135  12.57082
  3     variance    17.61431  0.3265218  12.41641
  3     extratrees  17.19702  0.3603340  12.49728

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 127 of 224 using max hudgins ranger 4 
Random Forest 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 895, 895, 895, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.13266  0.2877210  13.23271
   2    extratrees  18.35918  0.2764547  13.67464
   7    variance    18.01993  0.2929383  12.83324
   7    extratrees  17.97505  0.2999759  13.04037
  12    variance    18.11542  0.2862169  12.85814
  12    extratrees  17.92330  0.3021943  12.92419

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 128 of 224 using same hudgins ranger 4 
Random Forest 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 892, 895, 896, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.33802  0.2721147  13.54917
   2    extratrees  18.62329  0.2548922  13.95276
   7    variance    18.12494  0.2852038  13.13622
   7    extratrees  18.24302  0.2801444  13.39721
  12    variance    18.14622  0.2840947  13.06730
  12    extratrees  18.17951  0.2840176  13.29601

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 129 of 224 using max all lm 5 
Linear Regression 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 893, 895, 893, 894, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  19.73937  0.1056159  15.52649

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 130 of 224 using same all lm 5 
Linear Regression 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 895, 894, 892, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.53522  0.09414465  16.15264

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 131 of 224 using max du lm 5 
Linear Regression 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 893, 894, 893, 893, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  19.80459  0.1007407  15.55422

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 132 of 224 using same du lm 5 
Linear Regression 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 894, 894, 894, 894, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.57225  0.08838001  16.11271

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 133 of 224 using max rms lm 5 
Linear Regression 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 894, 893, 894, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.09698  0.06969231  16.16473

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 134 of 224 using same rms lm 5 
Linear Regression 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 894, 894, 894, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.88454  0.05820264  16.72084

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 135 of 224 using max hudgins lm 5 
Linear Regression 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 894, 893, 894, 893, 893, ... 
Resampling results:

  RMSE     Rsquared    MAE     
  19.9679  0.08430726  15.93518

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 136 of 224 using same hudgins lm 5 
Linear Regression 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 893, 894, 894, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.73857  0.07244826  16.49895

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 137 of 224 using max all knn 5 
k-Nearest Neighbors 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 895, 893, 893, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  16.96252  0.3514961  11.53956
  7  16.99468  0.3422647  11.88263
  9  17.09255  0.3318793  12.12074

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 138 of 224 using same all knn 5 
k-Nearest Neighbors 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 894, 893, 893, 894, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.86951  0.3247069  11.91620
  7  17.89183  0.3153677  12.21828
  9  18.05008  0.3011760  12.50837

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 139 of 224 using max du knn 5 
k-Nearest Neighbors 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 893, 893, 896, 893, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.06440  0.3435047  11.54738
  7  17.15271  0.3293909  11.94394
  9  17.20374  0.3219619  12.16266

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 140 of 224 using same du knn 5 
k-Nearest Neighbors 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 893, 893, 893, 894, 895, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.88926  0.3224964  11.96084
  7  17.91289  0.3135141  12.25517
  9  18.05049  0.3005293  12.50824

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 141 of 224 using max rms knn 5 
k-Nearest Neighbors 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 893, 893, 894, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.40922  0.3176562  11.97488
  7  17.28218  0.3185936  12.11704
  9  17.28393  0.3154347  12.39540

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 142 of 224 using same rms knn 5 
k-Nearest Neighbors 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 894, 893, 893, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.11405  0.3034674  12.34661
  7  18.13003  0.2953323  12.69026
  9  18.21884  0.2853139  12.95844

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 143 of 224 using max hudgins knn 5 
k-Nearest Neighbors 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 894, 893, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.63685  0.1542016  14.52238
  7  19.46021  0.1535507  14.52130
  9  19.25355  0.1629697  14.46201

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 144 of 224 using same hudgins knn 5 
k-Nearest Neighbors 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 894, 894, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.42120  0.1470246  14.83907
  7  20.09405  0.1526486  14.82145
  9  19.88690  0.1602546  14.79027

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 145 of 224 using max all svmPoly 5 
Support Vector Machines with Polynomial Kernel 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 893, 894, 894, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.01195  0.06374228  15.90474
  1       0.001  0.50  20.85441  0.06348387  15.85530
  1       0.001  1.00  20.71801  0.06635705  15.76678
  1       0.010  0.25  20.54073  0.07285773  15.62678
  1       0.010  0.50  20.40154  0.07949877  15.50421
  1       0.010  1.00  20.27112  0.08593725  15.39243
  1       0.100  0.25  20.15040  0.09299066  15.26851
  1       0.100  0.50  20.11108  0.09601810  15.18670
  1       0.100  1.00  20.11701  0.09721368  15.11541
  2       0.001  0.25  20.81670  0.06864421  15.81476
  2       0.001  0.50  20.64953  0.07492796  15.69448
  2       0.001  1.00  20.46661  0.08586529  15.53642
  2       0.010  0.25  19.90962  0.11748888  14.88903
  2       0.010  0.50  19.86620  0.11970931  14.77232
  2       0.010  1.00  19.80399  0.12409863  14.65191
  2       0.100  0.25  19.37135  0.15946211  14.06778
  2       0.100  0.50  19.30673  0.16568445  14.00409
  2       0.100  1.00  19.29266  0.16924519  13.95631
  3       0.001  0.25  20.66913  0.07858839  15.69663
  3       0.001  0.50  20.47830  0.09168312  15.52806
  3       0.001  1.00  20.24611  0.10558550  15.32651
  3       0.010  0.25  19.73750  0.12896775  14.60133
  3       0.010  0.50  19.66180  0.13501096  14.43512
  3       0.010  1.00  19.60748  0.13917682  14.31703
  3       0.100  0.25  21.08562  0.12085825  14.96392
  3       0.100  0.50  21.64726  0.11804831  15.20086
  3       0.100  1.00  23.30476  0.10358364  15.79192

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 146 of 224 using same all svmPoly 5 
Support Vector Machines with Polynomial Kernel 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 894, 893, 894, 893, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.91224  0.05181753  16.33077
  1       0.001  0.50  21.77475  0.05172237  16.29321
  1       0.001  1.00  21.68005  0.05267143  16.23203
  1       0.010  0.25  21.50184  0.05730305  16.09610
  1       0.010  0.50  21.37666  0.06298372  15.97929
  1       0.010  1.00  21.24131  0.07004220  15.85887
  1       0.100  0.25  21.06370  0.07882728  15.71608
  1       0.100  0.50  21.01354  0.08176903  15.65433
  1       0.100  1.00  21.00054  0.08353858  15.60151
  2       0.001  0.25  21.72597  0.05816509  16.24774
  2       0.001  0.50  21.60100  0.06403599  16.14284
  2       0.001  1.00  21.40062  0.07300861  15.98162
  2       0.010  0.25  20.68210  0.11203264  15.31762
  2       0.010  0.50  20.64705  0.11288237  15.24337
  2       0.010  1.00  20.59427  0.11719879  15.14868
  2       0.100  0.25  20.13971  0.15181052  14.52922
  2       0.100  0.50  20.09294  0.15718084  14.47136
  2       0.100  1.00  20.09493  0.15947407  14.47336
  3       0.001  0.25  21.59608  0.06993896  16.12403
  3       0.001  0.50  21.38656  0.08231422  15.93391
  3       0.001  1.00  21.13501  0.09734940  15.69897
  3       0.010  0.25  20.50607  0.12303502  15.05844
  3       0.010  0.50  20.41148  0.12887655  14.91355
  3       0.010  1.00  20.30146  0.13663987  14.77391
  3       0.100  0.25  22.85207  0.09791816  15.76289
  3       0.100  0.50  24.45062  0.08142263  16.35236
  3       0.100  1.00  26.88841  0.06394788  17.27715

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 147 of 224 using max du svmPoly 5 
Support Vector Machines with Polynomial Kernel 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 893, 894, 893, 893, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.11578  0.05760873  15.94524
  1       0.001  0.50  20.95501  0.05689912  15.88070
  1       0.001  1.00  20.83049  0.05815012  15.81144
  1       0.010  0.25  20.65960  0.06541847  15.67642
  1       0.010  0.50  20.50311  0.07401978  15.56266
  1       0.010  1.00  20.35055  0.08336839  15.43747
  1       0.100  0.25  20.19458  0.09095870  15.31177
  1       0.100  0.50  20.13278  0.09339317  15.24824
  1       0.100  1.00  20.13368  0.09348179  15.21668
  2       0.001  0.25  20.93847  0.05914632  15.86307
  2       0.001  0.50  20.80298  0.06190678  15.78021
  2       0.001  1.00  20.64474  0.07045207  15.65623
  2       0.010  0.25  20.10830  0.11260352  15.15601
  2       0.010  0.50  19.92707  0.11490431  15.00773
  2       0.010  1.00  19.88728  0.11727563  14.88145
  2       0.100  0.25  19.60001  0.14008197  14.44277
  2       0.100  0.50  19.47771  0.15061983  14.29883
  2       0.100  1.00  19.46034  0.15435209  14.27090
  3       0.001  0.25  20.83398  0.06321339  15.79408
  3       0.001  0.50  20.67176  0.07234745  15.67150
  3       0.001  1.00  20.48744  0.08526610  15.52328
  3       0.010  0.25  19.91618  0.11662591  14.97672
  3       0.010  0.50  19.85184  0.12271047  14.80144
  3       0.010  1.00  19.81029  0.12524263  14.68208
  3       0.100  0.25  20.91609  0.11269293  15.05255
  3       0.100  0.50  22.18889  0.10098397  15.46016
  3       0.100  1.00  23.30829  0.09764754  15.76837

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 148 of 224 using same du svmPoly 5 
Support Vector Machines with Polynomial Kernel 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 895, 893, 894, 893, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.92223  0.04738477  16.34790
  1       0.001  0.50  21.84702  0.04656825  16.30963
  1       0.001  1.00  21.73873  0.04677193  16.26477
  1       0.010  0.25  21.57374  0.05140626  16.13799
  1       0.010  0.50  21.42380  0.05874924  16.01172
  1       0.010  1.00  21.26251  0.06770060  15.89684
  1       0.100  0.25  21.09322  0.07614486  15.76168
  1       0.100  0.50  21.04286  0.07798583  15.72106
  1       0.100  1.00  21.04870  0.07723174  15.71028
  2       0.001  0.25  21.82899  0.04967609  16.28905
  2       0.001  0.50  21.70767  0.05223390  16.22544
  2       0.001  1.00  21.56386  0.05831991  16.10773
  2       0.010  0.25  20.90077  0.10718618  15.51678
  2       0.010  0.50  20.71258  0.10790300  15.41590
  2       0.010  1.00  20.65499  0.10908832  15.32544
  2       0.100  0.25  20.31673  0.13292407  14.89530
  2       0.100  0.50  20.21877  0.14206247  14.79010
  2       0.100  1.00  20.20810  0.14477442  14.76509
  3       0.001  0.25  21.73300  0.05527693  16.22953
  3       0.001  0.50  21.59422  0.06244236  16.10353
  3       0.001  1.00  21.34845  0.07681235  15.92060
  3       0.010  0.25  20.67538  0.11068272  15.37334
  3       0.010  0.50  20.59671  0.11493026  15.22018
  3       0.010  1.00  20.54305  0.11833319  15.11812
  3       0.100  0.25  22.40207  0.10523681  15.60887
  3       0.100  0.50  24.12343  0.09028163  16.14362
  3       0.100  1.00  26.17871  0.07621993  16.72300

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 149 of 224 using max rms svmPoly 5 
Support Vector Machines with Polynomial Kernel 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 893, 894, 894, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.49480  0.06857480  16.10560
  1       0.001  0.50  21.31389  0.06932117  16.03787
  1       0.001  1.00  21.13777  0.06864473  15.97641
  1       0.010  0.25  20.87049  0.06769956  15.92738
  1       0.010  0.50  20.75807  0.06660667  15.92805
  1       0.010  1.00  20.69114  0.06473592  15.94371
  1       0.100  0.25  20.64600  0.06332050  15.95815
  1       0.100  0.50  20.63206  0.06266165  15.96421
  1       0.100  1.00  20.62354  0.06224122  15.96830
  2       0.001  0.25  21.31327  0.06967510  16.03668
  2       0.001  0.50  21.13527  0.06914974  15.97402
  2       0.001  1.00  20.90809  0.06851368  15.93056
  2       0.010  0.25  20.62584  0.08354942  15.79195
  2       0.010  0.50  20.46208  0.09408491  15.68681
  2       0.010  1.00  20.26094  0.10974174  15.49409
  2       0.100  0.25  19.83363  0.12762362  14.64422
  2       0.100  0.50  19.85970  0.12666561  14.64948
  2       0.100  1.00  19.87428  0.12608397  14.65310
  3       0.001  0.25  21.25848  0.06984302  15.99960
  3       0.001  0.50  20.97862  0.06925609  15.94029
  3       0.001  1.00  20.82234  0.06966885  15.90896
  3       0.010  0.25  20.39141  0.10790699  15.57380
  3       0.010  0.50  20.15142  0.12167947  15.29341
  3       0.010  1.00  19.95325  0.12661659  14.89260
  3       0.100  0.25  19.66636  0.14297591  14.26255
  3       0.100  0.50  19.63748  0.14522803  14.16138
  3       0.100  1.00  19.62586  0.14607490  14.09527

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 150 of 224 using same rms svmPoly 5 
Support Vector Machines with Polynomial Kernel 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 892, 894, 893, 895, 895, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.35800  0.05599143  16.48434
  1       0.001  0.50  22.08027  0.05663734  16.42129
  1       0.001  1.00  22.05633  0.05613218  16.38381
  1       0.010  0.25  21.78041  0.05494398  16.35054
  1       0.010  0.50  21.67409  0.05510773  16.33900
  1       0.010  1.00  21.63127  0.05454268  16.34983
  1       0.100  0.25  21.59963  0.05363106  16.36308
  1       0.100  0.50  21.58502  0.05311404  16.37043
  1       0.100  1.00  21.58021  0.05268801  16.37511
  2       0.001  0.25  22.07937  0.05705167  16.41988
  2       0.001  0.50  22.05417  0.05673181  16.38161
  2       0.001  1.00  21.83170  0.05622361  16.35249
  2       0.010  0.25  21.51541  0.07460541  16.19552
  2       0.010  0.50  21.33703  0.08875469  16.06854
  2       0.010  1.00  21.14766  0.10454992  15.84421
  2       0.100  0.25  20.59371  0.11724559  15.07639
  2       0.100  0.50  20.60819  0.11607375  15.08962
  2       0.100  1.00  20.62640  0.11515232  15.10484
  3       0.001  0.25  22.08075  0.05733549  16.39779
  3       0.001  0.50  21.93922  0.05718300  16.36455
  3       0.001  1.00  21.72515  0.05741929  16.32853
  3       0.010  0.25  21.26855  0.10208750  15.94741
  3       0.010  0.50  21.01244  0.11439626  15.63450
  3       0.010  1.00  20.72292  0.11717508  15.28275
  3       0.100  0.25  20.33156  0.13764368  14.65005
  3       0.100  0.50  20.28146  0.14114564  14.55386
  3       0.100  1.00  20.26084  0.14430902  14.48408

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 151 of 224 using max hudgins svmPoly 5 
Support Vector Machines with Polynomial Kernel 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 893, 894, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.19193  0.05188727  16.01473
  1       0.001  0.50  21.12254  0.05009988  15.96016
  1       0.001  1.00  21.00514  0.04843530  15.91509
  1       0.010  0.25  20.91600  0.04858682  15.85213
  1       0.010  0.50  20.84330  0.05332729  15.77070
  1       0.010  1.00  20.74303  0.05936176  15.70087
  1       0.100  0.25  20.58539  0.06710540  15.62853
  1       0.100  0.50  20.45049  0.07258520  15.55011
  1       0.100  1.00  20.43637  0.07321926  15.51434
  2       0.001  0.25  21.11746  0.05089478  15.95348
  2       0.001  0.50  20.99616  0.04990547  15.90367
  2       0.001  1.00  20.91934  0.05019441  15.85076
  2       0.010  0.25  20.50749  0.09075908  15.46495
  2       0.010  0.50  20.24293  0.09932494  15.26875
  2       0.010  1.00  20.07277  0.10312973  15.10705
  2       0.100  0.25  19.93181  0.11930438  14.80438
  2       0.100  0.50  19.84609  0.12557267  14.68752
  2       0.100  1.00  19.77591  0.13176099  14.61107
  3       0.001  0.25  21.03955  0.05122837  15.91245
  3       0.001  0.50  20.93727  0.05103230  15.86132
  3       0.001  1.00  20.83166  0.05679278  15.76890
  3       0.010  0.25  20.20343  0.10148362  15.22650
  3       0.010  0.50  20.05342  0.10477439  15.07578
  3       0.010  1.00  20.02952  0.10785969  14.98298
  3       0.100  0.25  20.28149  0.12068236  14.80681
  3       0.100  0.50  20.54169  0.11869187  14.91652
  3       0.100  1.00  21.24799  0.11233494  15.12051

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 152 of 224 using same hudgins svmPoly 5 
Support Vector Machines with Polynomial Kernel 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 894, 894, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.96283  0.04079291  16.41876
  1       0.001  0.50  21.95531  0.03927281  16.37433
  1       0.001  1.00  21.91773  0.03649109  16.34709
  1       0.010  0.25  21.88730  0.03397291  16.30897
  1       0.010  0.50  21.81620  0.03558072  16.24894
  1       0.010  1.00  21.70717  0.04057110  16.18337
  1       0.100  0.25  21.52908  0.04841552  16.10501
  1       0.100  0.50  21.38905  0.05349304  16.06215
  1       0.100  1.00  21.32868  0.05630038  16.01313
  2       0.001  0.25  21.95090  0.04045111  16.36665
  2       0.001  0.50  21.90714  0.03840183  16.33244
  2       0.001  1.00  21.87267  0.03734486  16.29480
  2       0.010  0.25  21.34596  0.08685412  15.80570
  2       0.010  0.50  21.06301  0.09519422  15.63558
  2       0.010  1.00  20.85007  0.09885913  15.50248
  2       0.100  0.25  20.75420  0.10742739  15.31090
  2       0.100  0.50  20.66083  0.11336621  15.20728
  2       0.100  1.00  20.57898  0.11855495  15.14210
  3       0.001  0.25  21.92313  0.04068747  16.33607
  3       0.001  0.50  21.86830  0.04030389  16.29210
  3       0.001  1.00  21.78185  0.04274179  16.21071
  3       0.010  0.25  21.02223  0.09700873  15.61835
  3       0.010  0.50  20.86218  0.09864390  15.50266
  3       0.010  1.00  20.81607  0.10142726  15.40699
  3       0.100  0.25  20.84016  0.11379978  15.24845
  3       0.100  0.50  21.49066  0.10084503  15.55778
  3       0.100  1.00  22.99922  0.08773296  15.98914

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 153 of 224 using max all ranger 5 
Random Forest 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 893, 893, 893, 893, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.43443  0.3911940  11.90830
   2    extratrees  16.65648  0.3910898  12.34969
  13    variance    16.46473  0.3786417  11.74094
  13    extratrees  16.13218  0.4136971  11.55293
  24    variance    16.66853  0.3625682  11.89869
  24    extratrees  16.14010  0.4096448  11.52008

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 154 of 224 using same all ranger 5 
Random Forest 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 894, 895, 893, 895, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.19012  0.3721182  12.40350
   2    extratrees  17.36149  0.3764485  12.78598
  13    variance    17.35077  0.3510704  12.30144
  13    extratrees  16.94715  0.3884616  12.08125
  24    variance    17.58052  0.3338677  12.45272
  24    extratrees  17.00738  0.3814519  12.08208

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 155 of 224 using max du ranger 5 
Random Forest 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 895, 894, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.75302  0.3672221  12.19477
   2    extratrees  17.11782  0.3519135  12.77008
  10    variance    16.56400  0.3706960  11.80302
  10    extratrees  16.50990  0.3853298  11.90674
  18    variance    16.70922  0.3591436  11.90303
  18    extratrees  16.47306  0.3852594  11.82497

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 156 of 224 using same du ranger 5 
Random Forest 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 894, 894, 894, 892, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.50297  0.3492237  12.70440
   2    extratrees  17.80817  0.3375186  13.21487
  10    variance    17.46685  0.3430247  12.41301
  10    extratrees  17.29093  0.3636492  12.44579
  18    variance    17.66048  0.3284439  12.50977
  18    extratrees  17.30480  0.3600817  12.40545

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = extratrees
 and min.node.size = 5.


Now processing model 157 of 224 using max rms ranger 5 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 894, 893, 894, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.51240  0.3739043  11.70193
  2     extratrees  16.36658  0.3960715  11.94109
  3     variance    16.61920  0.3658346  11.70182
  3     extratrees  16.33590  0.3935632  11.80038

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 158 of 224 using same rms ranger 5 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 894, 894, 894, 893, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.38801  0.3469693  12.31198
  2     extratrees  17.09001  0.3788085  12.42907
  3     variance    17.55636  0.3360653  12.34209
  3     extratrees  17.06052  0.3764151  12.29896

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 159 of 224 using max hudgins ranger 5 
Random Forest 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 893, 894, 895, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.39524  0.3124219  12.84349
   2    extratrees  17.70302  0.2974934  13.35139
   7    variance    17.01056  0.3369517  12.26917
   7    extratrees  17.15692  0.3332950  12.59917
  12    variance    17.05601  0.3330873  12.24682
  12    extratrees  17.03527  0.3409182  12.42541

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 160 of 224 using same hudgins ranger 5 
Random Forest 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 894, 894, 895, 893, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.23263  0.2858898  13.39536
   2    extratrees  18.39998  0.2829235  13.80439
   7    variance    18.09830  0.2930541  13.04352
   7    extratrees  17.91084  0.3126346  13.08619
  12    variance    18.13187  0.2914644  12.99328
  12    extratrees  17.88920  0.3118820  12.99718

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 161 of 224 using max all lm 6 
Linear Regression 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 895, 892, 893, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.56539  0.09553335  16.14983

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 162 of 224 using same all lm 6 
Linear Regression 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 894, 893, 894, 893, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.19961  0.1042809  15.95183

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 163 of 224 using max du lm 6 
Linear Regression 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 894, 893, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.61806  0.08923342  16.17413

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 164 of 224 using same du lm 6 
Linear Regression 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 893, 894, 895, 892, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.26449  0.09643393  15.98149

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 165 of 224 using max rms lm 6 
Linear Regression 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 895, 895, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE    
  20.91928  0.06020682  16.7094

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 166 of 224 using same rms lm 6 
Linear Regression 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 894, 894, 894, 893, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.57574  0.06754517  16.50918

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 167 of 224 using max hudgins lm 6 
Linear Regression 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 894, 894, 892, 894, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.74385  0.0768247  16.54652

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 168 of 224 using same hudgins lm 6 
Linear Regression 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 894, 893, 893, 895, ... 
Resampling results:

  RMSE      Rsquared    MAE    
  20.38164  0.08566073  16.3715

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 169 of 224 using max all knn 6 
k-Nearest Neighbors 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 894, 894, 893, 894, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.01289  0.3196044  12.18135
  7  18.06721  0.3063959  12.53033
  9  18.04577  0.3036796  12.66227

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 170 of 224 using same all knn 6 
k-Nearest Neighbors 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 893, 893, 893, 895, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.78842  0.3197000  12.14840
  7  17.99233  0.2969775  12.57705
  9  18.02885  0.2896237  12.79721

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 171 of 224 using max du knn 6 
k-Nearest Neighbors 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 894, 893, 894, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.96414  0.3234640  12.12637
  7  18.09047  0.3060662  12.49969
  9  18.07275  0.3019307  12.68155

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 172 of 224 using same du knn 6 
k-Nearest Neighbors 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 894, 893, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.82693  0.3136068  12.11576
  7  18.04955  0.2894413  12.56972
  9  18.10328  0.2810003  12.80007

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 173 of 224 using max rms knn 6 
k-Nearest Neighbors 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 894, 894, 895, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.89679  0.3222334  12.18608
  7  17.95979  0.3121395  12.56869
  9  18.09810  0.2988718  12.86262

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 174 of 224 using same rms knn 6 
k-Nearest Neighbors 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 893, 893, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.87482  0.3070418  12.38160
  7  17.84598  0.3029397  12.73549
  9  17.98564  0.2893491  13.04903

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 175 of 224 using max hudgins knn 6 
k-Nearest Neighbors 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 894, 895, 893, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.38800  0.1533415  14.91055
  7  20.15592  0.1533668  14.94132
  9  20.21889  0.1423526  15.12198

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 176 of 224 using same hudgins knn 6 
k-Nearest Neighbors 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 894, 894, 893, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.29342  0.1427694  14.91668
  7  20.14575  0.1381363  14.95688
  9  20.07975  0.1330653  15.03909

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 177 of 224 using max all svmPoly 6 
Support Vector Machines with Polynomial Kernel 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 893, 893, 894, 893, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.88344  0.05012310  16.35852
  1       0.001  0.50  21.79241  0.04757767  16.31776
  1       0.001  1.00  21.64656  0.04968024  16.23564
  1       0.010  0.25  21.45373  0.05784840  16.10871
  1       0.010  0.50  21.35218  0.06486055  16.01904
  1       0.010  1.00  21.24121  0.07244540  15.91656
  1       0.100  0.25  21.11179  0.08085439  15.77821
  1       0.100  0.50  21.05958  0.08369031  15.72013
  1       0.100  1.00  21.00733  0.08694312  15.65720
  2       0.001  0.25  21.75191  0.05249485  16.27474
  2       0.001  0.50  21.56413  0.05851646  16.15451
  2       0.001  1.00  21.37080  0.07178966  16.02185
  2       0.010  0.25  20.71659  0.11065057  15.52397
  2       0.010  0.50  20.63999  0.11532950  15.39332
  2       0.010  1.00  20.55847  0.12135439  15.24981
  2       0.100  0.25  20.10422  0.15861409  14.62426
  2       0.100  0.50  20.00089  0.16799328  14.49642
  2       0.100  1.00  19.97038  0.17212424  14.46075
  3       0.001  0.25  21.57405  0.06251087  16.14213
  3       0.001  0.50  21.37433  0.07849470  16.00094
  3       0.001  1.00  21.15601  0.09241243  15.84637
  3       0.010  0.25  20.48516  0.12742466  15.18319
  3       0.010  0.50  20.42245  0.13230995  15.02870
  3       0.010  1.00  20.34440  0.13981135  14.87047
  3       0.100  0.25  21.36956  0.12289512  15.39163
  3       0.100  0.50  22.28618  0.11477736  15.80873
  3       0.100  1.00  23.64060  0.10434145  16.33130

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 178 of 224 using same all svmPoly 6 
Support Vector Machines with Polynomial Kernel 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 893, 894, 894, 895, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.57741  0.05718414  16.19251
  1       0.001  0.50  21.41088  0.05774948  16.15030
  1       0.001  1.00  21.30214  0.05745519  16.10358
  1       0.010  0.25  21.15332  0.06222142  15.99145
  1       0.010  0.50  21.02019  0.06836443  15.89397
  1       0.010  1.00  20.86262  0.07539600  15.78546
  1       0.100  0.25  20.75090  0.08201444  15.67069
  1       0.100  0.50  20.70555  0.08497900  15.60655
  1       0.100  1.00  20.68362  0.08675695  15.53033
  2       0.001  0.25  21.37359  0.06316272  16.11355
  2       0.001  0.50  21.23704  0.06707961  16.03129
  2       0.001  1.00  21.07165  0.07456806  15.90247
  2       0.010  0.25  20.40057  0.11250750  15.29575
  2       0.010  0.50  20.30278  0.11597017  15.19808
  2       0.010  1.00  20.23809  0.11985343  15.08611
  2       0.100  0.25  19.85874  0.15038118  14.52919
  2       0.100  0.50  19.81845  0.15546848  14.45688
  2       0.100  1.00  19.86010  0.15795901  14.42442
  3       0.001  0.25  21.24803  0.07174092  16.01932
  3       0.001  0.50  21.07245  0.08153960  15.87231
  3       0.001  1.00  20.83414  0.09557828  15.67708
  3       0.010  0.25  20.20217  0.12318841  15.04880
  3       0.010  0.50  20.13596  0.12724974  14.90322
  3       0.010  1.00  20.04041  0.13461168  14.72782
  3       0.100  0.25  21.50615  0.12327810  15.21943
  3       0.100  0.50  22.43279  0.11299131  15.61543
  3       0.100  1.00  22.95916  0.10329651  16.05715

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 179 of 224 using max du svmPoly 6 
Support Vector Machines with Polynomial Kernel 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 894, 893, 893, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.89015  0.04980529  16.36500
  1       0.001  0.50  21.83206  0.04640512  16.31947
  1       0.001  1.00  21.74615  0.04540977  16.25669
  1       0.010  0.25  21.56223  0.05311018  16.13204
  1       0.010  0.50  21.41412  0.06276568  16.03797
  1       0.010  1.00  21.29306  0.07074353  15.94650
  1       0.100  0.25  21.14724  0.07793371  15.83282
  1       0.100  0.50  21.10633  0.07937389  15.79412
  1       0.100  1.00  21.13272  0.07748521  15.79227
  2       0.001  0.25  21.82057  0.04809457  16.30146
  2       0.001  0.50  21.71974  0.04874382  16.22409
  2       0.001  1.00  21.54647  0.05826945  16.10615
  2       0.010  0.25  20.97242  0.10568855  15.71216
  2       0.010  0.50  20.77414  0.10735082  15.62120
  2       0.010  1.00  20.67550  0.11185123  15.50488
  2       0.100  0.25  20.33484  0.13795134  15.00813
  2       0.100  0.50  20.23421  0.14717043  14.87553
  2       0.100  1.00  20.21909  0.15115133  14.82982
  3       0.001  0.25  21.75277  0.04997660  16.23398
  3       0.001  0.50  21.57470  0.06010994  16.11396
  3       0.001  1.00  21.37718  0.07460534  15.98051
  3       0.010  0.25  20.70929  0.11221385  15.55896
  3       0.010  0.50  20.61147  0.11882846  15.39841
  3       0.010  1.00  20.56458  0.12301132  15.25097
  3       0.100  0.25  21.13805  0.12650487  15.29863
  3       0.100  0.50  21.62429  0.11610048  15.55337
  3       0.100  1.00  23.01704  0.10387617  16.06250

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 180 of 224 using same du svmPoly 6 
Support Vector Machines with Polynomial Kernel 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 893, 893, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.64224  0.05537685  16.21358
  1       0.001  0.50  21.51052  0.05528120  16.17510
  1       0.001  1.00  21.38520  0.05626452  16.13201
  1       0.010  0.25  21.24288  0.06236920  16.02286
  1       0.010  0.50  21.08460  0.07115854  15.91025
  1       0.010  1.00  20.94157  0.07762991  15.81255
  1       0.100  0.25  20.80255  0.08354035  15.71435
  1       0.100  0.50  20.74455  0.08424934  15.68478
  1       0.100  1.00  20.77443  0.08180938  15.66901
  2       0.001  0.25  21.49343  0.05784365  16.15952
  2       0.001  0.50  21.35868  0.06074501  16.10154
  2       0.001  1.00  21.23164  0.06683343  16.00339
  2       0.010  0.25  20.63647  0.10819428  15.49495
  2       0.010  0.50  20.38507  0.11226196  15.35906
  2       0.010  1.00  20.32572  0.11325313  15.29420
  2       0.100  0.25  20.10615  0.12925321  14.94337
  2       0.100  0.50  20.02759  0.13611587  14.83722
  2       0.100  1.00  20.01706  0.14071003  14.77656
  3       0.001  0.25  21.38725  0.06225778  16.10841
  3       0.001  0.50  21.25377  0.06925759  16.00891
  3       0.001  1.00  21.05165  0.08132485  15.85094
  3       0.010  0.25  20.37313  0.11433879  15.33296
  3       0.010  0.50  20.30492  0.11712068  15.22409
  3       0.010  1.00  20.27938  0.12027828  15.10458
  3       0.100  0.25  20.91138  0.12099000  15.23604
  3       0.100  0.50  21.70985  0.11194797  15.50870
  3       0.100  1.00  22.65707  0.09975615  15.93425

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 181 of 224 using max rms svmPoly 6 
Support Vector Machines with Polynomial Kernel 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 894, 895, 892, 895, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.40760  0.06096330  16.51169
  1       0.001  0.50  22.09014  0.06160687  16.45125
  1       0.001  1.00  22.01813  0.06088385  16.40816
  1       0.010  0.25  21.80795  0.05691782  16.38153
  1       0.010  0.50  21.72393  0.05476821  16.37542
  1       0.010  1.00  21.64619  0.05198926  16.39565
  1       0.100  0.25  21.55338  0.04988339  16.40590
  1       0.100  0.50  21.50736  0.04955561  16.40340
  1       0.100  1.00  21.48962  0.04900002  16.40160
  2       0.001  0.25  22.08946  0.06202909  16.44984
  2       0.001  0.50  22.01685  0.06149341  16.40588
  2       0.001  1.00  21.86409  0.05856916  16.38495
  2       0.010  0.25  21.55540  0.07474619  16.24079
  2       0.010  0.50  21.36349  0.08635780  16.13690
  2       0.010  1.00  21.09469  0.10584329  15.92469
  2       0.100  0.25  20.68668  0.12124075  15.23532
  2       0.100  0.50  20.70125  0.12062668  15.21776
  2       0.100  1.00  20.71897  0.12026004  15.21298
  3       0.001  0.25  22.05871  0.06242140  16.41845
  3       0.001  0.50  21.94317  0.06079318  16.38991
  3       0.001  1.00  21.76035  0.05872371  16.36180
  3       0.010  0.25  21.26664  0.09953536  16.02356
  3       0.010  0.50  20.98871  0.11979259  15.75165
  3       0.010  1.00  20.81056  0.12108813  15.47576
  3       0.100  0.25  20.38009  0.14360397  14.75133
  3       0.100  0.50  20.31751  0.14783425  14.63967
  3       0.100  1.00  20.29739  0.15056761  14.58946

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 182 of 224 using same rms svmPoly 6 
Support Vector Machines with Polynomial Kernel 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 894, 893, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.07571  0.06427316  16.35612
  1       0.001  0.50  21.82386  0.06476431  16.28897
  1       0.001  1.00  21.75270  0.06383356  16.24109
  1       0.010  0.25  21.43121  0.06274142  16.20095
  1       0.010  0.50  21.30589  0.06221614  16.19844
  1       0.010  1.00  21.24501  0.06106320  16.21714
  1       0.100  0.25  21.20491  0.05926226  16.23847
  1       0.100  0.50  21.19036  0.05839803  16.24743
  1       0.100  1.00  21.18006  0.05822776  16.24980
  2       0.001  0.25  21.82369  0.06512883  16.28785
  2       0.001  0.50  21.75057  0.06439243  16.23906
  2       0.001  1.00  21.47914  0.06378682  16.20087
  2       0.010  0.25  21.17130  0.07948226  16.07202
  2       0.010  0.50  21.00533  0.09115968  15.96323
  2       0.010  1.00  20.83368  0.10435964  15.78733
  2       0.100  0.25  20.40988  0.11560501  15.12312
  2       0.100  0.50  20.43275  0.11473810  15.11672
  2       0.100  1.00  20.45614  0.11356422  15.12520
  3       0.001  0.25  21.79878  0.06522730  16.25502
  3       0.001  0.50  21.57768  0.06484372  16.21150
  3       0.001  1.00  21.37163  0.06492732  16.18311
  3       0.010  0.25  20.95890  0.10176331  15.86107
  3       0.010  0.50  20.72798  0.11379101  15.60592
  3       0.010  1.00  20.50546  0.11778829  15.30909
  3       0.100  0.25  20.13577  0.13992870  14.68001
  3       0.100  0.50  20.05407  0.14605465  14.55870
  3       0.100  1.00  20.06643  0.14646846  14.53149

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 183 of 224 using max hudgins svmPoly 6 
Support Vector Machines with Polynomial Kernel 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 894, 895, 894, 895, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.97910  0.04220315  16.44365
  1       0.001  0.50  21.92926  0.04046144  16.38702
  1       0.001  1.00  21.88116  0.03797250  16.35112
  1       0.010  0.25  21.82697  0.03623504  16.29483
  1       0.010  0.50  21.75343  0.03978707  16.23802
  1       0.010  1.00  21.65861  0.04557506  16.17940
  1       0.100  0.25  21.52133  0.05223210  16.14879
  1       0.100  0.50  21.41213  0.05700030  16.11995
  1       0.100  1.00  21.37231  0.05953138  16.09697
  2       0.001  0.25  21.92471  0.04147582  16.37967
  2       0.001  0.50  21.87361  0.03916315  16.33911
  2       0.001  1.00  21.82039  0.03805525  16.28752
  2       0.010  0.25  21.41335  0.07546980  15.95143
  2       0.010  0.50  21.18558  0.08405767  15.84410
  2       0.010  1.00  20.98250  0.08786662  15.75275
  2       0.100  0.25  20.76401  0.10662987  15.44020
  2       0.100  0.50  20.71192  0.11232006  15.34950
  2       0.100  1.00  20.60513  0.12015031  15.24821
  3       0.001  0.25  21.89497  0.04117689  16.34420
  3       0.001  0.50  21.83006  0.03974607  16.29313
  3       0.001  1.00  21.74473  0.04443959  16.19807
  3       0.010  0.25  21.16202  0.08523809  15.84170
  3       0.010  0.50  20.94267  0.09082658  15.73031
  3       0.010  1.00  20.88228  0.09642333  15.61543
  3       0.100  0.25  20.84975  0.12022502  15.28923
  3       0.100  0.50  20.85283  0.12506801  15.21486
  3       0.100  1.00  21.14151  0.12097544  15.33382

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 184 of 224 using same hudgins svmPoly 6 
Support Vector Machines with Polynomial Kernel 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 892, 893, 894, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.70089  0.04846462  16.27870
  1       0.001  0.50  21.66865  0.04638361  16.23724
  1       0.001  1.00  21.57029  0.04434007  16.20961
  1       0.010  0.25  21.47154  0.04217953  16.18222
  1       0.010  0.50  21.41178  0.04398456  16.13025
  1       0.010  1.00  21.33586  0.04680271  16.08660
  1       0.100  0.25  21.20320  0.05204453  16.03878
  1       0.100  0.50  21.08179  0.05752539  15.99215
  1       0.100  1.00  21.01614  0.06093558  15.95696
  2       0.001  0.25  21.66455  0.04737526  16.23093
  2       0.001  0.50  21.56080  0.04581314  16.19813
  2       0.001  1.00  21.46710  0.04529641  16.16661
  2       0.010  0.25  21.08700  0.08408289  15.78477
  2       0.010  0.50  20.79980  0.09483677  15.62089
  2       0.010  1.00  20.57112  0.10032185  15.49021
  2       0.100  0.25  20.51561  0.10455286  15.32063
  2       0.100  0.50  20.52903  0.10529125  15.30387
  2       0.100  1.00  20.48371  0.10993794  15.24865
  3       0.001  0.25  21.60302  0.04796013  16.20057
  3       0.001  0.50  21.48359  0.04762910  16.16331
  3       0.001  1.00  21.39827  0.04887418  16.10047
  3       0.010  0.25  20.73487  0.09940227  15.57468
  3       0.010  0.50  20.54332  0.10222476  15.46903
  3       0.010  1.00  20.50022  0.10411092  15.37705
  3       0.100  0.25  20.31596  0.12688382  15.00919
  3       0.100  0.50  20.40852  0.12744205  15.03388
  3       0.100  1.00  21.13065  0.11497726  15.33301

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.25.


Now processing model 185 of 224 using max all ranger 6 
Random Forest 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 894, 895, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.97227  0.3959186  12.19580
   2    extratrees  17.26643  0.3912436  12.72282
  13    variance    17.03023  0.3798279  11.99528
  13    extratrees  16.78286  0.4073577  11.90786
  24    variance    17.29930  0.3589699  12.21176
  24    extratrees  16.79911  0.4026169  11.86236

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 186 of 224 using same all ranger 6 
Random Forest 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 894, 895, 893, 895, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.87302  0.3862715  12.12684
   2    extratrees  17.19263  0.3762557  12.69822
  13    variance    16.86788  0.3763663  11.88463
  13    extratrees  16.64176  0.4018186  11.83057
  24    variance    17.10160  0.3581787  12.07822
  24    extratrees  16.60339  0.4019964  11.76790

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 187 of 224 using max du ranger 6 
Random Forest 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 895, 893, 893, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.39080  0.3644355  12.58453
   2    extratrees  17.72587  0.3542628  13.14493
  10    variance    17.20410  0.3673904  12.16053
  10    extratrees  17.11408  0.3860523  12.26615
  18    variance    17.36407  0.3545388  12.27529
  18    extratrees  17.07820  0.3848493  12.17377

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 188 of 224 using same du ranger 6 
Random Forest 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 894, 894, 895, 894, 895, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.16596  0.3641610  12.44175
   2    extratrees  17.58226  0.3436081  13.06850
  10    variance    16.95637  0.3697312  12.00998
  10    extratrees  16.90884  0.3833864  12.13484
  18    variance    17.12492  0.3563130  12.11295
  18    extratrees  16.83371  0.3863422  12.01709

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 189 of 224 using max rms ranger 6 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 895, 895, 893, 894, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.97555  0.3826840  11.93752
  2     extratrees  16.96541  0.3946260  12.33750
  3     variance    16.98152  0.3823269  11.82409
  3     extratrees  16.99338  0.3872597  12.24689

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 190 of 224 using same rms ranger 6 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 894, 894, 893, 893, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.00676  0.3647158  12.05119
  2     extratrees  16.95540  0.3781832  12.40799
  3     variance    17.08082  0.3596891  11.98745
  3     extratrees  16.97513  0.3719965  12.31413

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 191 of 224 using max hudgins ranger 6 
Random Forest 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 892, 893, 894, 894, 895, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.06036  0.3081961  13.25230
   2    extratrees  18.39160  0.2922384  13.80021
   7    variance    17.63862  0.3342130  12.69526
   7    extratrees  17.85527  0.3247957  13.06752
  12    variance    17.67412  0.3309214  12.66755
  12    extratrees  17.77887  0.3283840  12.93698

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 192 of 224 using same hudgins ranger 6 
Random Forest 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 892, 893, 895, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.87952  0.3044129  13.13616
   2    extratrees  18.20929  0.2866834  13.70313
   7    variance    17.45710  0.3313280  12.54953
   7    extratrees  17.65305  0.3232623  12.90778
  12    variance    17.50771  0.3263328  12.52439
  12    extratrees  17.55318  0.3292059  12.75856

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 5.


Now processing model 193 of 224 using max all lm 7 
Linear Regression 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 893, 894, 894, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  20.39902  0.10448   16.14143

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 194 of 224 using same all lm 7 
Linear Regression 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 895, 894, 893, 894, 894, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.13565  0.1019841  15.88998

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 195 of 224 using max du lm 7 
Linear Regression 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 895, 895, 893, 895, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.48051  0.09812959  16.15262

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 196 of 224 using same du lm 7 
Linear Regression 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 894, 894, 893, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.19682  0.09621323  15.90177

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 197 of 224 using max rms lm 7 
Linear Regression 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 894, 894, 895, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.84211  0.06607719  16.66977

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 198 of 224 using same rms lm 7 
Linear Regression 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 894, 896, 893, 894, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.54058  0.06273536  16.41776

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 199 of 224 using max hudgins lm 7 
Linear Regression 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 894, 895, 894, 892, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.64815  0.08194918  16.51178

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 200 of 224 using same hudgins lm 7 
Linear Regression 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 894, 893, 894, 894, 893, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  20.35795  0.08050862  16.27899

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 201 of 224 using max all knn 7 
k-Nearest Neighbors 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 893, 894, 894, 895, 895, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.04767  0.3104184  12.32380
  7  18.05552  0.3020600  12.66044
  9  17.88827  0.3109839  12.71852

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 202 of 224 using same all knn 7 
k-Nearest Neighbors 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 896, 894, 893, 893, 893, 895, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.45875  0.3357836  11.93842
  7  17.50716  0.3241692  12.34263
  9  17.46862  0.3241665  12.46835

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 203 of 224 using max du knn 7 
k-Nearest Neighbors 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 893, 893, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.05580  0.3103154  12.27931
  7  18.02001  0.3051239  12.56771
  9  17.84166  0.3143009  12.69776

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 204 of 224 using same du knn 7 
k-Nearest Neighbors 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 893, 893, 894, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.46378  0.3344743  11.93144
  7  17.49859  0.3260048  12.32879
  9  17.42818  0.3273608  12.43051

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 205 of 224 using max rms knn 7 
k-Nearest Neighbors 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 894, 895, 893, 892, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.32497  0.2934083  12.42572
  7  18.22680  0.2909660  12.74361
  9  18.36660  0.2758800  13.07115

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 206 of 224 using same rms knn 7 
k-Nearest Neighbors 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 896, 893, 895, 895, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.63808  0.3233387  12.06942
  7  17.61649  0.3152807  12.47860
  9  17.80674  0.2980234  12.79271

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 207 of 224 using max hudgins knn 7 
k-Nearest Neighbors 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 894, 894, 895, 893, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.44183  0.1477976  14.82077
  7  20.32532  0.1420549  14.77498
  9  20.25248  0.1378819  14.87107

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 208 of 224 using same hudgins knn 7 
k-Nearest Neighbors 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 895, 893, 892, 894, 894, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.76262  0.1743288  14.39915
  7  19.46901  0.1794617  14.30122
  9  19.42464  0.1765688  14.34074

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 209 of 224 using max all svmPoly 7 
Support Vector Machines with Polynomial Kernel 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 893, 895, 894, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.74296  0.05610174  16.38639
  1       0.001  0.50  21.54265  0.05479513  16.31412
  1       0.001  1.00  21.43350  0.05673568  16.22508
  1       0.010  0.25  21.32026  0.06422644  16.10318
  1       0.010  0.50  21.17927  0.07132979  16.00466
  1       0.010  1.00  21.03645  0.07857360  15.91186
  1       0.100  0.25  20.90344  0.08672889  15.80052
  1       0.100  0.50  20.84081  0.09043899  15.73826
  1       0.100  1.00  20.81912  0.09177880  15.69828
  2       0.001  0.25  21.50996  0.05963361  16.27051
  2       0.001  0.50  21.36532  0.06500257  16.14601
  2       0.001  1.00  21.20422  0.07645181  15.99923
  2       0.010  0.25  20.66005  0.10923779  15.55743
  2       0.010  0.50  20.59703  0.11367984  15.44259
  2       0.010  1.00  20.52708  0.11967105  15.32481
  2       0.100  0.25  19.75213  0.17912686  14.40722
  2       0.100  0.50  19.71494  0.18374193  14.33301
  2       0.100  1.00  19.76014  0.18382673  14.32307
  3       0.001  0.25  21.37737  0.06937536  16.14576
  3       0.001  0.50  21.20233  0.08135590  15.98769
  3       0.001  1.00  20.95351  0.09578494  15.83094
  3       0.010  0.25  20.38231  0.13098974  15.15128
  3       0.010  0.50  20.19826  0.14206636  14.95290
  3       0.010  1.00  20.05094  0.15300380  14.76041
  3       0.100  0.25  21.98036  0.12257826  15.42140
  3       0.100  0.50  22.73888  0.10983323  15.72803
  3       0.100  1.00  24.71510  0.09599910  16.45091

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 210 of 224 using same all svmPoly 7 
Support Vector Machines with Polynomial Kernel 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 895, 893, 893, 893, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.48589  0.05302558  16.09415
  1       0.001  0.50  21.31637  0.05167961  16.03687
  1       0.001  1.00  21.19416  0.05464036  15.96274
  1       0.010  0.25  21.08179  0.06058221  15.85067
  1       0.010  0.50  20.97474  0.06648321  15.76388
  1       0.010  1.00  20.88625  0.07240242  15.68742
  1       0.100  0.25  20.81663  0.07768066  15.59967
  1       0.100  0.50  20.80119  0.07920804  15.54034
  1       0.100  1.00  20.78604  0.07982085  15.50729
  2       0.001  0.25  21.28161  0.05688678  15.99458
  2       0.001  0.50  21.12563  0.06302252  15.89001
  2       0.001  1.00  20.97846  0.07347940  15.76457
  2       0.010  0.25  20.33369  0.11272506  15.28621
  2       0.010  0.50  20.22123  0.11884839  15.15531
  2       0.010  1.00  20.14206  0.12497108  15.03182
  2       0.100  0.25  19.52406  0.17579686  14.20577
  2       0.100  0.50  19.53079  0.17896829  14.10302
  2       0.100  1.00  19.64343  0.17740922  14.06836
  3       0.001  0.25  21.13704  0.06812582  15.87634
  3       0.001  0.50  20.96307  0.07967967  15.74386
  3       0.001  1.00  20.79087  0.09145621  15.62043
  3       0.010  0.25  20.04200  0.13173804  14.92204
  3       0.010  0.50  19.89286  0.14355220  14.68862
  3       0.010  1.00  19.81387  0.15181539  14.52020
  3       0.100  0.25  21.19241  0.12404286  15.10640
  3       0.100  0.50  23.01329  0.10283574  15.69954
  3       0.100  1.00  27.42439  0.08118358  16.86573

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.25.


Now processing model 211 of 224 using max du svmPoly 7 
Support Vector Machines with Polynomial Kernel 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 893, 894, 895, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.84150  0.05132960  16.41982
  1       0.001  0.50  21.68191  0.05032566  16.34309
  1       0.001  1.00  21.54740  0.05165948  16.26396
  1       0.010  0.25  21.42007  0.05926369  16.13423
  1       0.010  0.50  21.28765  0.06735087  16.02620
  1       0.010  1.00  21.11365  0.07538778  15.93830
  1       0.100  0.25  20.98123  0.08230655  15.84766
  1       0.100  0.50  20.94472  0.08325631  15.82972
  1       0.100  1.00  20.95544  0.08154399  15.82622
  2       0.001  0.25  21.66636  0.05247978  16.32279
  2       0.001  0.50  21.52227  0.05498211  16.23065
  2       0.001  1.00  21.38767  0.06327429  16.09683
  2       0.010  0.25  20.87366  0.09685807  15.76309
  2       0.010  0.50  20.71736  0.10238246  15.63817
  2       0.010  1.00  20.67834  0.10675592  15.55150
  2       0.100  0.25  20.08499  0.15206147  14.93835
  2       0.100  0.50  19.89535  0.16717420  14.69556
  2       0.100  1.00  19.87764  0.17013823  14.64065
  3       0.001  0.25  21.56117  0.05614431  16.24646
  3       0.001  0.50  21.40467  0.06432341  16.10703
  3       0.001  1.00  21.21814  0.07707657  15.96545
  3       0.010  0.25  20.74368  0.10911647  15.56000
  3       0.010  0.50  20.58313  0.11916283  15.37700
  3       0.010  1.00  20.42788  0.12839903  15.23383
  3       0.100  0.25  22.00384  0.11968163  15.52736
  3       0.100  0.50  22.90880  0.10614096  15.91761
  3       0.100  1.00  23.82029  0.09611240  16.25958

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 212 of 224 using same du svmPoly 7 
Support Vector Machines with Polynomial Kernel 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 894, 893, 895, 893, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.53407  0.05195004  16.10857
  1       0.001  0.50  21.40961  0.04936465  16.05192
  1       0.001  1.00  21.28305  0.05082823  15.98807
  1       0.010  0.25  21.14002  0.05832059  15.87568
  1       0.010  0.50  21.02776  0.06633909  15.77235
  1       0.010  1.00  20.88760  0.07406724  15.69202
  1       0.100  0.25  20.77158  0.08101441  15.60648
  1       0.100  0.50  20.79048  0.08009054  15.58356
  1       0.100  1.00  20.83898  0.07741804  15.56244
  2       0.001  0.25  21.39532  0.05120239  16.03536
  2       0.001  0.50  21.25917  0.05442196  15.95448
  2       0.001  1.00  21.11424  0.06266832  15.84319
  2       0.010  0.25  20.56250  0.10082421  15.47788
  2       0.010  0.50  20.35879  0.10760462  15.35994
  2       0.010  1.00  20.24128  0.11384309  15.25588
  2       0.100  0.25  19.63033  0.16436680  14.54689
  2       0.100  0.50  19.55484  0.17105437  14.41730
  2       0.100  1.00  19.57978  0.17175995  14.34849
  3       0.001  0.25  21.29037  0.05532412  15.96522
  3       0.001  0.50  21.13681  0.06376720  15.84939
  3       0.001  1.00  20.97351  0.07567197  15.73033
  3       0.010  0.25  20.30554  0.11398691  15.26362
  3       0.010  0.50  20.12973  0.12368043  15.11152
  3       0.010  1.00  19.99041  0.13345496  14.95729
  3       0.100  0.25  20.93478  0.12502098  15.12866
  3       0.100  0.50  21.51840  0.11587144  15.40109
  3       0.100  1.00  22.98704  0.10400111  15.98043

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 213 of 224 using max rms svmPoly 7 
Support Vector Machines with Polynomial Kernel 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 893, 896, 894, 895, 892, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.04508  0.06002904  16.59751
  1       0.001  0.50  22.01156  0.06186958  16.53374
  1       0.001  1.00  21.82435  0.06114053  16.47164
  1       0.010  0.25  21.54828  0.05984651  16.42456
  1       0.010  0.50  21.34901  0.05845217  16.40837
  1       0.010  1.00  21.26173  0.05712617  16.38651
  1       0.100  0.25  21.22025  0.05628550  16.37692
  1       0.100  0.50  21.21591  0.05578104  16.37960
  1       0.100  1.00  21.21666  0.05546696  16.38430
  2       0.001  0.25  22.01065  0.06227782  16.53226
  2       0.001  0.50  21.82117  0.06170327  16.46861
  2       0.001  1.00  21.63655  0.06080397  16.42811
  2       0.010  0.25  21.26584  0.07335924  16.28240
  2       0.010  0.50  21.10570  0.08222494  16.15755
  2       0.010  1.00  20.92335  0.10011025  15.97396
  2       0.100  0.25  20.63652  0.11500034  15.28200
  2       0.100  0.50  20.66976  0.11460130  15.25768
  2       0.100  1.00  20.70120  0.11398625  15.26292
  3       0.001  0.25  21.91797  0.06269932  16.49849
  3       0.001  0.50  21.72916  0.06186597  16.43794
  3       0.001  1.00  21.46261  0.06201773  16.39905
  3       0.010  0.25  21.06809  0.09764074  16.05126
  3       0.010  0.50  20.84067  0.11262539  15.83350
  3       0.010  1.00  20.65134  0.11688977  15.61242
  3       0.100  0.25  20.29535  0.14299498  14.76017
  3       0.100  0.50  20.24593  0.14812207  14.65455
  3       0.100  1.00  20.24634  0.14820881  14.63032

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 214 of 224 using same rms svmPoly 7 
Support Vector Machines with Polynomial Kernel 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 895, 894, 894, 895, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.86151  0.05992698  16.27585
  1       0.001  0.50  21.71046  0.06074167  16.22021
  1       0.001  1.00  21.63049  0.06098687  16.17520
  1       0.010  0.25  21.36577  0.05974978  16.13934
  1       0.010  0.50  21.17881  0.05892020  16.13780
  1       0.010  1.00  21.04034  0.05791245  16.13433
  1       0.100  0.25  20.96005  0.05715350  16.11427
  1       0.100  0.50  20.94431  0.05661839  16.11320
  1       0.100  1.00  20.93545  0.05667170  16.11089
  2       0.001  0.25  21.70981  0.06121503  16.21862
  2       0.001  0.50  21.62818  0.06156694  16.17258
  2       0.001  1.00  21.41010  0.06085210  16.14023
  2       0.010  0.25  21.07542  0.07503863  16.01001
  2       0.010  0.50  20.88342  0.08404468  15.89477
  2       0.010  1.00  20.70974  0.10225893  15.70212
  2       0.100  0.25  20.38387  0.11491163  14.97032
  2       0.100  0.50  20.41973  0.11386931  14.96133
  2       0.100  1.00  20.44089  0.11338063  14.95905
  3       0.001  0.25  21.67244  0.06210852  16.18773
  3       0.001  0.50  21.48911  0.06179461  16.14975
  3       0.001  1.00  21.30417  0.06186573  16.12256
  3       0.010  0.25  20.83285  0.09879364  15.77643
  3       0.010  0.50  20.63044  0.11225545  15.57358
  3       0.010  1.00  20.42190  0.11329711  15.37664
  3       0.100  0.25  20.02028  0.14253437  14.47795
  3       0.100  0.50  19.96407  0.14801795  14.37130
  3       0.100  1.00  19.99275  0.14590915  14.35886

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 215 of 224 using max hudgins svmPoly 7 
Support Vector Machines with Polynomial Kernel 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 894, 894, 892, 895, 894, 892, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.92114  0.04728316  16.51071
  1       0.001  0.50  21.82342  0.04555299  16.43689
  1       0.001  1.00  21.70783  0.04371333  16.37612
  1       0.010  0.25  21.61277  0.04422877  16.30468
  1       0.010  0.50  21.57397  0.04673411  16.24738
  1       0.010  1.00  21.49491  0.05205196  16.19771
  1       0.100  0.25  21.35366  0.05900017  16.14965
  1       0.100  0.50  21.23797  0.06404331  16.11579
  1       0.100  1.00  21.19135  0.06690310  16.08409
  2       0.001  0.25  21.81830  0.04619120  16.42910
  2       0.001  0.50  21.69807  0.04502267  16.36257
  2       0.001  1.00  21.60274  0.04541639  16.29269
  2       0.010  0.25  21.21665  0.08106854  15.96041
  2       0.010  0.50  20.98505  0.08774156  15.86438
  2       0.010  1.00  20.84597  0.09197580  15.76305
  2       0.100  0.25  20.85128  0.10484969  15.57347
  2       0.100  0.50  20.79024  0.11233943  15.48195
  2       0.100  1.00  20.63272  0.12425688  15.33152
  3       0.001  0.25  21.73239  0.04581857  16.37533
  3       0.001  0.50  21.62325  0.04637890  16.30429
  3       0.001  1.00  21.53513  0.05171847  16.20575
  3       0.010  0.25  21.00509  0.08896422  15.85235
  3       0.010  0.50  20.86056  0.09466470  15.72061
  3       0.010  1.00  20.80129  0.10084360  15.61491
  3       0.100  0.25  20.89071  0.12012694  15.30974
  3       0.100  0.50  21.74343  0.10754094  15.61736
  3       0.100  1.00  22.93952  0.09442343  16.00484

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 216 of 224 using same hudgins svmPoly 7 
Support Vector Machines with Polynomial Kernel 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 893, 893, 894, 893, 894, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  21.58584  0.04564319  16.18645
  1       0.001  0.50  21.54244  0.04321804  16.12502
  1       0.001  1.00  21.44157  0.03958577  16.08618
  1       0.010  0.25  21.37401  0.03871746  16.03830
  1       0.010  0.50  21.34167  0.04097091  15.99202
  1       0.010  1.00  21.30327  0.04452982  15.95826
  1       0.100  0.25  21.20377  0.04961277  15.93541
  1       0.100  0.50  21.11990  0.05317854  15.93648
  1       0.100  1.00  21.05974  0.05646565  15.93344
  2       0.001  0.25  21.53950  0.04382679  16.11898
  2       0.001  0.50  21.43262  0.04075089  16.07437
  2       0.001  1.00  21.36360  0.04077586  16.02461
  2       0.010  0.25  20.98671  0.07552047  15.68831
  2       0.010  0.50  20.76534  0.08527360  15.57825
  2       0.010  1.00  20.51533  0.09551001  15.47695
  2       0.100  0.25  20.23783  0.12038553  15.14849
  2       0.100  0.50  20.16084  0.12857028  15.02585
  2       0.100  1.00  20.05907  0.13730555  14.87435
  3       0.001  0.25  21.47849  0.04260797  16.08307
  3       0.001  0.50  21.37619  0.04192948  16.03274
  3       0.001  1.00  21.30267  0.04643061  15.95260
  3       0.010  0.25  20.70021  0.09134051  15.53004
  3       0.010  0.50  20.46249  0.10024864  15.41858
  3       0.010  1.00  20.35682  0.10640315  15.32499
  3       0.100  0.25  20.36170  0.12911227  14.97527
  3       0.100  0.50  20.71250  0.12145597  15.13538
  3       0.100  1.00  21.21902  0.11157266  15.37119

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 217 of 224 using max all ranger 7 
Random Forest 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 894, 893, 894, 893, 895, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.15824  0.3741741  12.28079
   2    extratrees  17.35579  0.3749620  12.70645
  13    variance    17.31898  0.3542938  12.13616
  13    extratrees  16.96575  0.3868660  11.95280
  24    variance    17.51994  0.3400044  12.30204
  24    extratrees  17.02342  0.3797335  11.95285

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 218 of 224 using same all ranger 7 
Random Forest 

993 samples
 24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 893, 895, 895, 895, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.56230  0.4059148  11.89767
   2    extratrees  16.83827  0.4035979  12.39352
  13    variance    16.73820  0.3805470  11.80438
  13    extratrees  16.24760  0.4274506  11.53416
  24    variance    17.00681  0.3602291  11.99280
  24    extratrees  16.25163  0.4234084  11.48732

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 219 of 224 using max du ranger 7 
Random Forest 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 893, 893, 893, 895, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.47404  0.3503331  12.59331
   2    extratrees  17.76522  0.3405415  13.08164
  10    variance    17.39268  0.3491725  12.19463
  10    extratrees  17.28270  0.3641931  12.28010
  18    variance    17.53571  0.3390337  12.29305
  18    extratrees  17.26181  0.3631580  12.19670

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 220 of 224 using same du ranger 7 
Random Forest 

993 samples
 18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 894, 896, 894, 893, 892, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.94744  0.3749970  12.25998
   2    extratrees  17.26930  0.3648794  12.78494
  10    variance    16.86803  0.3696049  11.92665
  10    extratrees  16.58060  0.4028979  11.85956
  18    variance    17.04112  0.3564052  12.04799
  18    extratrees  16.50624  0.4055689  11.73938

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 221 of 224 using max rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 895, 892, 894, 893, 893, 893, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.26543  0.3591227  12.18156
  2     extratrees  17.24378  0.3668036  12.46824
  3     variance    17.26705  0.3595501  12.06712
  3     extratrees  17.29211  0.3589630  12.40515

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 2, splitrule = extratrees
 and min.node.size = 5.


Now processing model 222 of 224 using same rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

993 samples
  3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 893, 895, 892, 896, 892, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.72434  0.3795961  11.91658
  2     extratrees  16.66048  0.3965224  12.14237
  3     variance    16.77246  0.3756589  11.85033
  3     extratrees  16.65936  0.3909996  12.04686

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 223 of 224 using max hudgins ranger 7 
Random Forest 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 892, 894, 894, 894, 894, 894, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.11681  0.2981961  13.23402
   2    extratrees  18.36843  0.2879550  13.68766
   7    variance    17.82336  0.3170069  12.71266
   7    extratrees  17.91111  0.3154693  12.96144
  12    variance    17.89920  0.3120657  12.69278
  12    extratrees  17.81574  0.3210273  12.79973

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 224 of 224 using same hudgins ranger 7 
Random Forest 

993 samples
 12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 893, 893, 895, 894, 893, 895, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.67476  0.3146834  12.96464
   2    extratrees  17.92023  0.3059261  13.44422
   7    variance    17.35655  0.3345772  12.45801
   7    extratrees  17.31378  0.3447741  12.60654
  12    variance    17.45063  0.3280109  12.49271
  12    extratrees  17.19815  0.3514467  12.42887

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.
