Processing  44 subjects



Now processing model 1 of 224 using max all lm 1 
Linear Regression 

8593 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7733, 7733, 7735, 7734, 7733, 7732, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.04929  0.1247942  17.39384

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 2 of 224 using same all lm 1 
Linear Regression 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1104, 1106, 1106, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.37964  0.1161937  17.54877

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 3 of 224 using max du lm 1 
Linear Regression 

8593 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7733, 7733, 7734, 7734, 7734, 7733, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.32879  0.1014461  17.56538

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 4 of 224 using same du lm 1 
Linear Regression 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1107, 1106, 1106, 1105, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.63877  0.09727348  17.77148

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 5 of 224 using max rms lm 1 
Linear Regression 

8593 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7732, 7733, 7734, 7735, 7736, 7734, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  22.03505  0.04126242  18.30828

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 6 of 224 using same rms lm 1 
Linear Regression 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1106, 1105, 1105, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  22.19033  0.05129223  18.42306

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 7 of 224 using max hudgins lm 1 
Linear Regression 

8593 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7733, 7733, 7734, 7733, 7734, 7733, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.47375  0.08925143  17.82505

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 8 of 224 using same hudgins lm 1 
Linear Regression 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1105, 1105, 1105, 1106, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.76325  0.08630415  18.05799

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 9 of 224 using max all knn 1 
k-Nearest Neighbors 

8593 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7735, 7736, 7734, 7733, 7734, 7733, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.64433  0.3311061  12.80562
  7  18.32832  0.3439025  12.85171
  9  18.23774  0.3466392  12.96026

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 10 of 224 using same all knn 1 
k-Nearest Neighbors 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1105, 1105, 1104, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.03788  0.2473258  14.83075
  7  19.99471  0.2394540  15.03194
  9  19.95579  0.2370987  15.20199

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 11 of 224 using max du knn 1 
k-Nearest Neighbors 

8593 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7735, 7734, 7732, 7733, 7735, 7733, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.62645  0.3323015  12.79660
  7  18.32368  0.3443438  12.84696
  9  18.22597  0.3475387  12.94757

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 12 of 224 using same du knn 1 
k-Nearest Neighbors 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1105, 1106, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.97680  0.2511593  14.79151
  7  19.94924  0.2431706  15.01165
  9  19.94543  0.2385179  15.16823

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 13 of 224 using max rms knn 1 
k-Nearest Neighbors 

8593 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7734, 7734, 7735, 7734, 7732, 7734, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.29725  0.3543249  12.58483
  7  18.00373  0.3662775  12.59298
  9  17.90022  0.3700787  12.68471

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 14 of 224 using same rms knn 1 
k-Nearest Neighbors 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.22582  0.2384480  14.78784
  7  19.97814  0.2430610  14.82795
  9  19.83958  0.2453223  14.93796

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 15 of 224 using max hudgins knn 1 
k-Nearest Neighbors 

8593 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7733, 7734, 7735, 7735, 7733, 7734, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  22.25404  0.0933727  17.03602
  7  21.80998  0.1012512  16.88272
  9  21.53926  0.1087991  16.77959

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 16 of 224 using same hudgins knn 1 
k-Nearest Neighbors 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1106, 1105, 1105, 1104, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared    MAE     
  5  22.85412  0.06743482  18.04259
  7  22.39306  0.07471424  17.77155
  9  22.29592  0.07311654  17.73404

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 17 of 224 using max all svmPoly 1 
Support Vector Machines with Polynomial Kernel 

8593 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7734, 7733, 7734, 7734, 7733, 7734, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.79098  0.04853705  17.61543
  1       0.001  0.50  22.56955  0.05974635  17.50684
  1       0.001  1.00  22.30952  0.07260050  17.40271
  1       0.010  0.25  22.04458  0.08530770  17.30336
  1       0.010  0.50  21.88950  0.09321560  17.23929
  1       0.010  1.00  21.78741  0.09916624  17.17811
  1       0.100  0.25  21.70653  0.10433520  17.12118
  1       0.100  0.50  21.64305  0.10786173  17.09211
  1       0.100  1.00  21.60027  0.11008652  17.07773
  2       0.001  0.25  22.28503  0.07727433  17.35694
  2       0.001  0.50  21.93161  0.09683757  17.18081
  2       0.001  1.00  21.57933  0.11541054  16.99450
  2       0.010  0.25  20.80714  0.16504892  16.39449
  2       0.010  0.50  20.61545  0.17638129  16.19498
  2       0.010  1.00  20.49898  0.18270732  16.03941
  2       0.100  0.25  20.14650  0.20918342  15.48330
  2       0.100  0.50  20.05585  0.21593510  15.36352
  2       0.100  1.00  19.98314  0.22117114  15.26808
  3       0.001  0.25  21.90367  0.09994222  17.15144
  3       0.001  0.50  21.53313  0.11872340  16.95433
  3       0.001  1.00  21.23146  0.13473669  16.77333
  3       0.010  0.25  20.49382  0.18425505  15.93735
  3       0.010  0.50  20.35423  0.19325173  15.77252
  3       0.010  1.00  20.22297  0.20235897  15.59871
  3       0.100  0.25  19.84906  0.23698699  14.84531
  3       0.100  0.50  19.93382  0.23628194  14.80208
  3       0.100  1.00  20.03154  0.23541319  14.79212

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.25.


Now processing model 18 of 224 using same all svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1105, 1105, 1104, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.26334  0.04401961  17.95252
  1       0.001  0.50  23.15004  0.04702719  17.84989
  1       0.001  1.00  23.03717  0.05203490  17.76998
  1       0.010  0.25  22.79420  0.06341895  17.64262
  1       0.010  0.50  22.56348  0.07255722  17.57106
  1       0.010  1.00  22.33033  0.08259520  17.48331
  1       0.100  0.25  22.14137  0.09012188  17.45186
  1       0.100  0.50  22.07190  0.09349760  17.45036
  1       0.100  1.00  22.02807  0.09597986  17.44550
  2       0.001  0.25  23.11413  0.05024084  17.81622
  2       0.001  0.50  22.95918  0.05767928  17.70219
  2       0.001  1.00  22.65437  0.07010044  17.57996
  2       0.010  0.25  21.69426  0.12614499  17.07645
  2       0.010  0.50  21.47339  0.13672176  16.94037
  2       0.010  1.00  21.22273  0.15028681  16.77089
  2       0.100  0.25  20.88774  0.17270424  16.30034
  2       0.100  0.50  20.85318  0.17625664  16.25519
  2       0.100  1.00  20.82313  0.18018367  16.20867
  3       0.001  0.25  22.99248  0.05828435  17.70924
  3       0.001  0.50  22.68322  0.07074867  17.57844
  3       0.001  1.00  22.33788  0.08752410  17.44003
  3       0.010  0.25  21.24296  0.15372969  16.62887
  3       0.010  0.50  21.01212  0.16613633  16.41368
  3       0.010  1.00  20.90150  0.17158857  16.30010
  3       0.100  0.25  24.53347  0.11498524  17.46074
  3       0.100  0.50  27.17654  0.10267331  18.14232
  3       0.100  1.00  31.07596  0.09337693  19.02207

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 19 of 224 using max du svmPoly 1 
Support Vector Machines with Polynomial Kernel 

8593 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7734, 7733, 7734, 7733, 7733, 7735, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.95717  0.04117878  17.66197
  1       0.001  0.50  22.72690  0.05356420  17.54172
  1       0.001  1.00  22.44106  0.06770056  17.42869
  1       0.010  0.25  22.13212  0.08185732  17.32057
  1       0.010  0.50  21.96869  0.08808215  17.26974
  1       0.010  1.00  21.87346  0.09102430  17.23071
  1       0.100  0.25  21.84552  0.09088086  17.21482
  1       0.100  0.50  21.85037  0.08999938  17.21582
  1       0.100  1.00  21.85751  0.08942834  17.21617
  2       0.001  0.25  22.57891  0.06299433  17.46047
  2       0.001  0.50  22.23602  0.08201005  17.30283
  2       0.001  1.00  21.89524  0.10071806  17.13810
  2       0.010  0.25  20.99865  0.14994305  16.61658
  2       0.010  0.50  20.77635  0.16444862  16.44216
  2       0.010  1.00  20.62488  0.17396812  16.29161
  2       0.100  0.25  20.36789  0.19185514  15.90003
  2       0.100  0.50  20.30240  0.19682171  15.79549
  2       0.100  1.00  20.22480  0.20261363  15.67596
  3       0.001  0.25  22.24970  0.08296864  17.29498
  3       0.001  0.50  21.88272  0.10263082  17.11096
  3       0.001  1.00  21.54320  0.11959035  16.93716
  3       0.010  0.25  20.66585  0.17457662  16.19949
  3       0.010  0.50  20.51558  0.18246910  16.06218
  3       0.010  1.00  20.38986  0.19012861  15.92014
  3       0.100  0.25  20.08329  0.21598598  15.30738
  3       0.100  0.50  20.08815  0.21944595  15.22104
  3       0.100  1.00  20.16557  0.21942647  15.18549

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.25.


Now processing model 20 of 224 using same du svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.36382  0.04195395  18.02690
  1       0.001  0.50  23.26951  0.04400561  17.90913
  1       0.001  1.00  23.17755  0.04965262  17.81044
  1       0.010  0.25  22.96995  0.05943746  17.69033
  1       0.010  0.50  22.71544  0.07058109  17.59078
  1       0.010  1.00  22.44299  0.08010480  17.51776
  1       0.100  0.25  22.22476  0.08688563  17.48197
  1       0.100  0.50  22.18276  0.08719238  17.50786
  1       0.100  1.00  22.18627  0.08524013  17.54930
  2       0.001  0.25  23.25812  0.04538876  17.89456
  2       0.001  0.50  23.14188  0.05211747  17.78137
  2       0.001  1.00  22.94420  0.06143991  17.67539
  2       0.010  0.25  22.05225  0.11083184  17.20381
  2       0.010  0.50  21.73578  0.12668311  17.02581
  2       0.010  1.00  21.50561  0.13750113  16.92543
  2       0.100  0.25  21.05907  0.15959799  16.53446
  2       0.100  0.50  21.06779  0.15963759  16.53239
  2       0.100  1.00  21.12158  0.15820293  16.56694
  3       0.001  0.25  23.16950  0.05112955  17.80505
  3       0.001  0.50  22.99075  0.06035416  17.69104
  3       0.001  1.00  22.65681  0.07540002  17.53844
  3       0.010  0.25  21.69140  0.13280088  16.93063
  3       0.010  0.50  21.34922  0.14917145  16.74197
  3       0.010  1.00  21.04876  0.16269068  16.54483
  3       0.100  0.25  24.08895  0.11489603  17.52272
  3       0.100  0.50  26.12697  0.10472989  18.07578
  3       0.100  1.00  29.14544  0.09441549  18.81639

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01 and C = 1.


Now processing model 21 of 224 using max rms svmPoly 1 
Support Vector Machines with Polynomial Kernel 

8593 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7734, 7734, 7735, 7733, 7735, 7734, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.05515  0.03886717  17.89655
  1       0.001  0.50  22.90210  0.03892477  17.80991
  1       0.001  1.00  22.84502  0.03915840  17.77413
  1       0.010  0.25  22.81649  0.03927547  17.76528
  1       0.010  0.50  22.81068  0.03926408  17.76455
  1       0.010  1.00  22.80557  0.03924458  17.76364
  1       0.100  0.25  22.80312  0.03923222  17.76327
  1       0.100  0.50  22.80215  0.03923305  17.76311
  1       0.100  1.00  22.80144  0.03923225  17.76300
  2       0.001  0.25  22.89535  0.03955058  17.80242
  2       0.001  0.50  22.83134  0.04033661  17.75950
  2       0.001  1.00  22.79280  0.04153748  17.73831
  2       0.010  0.25  22.19738  0.07488810  17.39805
  2       0.010  0.50  21.90616  0.08739350  17.26131
  2       0.010  1.00  21.70317  0.09984237  17.13580
  2       0.100  0.25  21.50228  0.11255620  16.97512
  2       0.100  0.50  21.49235  0.11285556  16.97544
  2       0.100  1.00  21.48804  0.11302909  16.97573
  3       0.001  0.25  22.83940  0.04077856  17.76031
  3       0.001  0.50  22.79188  0.04272171  17.72816
  3       0.001  1.00  22.72420  0.04588350  17.68450
  3       0.010  0.25  21.76184  0.09641375  17.16685
  3       0.010  0.50  21.60748  0.10755377  17.06025
  3       0.010  1.00  21.50980  0.11456381  16.96593
  3       0.100  0.25  20.96627  0.15382950  16.10105
  3       0.100  0.50  20.96063  0.15421468  16.08473
  3       0.100  1.00  20.96413  0.15402543  16.07997

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 22 of 224 using same rms svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1106, 1104, 1105, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.72228  0.04281653  18.30886
  1       0.001  0.50  23.54858  0.04822676  18.22650
  1       0.001  1.00  23.39054  0.05208599  18.10239
  1       0.010  0.25  23.07610  0.05198128  17.93464
  1       0.010  0.50  22.97135  0.05238814  17.86524
  1       0.010  1.00  22.91384  0.05267293  17.82807
  1       0.100  0.25  22.90657  0.05270316  17.82382
  1       0.100  0.50  22.90845  0.05270283  17.82477
  1       0.100  1.00  22.90822  0.05266622  17.82526
  2       0.001  0.25  23.54760  0.04854014  18.22488
  2       0.001  0.50  23.38993  0.05246552  18.09950
  2       0.001  1.00  23.13712  0.05250888  17.96585
  2       0.010  0.25  22.88597  0.05980968  17.76070
  2       0.010  0.50  22.75753  0.06502846  17.68205
  2       0.010  1.00  22.54068  0.07332034  17.58713
  2       0.100  0.25  21.80430  0.11236854  17.24426
  2       0.100  0.50  21.75651  0.11500608  17.23607
  2       0.100  1.00  21.73107  0.11594225  17.24118
  3       0.001  0.25  23.48077  0.05137094  18.15066
  3       0.001  0.50  23.24424  0.05291167  18.02195
  3       0.001  1.00  23.01717  0.05325152  17.89050
  3       0.010  0.25  22.68619  0.06995586  17.63704
  3       0.010  0.50  22.44117  0.07959270  17.53663
  3       0.010  1.00  22.21792  0.09005815  17.43458
  3       0.100  0.25  21.27223  0.15076770  16.41182
  3       0.100  0.50  21.22754  0.15281197  16.30100
  3       0.100  1.00  21.18310  0.15618109  16.22674

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 23 of 224 using max hudgins svmPoly 1 
Support Vector Machines with Polynomial Kernel 

8593 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7734, 7733, 7733, 7734, 7734, 7734, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.11710  0.02970925  17.81791
  1       0.001  0.50  22.95923  0.03963599  17.72431
  1       0.001  1.00  22.75037  0.05213603  17.61032
  1       0.010  0.25  22.43334  0.06690533  17.47721
  1       0.010  0.50  22.25002  0.07388057  17.41556
  1       0.010  1.00  22.14496  0.07732266  17.38757
  1       0.100  0.25  22.11223  0.07868116  17.37596
  1       0.100  0.50  22.10530  0.07876697  17.37419
  1       0.100  1.00  22.10485  0.07866755  17.37486
  2       0.001  0.25  22.91290  0.04310054  17.68438
  2       0.001  0.50  22.64508  0.05846739  17.54017
  2       0.001  1.00  22.32894  0.07489941  17.39429
  2       0.010  0.25  21.54089  0.11533173  16.97297
  2       0.010  0.50  21.39077  0.12395333  16.87723
  2       0.010  1.00  21.23418  0.13302456  16.77051
  2       0.100  0.25  20.86143  0.15662706  16.35802
  2       0.100  0.50  20.84258  0.15819442  16.29775
  2       0.100  1.00  20.84296  0.15840654  16.24803
  3       0.001  0.25  22.69967  0.05656545  17.55721
  3       0.001  0.50  22.33908  0.07557193  17.38983
  3       0.001  1.00  22.02434  0.09089675  17.23319
  3       0.010  0.25  21.21005  0.13917359  16.65647
  3       0.010  0.50  21.02658  0.14896120  16.47826
  3       0.010  1.00  20.89135  0.15529145  16.35024
  3       0.100  0.25  20.53628  0.18329964  15.70310
  3       0.100  0.50  20.50424  0.18778270  15.60906
  3       0.100  1.00  20.49245  0.19054844  15.55532

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 24 of 224 using same hudgins svmPoly 1 
Support Vector Machines with Polynomial Kernel 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1106, 1104, 1105, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.50108  0.03108510  18.18203
  1       0.001  0.50  23.37818  0.03365893  18.06229
  1       0.001  1.00  23.32514  0.03581794  17.98216
  1       0.010  0.25  23.21906  0.04409198  17.87871
  1       0.010  0.50  23.05379  0.05272140  17.80318
  1       0.010  1.00  22.78868  0.06475044  17.69300
  1       0.100  0.25  22.49880  0.07398680  17.60169
  1       0.100  0.50  22.39853  0.07601542  17.59471
  1       0.100  1.00  22.37874  0.07602035  17.60521
  2       0.001  0.25  23.37710  0.03411684  18.05758
  2       0.001  0.50  23.31566  0.03684407  17.97045
  2       0.001  1.00  23.22363  0.04334958  17.88132
  2       0.010  0.25  22.60208  0.07877894  17.54326
  2       0.010  0.50  22.26173  0.09187232  17.38077
  2       0.010  1.00  21.97677  0.10559119  17.23851
  2       0.100  0.25  21.70751  0.12104649  17.05750
  2       0.100  0.50  21.62777  0.12862139  16.97750
  2       0.100  1.00  21.56520  0.13456183  16.92841
  3       0.001  0.25  23.33963  0.03599517  17.99973
  3       0.001  0.50  23.25037  0.04165555  17.90288
  3       0.001  1.00  23.09831  0.05137546  17.79129
  3       0.010  0.25  22.25164  0.09787594  17.31336
  3       0.010  0.50  21.96235  0.10925647  17.17257
  3       0.010  1.00  21.74152  0.11946539  17.06158
  3       0.100  0.25  22.02605  0.13180413  16.92605
  3       0.100  0.50  22.58220  0.12443837  17.20707
  3       0.100  1.00  23.51962  0.11685134  17.56175

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 25 of 224 using max all ranger 1 
Random Forest 

8593 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7735, 7733, 7735, 7733, 7734, 7733, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.75292  0.4563320  12.17878
   2    extratrees  17.29200  0.4355132  13.07766
  13    variance    16.39210  0.4708796  11.39761
  13    extratrees  16.29237  0.4841117  11.64340
  24    variance    16.53893  0.4606274  11.50460
  24    extratrees  16.23846  0.4856887  11.52146

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 26 of 224 using same all ranger 1 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1105, 1105, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.77795  0.3234421  14.54538
   2    extratrees  19.06023  0.3161850  15.02991
  13    variance    18.75637  0.3196901  14.36182
  13    extratrees  18.62978  0.3328627  14.32047
  24    variance    18.92022  0.3078824  14.51822
  24    extratrees  18.62701  0.3318755  14.29469

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 27 of 224 using max du ranger 1 
Random Forest 

8593 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7733, 7732, 7734, 7733, 7735, 7733, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.27675  0.4244193  12.85770
   2    extratrees  18.01309  0.3853026  13.85666
  10    variance    16.44676  0.4682114  11.53475
  10    extratrees  16.67167  0.4630134  12.15915
  18    variance    16.52480  0.4615475  11.50156
  18    extratrees  16.51543  0.4707912  11.92810

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 28 of 224 using same du ranger 1 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1104, 1105, 1105, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    19.02938  0.3059960  14.85082
   2    extratrees  19.43327  0.2872491  15.42536
  10    variance    18.82000  0.3148526  14.42673
  10    extratrees  18.91548  0.3122998  14.62931
  18    variance    18.87243  0.3108156  14.47123
  18    extratrees  18.84614  0.3161603  14.53450

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 29 of 224 using max rms ranger 1 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

8593 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7733, 7733, 7733, 7734, 7735, 7735, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.29096  0.4099848  12.05215
  2     extratrees  17.13049  0.4235191  12.44845
  3     variance    17.40480  0.4035978  12.03209
  3     extratrees  17.08907  0.4242094  12.26894

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 30 of 224 using same rms ranger 1 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1106, 1104, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    19.04006  0.3003565  14.28732
  2     extratrees  18.90899  0.3105571  14.56083
  3     variance    19.13116  0.2959279  14.26320
  3     extratrees  18.88842  0.3099409  14.42484

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 31 of 224 using max hudgins ranger 1 
Random Forest 

8593 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 7734, 7733, 7733, 7733, 7734, 7733, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.18476  0.3603432  13.87004
   2    extratrees  18.80783  0.3252217  14.70618
   7    variance    17.25083  0.4136629  12.40338
   7    extratrees  17.53621  0.4036839  13.08049
  12    variance    17.24275  0.4127444  12.23548
  12    extratrees  17.27274  0.4182819  12.71279

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 32 of 224 using same hudgins ranger 1 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    19.70880  0.2528768  15.60604
   2    extratrees  20.02972  0.2381559  16.07698
   7    variance    19.35895  0.2748212  15.04708
   7    extratrees  19.55490  0.2633938  15.36389
  12    variance    19.32651  0.2773508  14.92829
  12    extratrees  19.44881  0.2702052  15.22469

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 33 of 224 using max all lm 2 
Linear Regression 

3683 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3316, 3316, 3313, 3315, 3315, 3314, ... 
Resampling results:

  RMSE      Rsquared   MAE    
  20.81326  0.1390757  17.1127

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 34 of 224 using same all lm 2 
Linear Regression 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1106, 1104, 1104, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.08338  0.1281212  17.26424

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 35 of 224 using max du lm 2 
Linear Regression 

3683 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3314, 3316, 3316, 3314, 3315, 3314, ... 
Resampling results:

  RMSE      Rsquared   MAE    
  21.15913  0.1111884  17.3291

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 36 of 224 using same du lm 2 
Linear Regression 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1104, 1106, 1105, 1106, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.37638  0.1046092  17.48688

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 37 of 224 using max rms lm 2 
Linear Regression 

3683 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3317, 3315, 3314, 3313, 3316, 3315, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.92944  0.04436606  18.19102

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 38 of 224 using same rms lm 2 
Linear Regression 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1106, 1105, 1105, 1105, 1105, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  22.15197  0.03642628  18.38847

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 39 of 224 using max hudgins lm 2 
Linear Regression 

3683 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3316, 3313, 3315, 3314, 3314, 3314, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.34955  0.09379036  17.64992

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 40 of 224 using same hudgins lm 2 
Linear Regression 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1105, 1106, 1105, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.58946  0.08487438  17.87282

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 41 of 224 using max all knn 2 
k-Nearest Neighbors 

3683 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3314, 3315, 3316, 3315, 3313, 3314, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.54725  0.3963472  11.79315
  7  17.44780  0.3975886  12.00585
  9  17.45534  0.3947465  12.16950

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 42 of 224 using same all knn 2 
k-Nearest Neighbors 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1103, 1106, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.19149  0.2898118  13.80928
  7  18.92906  0.2998994  13.86780
  9  19.01048  0.2913737  14.09418

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 43 of 224 using max du knn 2 
k-Nearest Neighbors 

3683 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3314, 3314, 3315, 3313, 3314, 3315, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.59454  0.3938697  11.81122
  7  17.46519  0.3969650  12.00698
  9  17.48131  0.3936104  12.17188

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 44 of 224 using same du knn 2 
k-Nearest Neighbors 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1105, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  19.15007  0.2949725  13.72966
  7  18.92559  0.3023172  13.81064
  9  18.99137  0.2944610  14.04647

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 45 of 224 using max rms knn 2 
k-Nearest Neighbors 

3683 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3314, 3314, 3315, 3315, 3315, 3316, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.88625  0.3754383  12.20418
  7  17.70882  0.3807500  12.28496
  9  17.60631  0.3849501  12.34971

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 46 of 224 using same rms knn 2 
k-Nearest Neighbors 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1106, 1106, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.99405  0.3083971  13.82298
  7  19.13659  0.2900400  14.16093
  9  19.20898  0.2808571  14.31282

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 47 of 224 using max hudgins knn 2 
k-Nearest Neighbors 

3683 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3314, 3316, 3315, 3314, 3314, 3315, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.65503  0.1202547  16.53455
  7  21.32592  0.1252100  16.46967
  9  21.08707  0.1324696  16.41004

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 48 of 224 using same hudgins knn 2 
k-Nearest Neighbors 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1106, 1105, 1104, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  22.00914  0.1011787  17.11886
  7  21.65375  0.1090671  16.96739
  9  21.52931  0.1096544  16.98208

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 49 of 224 using max all svmPoly 2 
Support Vector Machines with Polynomial Kernel 

3683 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3315, 3316, 3313, 3316, 3314, 3315, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.85797  0.04121625  17.63438
  1       0.001  0.50  22.69252  0.04964302  17.53857
  1       0.001  1.00  22.46024  0.06079857  17.42795
  1       0.010  0.25  22.12777  0.07640122  17.27354
  1       0.010  0.50  21.90468  0.08569817  17.18475
  1       0.010  1.00  21.72436  0.09480118  17.11112
  1       0.100  0.25  21.51327  0.10663559  16.99104
  1       0.100  0.50  21.44403  0.11145192  16.92890
  1       0.100  1.00  21.39482  0.11540478  16.87685
  2       0.001  0.25  22.51131  0.06023482  17.43785
  2       0.001  0.50  22.19479  0.07779171  17.27071
  2       0.001  1.00  21.82213  0.09706143  17.06822
  2       0.010  0.25  20.88506  0.15480576  16.38224
  2       0.010  0.50  20.58063  0.17518934  16.11736
  2       0.010  1.00  20.32712  0.19196223  15.84637
  2       0.100  0.25  19.91762  0.22347428  15.21862
  2       0.100  0.50  19.82574  0.23029683  15.11704
  2       0.100  1.00  19.72489  0.23755103  15.00159
  3       0.001  0.25  22.18172  0.08015014  17.25712
  3       0.001  0.50  21.78588  0.10069233  17.04705
  3       0.001  1.00  21.38691  0.12187389  16.80179
  3       0.010  0.25  20.44966  0.18623652  15.79886
  3       0.010  0.50  20.24637  0.19911071  15.60281
  3       0.010  1.00  20.09515  0.20921473  15.43230
  3       0.100  0.25  20.06488  0.22716316  14.83998
  3       0.100  0.50  20.24187  0.22476957  14.84174
  3       0.100  1.00  20.56678  0.21758951  14.92666

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 50 of 224 using same all svmPoly 2 
Support Vector Machines with Polynomial Kernel 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1106, 1105, 1106, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.24184  0.03112007  17.85357
  1       0.001  0.50  23.11927  0.03336942  17.77376
  1       0.001  1.00  23.02823  0.03807624  17.71859
  1       0.010  0.25  22.83764  0.04897765  17.60263
  1       0.010  0.50  22.60565  0.05874667  17.51449
  1       0.010  1.00  22.34841  0.06812590  17.44510
  1       0.100  0.25  22.11332  0.07976454  17.37532
  1       0.100  0.50  21.94973  0.09120789  17.29093
  1       0.100  1.00  21.82545  0.09831295  17.16606
  2       0.001  0.25  23.08774  0.03613675  17.74605
  2       0.001  0.50  22.91537  0.04468565  17.65031
  2       0.001  1.00  22.66922  0.05754544  17.52383
  2       0.010  0.25  21.44806  0.12953836  16.87384
  2       0.010  0.50  21.17714  0.14263899  16.68090
  2       0.010  1.00  20.93879  0.15804780  16.48759
  2       0.100  0.25  20.59512  0.18332975  15.92937
  2       0.100  0.50  20.51885  0.18970302  15.82087
  2       0.100  1.00  20.46469  0.19580410  15.73660
  3       0.001  0.25  22.91916  0.04487845  17.65365
  3       0.001  0.50  22.63386  0.05927178  17.51614
  3       0.001  1.00  22.30702  0.07879084  17.36092
  3       0.010  0.25  21.07378  0.15428793  16.44829
  3       0.010  0.50  20.86327  0.16694669  16.26843
  3       0.010  1.00  20.70866  0.17555138  16.13574
  3       0.100  0.25  23.31233  0.13866643  16.58979
  3       0.100  0.50  25.49390  0.12365085  16.94855
  3       0.100  1.00  28.94796  0.10547696  17.58839

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 51 of 224 using max du svmPoly 2 
Support Vector Machines with Polynomial Kernel 

3683 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3313, 3313, 3315, 3316, 3315, 3314, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.99653  0.03560917  17.67158
  1       0.001  0.50  22.86000  0.04338119  17.58166
  1       0.001  1.00  22.61847  0.05570438  17.46186
  1       0.010  0.25  22.25266  0.07373604  17.29351
  1       0.010  0.50  21.99085  0.08425485  17.19665
  1       0.010  1.00  21.79238  0.09177688  17.13077
  1       0.100  0.25  21.64163  0.09722570  17.05872
  1       0.100  0.50  21.62527  0.09726127  17.01795
  1       0.100  1.00  21.65506  0.09611183  16.99958
  2       0.001  0.25  22.79216  0.04825697  17.53808
  2       0.001  0.50  22.46842  0.06494584  17.37871
  2       0.001  1.00  22.14776  0.08330215  17.20899
  2       0.010  0.25  21.09959  0.14009068  16.58895
  2       0.010  0.50  20.79264  0.15988000  16.36981
  2       0.010  1.00  20.51136  0.17828407  16.14308
  2       0.100  0.25  20.08306  0.20806054  15.55395
  2       0.100  0.50  20.03418  0.21208769  15.47340
  2       0.100  1.00  19.96576  0.21724939  15.37887
  3       0.001  0.25  22.51160  0.06366440  17.39397
  3       0.001  0.50  22.16493  0.08410606  17.20622
  3       0.001  1.00  21.75499  0.10541131  16.99701
  3       0.010  0.25  20.64099  0.17519911  16.12317
  3       0.010  0.50  20.38150  0.18950332  15.88930
  3       0.010  1.00  20.20969  0.19918468  15.71782
  3       0.100  0.25  20.28202  0.20729410  15.28045
  3       0.100  0.50  20.49543  0.20448344  15.29477
  3       0.100  1.00  20.80111  0.19926259  15.37452

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 52 of 224 using same du svmPoly 2 
Support Vector Machines with Polynomial Kernel 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1106, 1106, 1106, 1104, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.31030  0.02691187  17.90923
  1       0.001  0.50  23.23773  0.02828037  17.82276
  1       0.001  1.00  23.14971  0.03275877  17.75596
  1       0.010  0.25  22.95253  0.04404306  17.62815
  1       0.010  0.50  22.69947  0.05565685  17.52594
  1       0.010  1.00  22.44106  0.06602272  17.45810
  1       0.100  0.25  22.16902  0.07698808  17.39593
  1       0.100  0.50  22.02372  0.08498785  17.33459
  1       0.100  1.00  21.95490  0.08835208  17.25494
  2       0.001  0.25  23.22923  0.02960327  17.81071
  2       0.001  0.50  23.10671  0.03532883  17.72624
  2       0.001  1.00  22.91101  0.04636730  17.61067
  2       0.010  0.25  21.87592  0.10794120  17.10616
  2       0.010  0.50  21.49180  0.12586208  16.88755
  2       0.010  1.00  21.20165  0.13944896  16.72112
  2       0.100  0.25  20.64672  0.17611035  16.22636
  2       0.100  0.50  20.59040  0.18054702  16.14560
  2       0.100  1.00  20.63598  0.18041307  16.13158
  3       0.001  0.25  23.12860  0.03460003  17.73653
  3       0.001  0.50  22.92846  0.04507435  17.63263
  3       0.001  1.00  22.62368  0.06105671  17.48155
  3       0.010  0.25  21.48506  0.12793925  16.82789
  3       0.010  0.50  21.12257  0.14696978  16.60799
  3       0.010  1.00  20.84738  0.16511702  16.40156
  3       0.100  0.25  22.31501  0.13712486  16.76198
  3       0.100  0.50  24.12563  0.11793275  17.20543
  3       0.100  1.00  27.22382  0.09735685  17.75310

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 53 of 224 using max rms svmPoly 2 
Support Vector Machines with Polynomial Kernel 

3683 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3314, 3314, 3314, 3316, 3315, 3314, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.21972  0.04338716  17.95170
  1       0.001  0.50  22.98167  0.04268246  17.83412
  1       0.001  1.00  22.78090  0.04234944  17.73137
  1       0.010  0.25  22.70663  0.04330531  17.68713
  1       0.010  0.50  22.67926  0.04312801  17.67829
  1       0.010  1.00  22.66808  0.04303997  17.67637
  1       0.100  0.25  22.66175  0.04303985  17.67651
  1       0.100  0.50  22.65860  0.04302851  17.67669
  1       0.100  1.00  22.65702  0.04303025  17.67676
  2       0.001  0.25  22.97781  0.04316737  17.82935
  2       0.001  0.50  22.77263  0.04303430  17.72244
  2       0.001  1.00  22.70751  0.04433863  17.67878
  2       0.010  0.25  22.27582  0.06768475  17.40764
  2       0.010  0.50  22.01043  0.08034726  17.27051
  2       0.010  1.00  21.73573  0.09272007  17.11518
  2       0.100  0.25  21.51173  0.11239801  16.74943
  2       0.100  0.50  21.50412  0.11286050  16.75202
  2       0.100  1.00  21.49840  0.11323493  16.75285
  3       0.001  0.25  22.82616  0.04342046  17.75558
  3       0.001  0.50  22.72108  0.04466921  17.68069
  3       0.001  1.00  22.65294  0.04679018  17.64261
  3       0.010  0.25  21.86209  0.08829935  17.18666
  3       0.010  0.50  21.63990  0.10222088  17.01071
  3       0.010  1.00  21.51404  0.11059536  16.86362
  3       0.100  0.25  20.83420  0.16377257  15.80548
  3       0.100  0.50  20.79257  0.16575113  15.76767
  3       0.100  1.00  20.77748  0.16641012  15.75127

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 54 of 224 using same rms svmPoly 2 
Support Vector Machines with Polynomial Kernel 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1104, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.48582  0.03119674  18.12266
  1       0.001  0.50  23.44129  0.03426091  18.05935
  1       0.001  1.00  23.32911  0.03589671  17.95881
  1       0.010  0.25  23.04741  0.03480707  17.84183
  1       0.010  0.50  22.98574  0.03490398  17.79113
  1       0.010  1.00  22.97183  0.03553134  17.78100
  1       0.100  0.25  22.95488  0.03586994  17.78039
  1       0.100  0.50  22.95095  0.03585345  17.78068
  1       0.100  1.00  22.94911  0.03581540  17.78058
  2       0.001  0.25  23.43996  0.03457349  18.05777
  2       0.001  0.50  23.32801  0.03627073  17.95593
  2       0.001  1.00  23.09265  0.03538177  17.86261
  2       0.010  0.25  22.85611  0.04308366  17.69222
  2       0.010  0.50  22.68915  0.04961153  17.61675
  2       0.010  1.00  22.46237  0.06006648  17.51441
  2       0.100  0.25  21.76029  0.10763373  17.00234
  2       0.100  0.50  21.70242  0.10945878  16.98263
  2       0.100  1.00  21.67426  0.11087239  16.97600
  3       0.001  0.25  23.39909  0.03562805  17.99918
  3       0.001  0.50  23.18394  0.03588937  17.89973
  3       0.001  1.00  23.00341  0.03594645  17.80807
  3       0.010  0.25  22.58314  0.05628110  17.55908
  3       0.010  0.50  22.35163  0.06745672  17.45883
  3       0.010  1.00  22.06348  0.08209573  17.31769
  3       0.100  0.25  21.19266  0.14705978  16.21670
  3       0.100  0.50  21.12212  0.15140914  16.11913
  3       0.100  1.00  21.09144  0.15366768  16.07238

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 55 of 224 using max hudgins svmPoly 2 
Support Vector Machines with Polynomial Kernel 

3683 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3314, 3314, 3315, 3313, 3315, 3316, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.11737  0.02550444  17.81069
  1       0.001  0.50  23.05673  0.03060499  17.74058
  1       0.001  1.00  22.91933  0.03907927  17.66173
  1       0.010  0.25  22.61183  0.05632284  17.51161
  1       0.010  0.50  22.30832  0.06826824  17.38612
  1       0.010  1.00  22.10658  0.07591167  17.30293
  1       0.100  0.25  21.97395  0.08072832  17.24036
  1       0.100  0.50  21.94477  0.08182287  17.21742
  1       0.100  1.00  21.94773  0.08147865  17.21676
  2       0.001  0.25  23.03699  0.03225424  17.72382
  2       0.001  0.50  22.87657  0.04222005  17.62869
  2       0.001  1.00  22.59502  0.05851799  17.48329
  2       0.010  0.25  21.62007  0.10690507  16.93785
  2       0.010  0.50  21.43442  0.11637141  16.80159
  2       0.010  1.00  21.27341  0.12474529  16.71623
  2       0.100  0.25  20.72864  0.16250156  16.20523
  2       0.100  0.50  20.66982  0.16649979  16.12065
  2       0.100  1.00  20.62752  0.16906872  16.05210
  3       0.001  0.25  22.91706  0.03965825  17.64983
  3       0.001  0.50  22.65648  0.05592889  17.50010
  3       0.001  1.00  22.26847  0.07507866  17.31382
  3       0.010  0.25  21.36027  0.12352986  16.70872
  3       0.010  0.50  21.09551  0.13978606  16.50108
  3       0.010  1.00  20.86254  0.15360677  16.29362
  3       0.100  0.25  20.69513  0.17361889  15.70192
  3       0.100  0.50  20.80496  0.17386072  15.68559
  3       0.100  1.00  20.80790  0.17814158  15.63706

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 56 of 224 using same hudgins svmPoly 2 
Support Vector Machines with Polynomial Kernel 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1106, 1105, 1106, 1106, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.41059  0.01994268  18.03786
  1       0.001  0.50  23.34224  0.02155779  17.93647
  1       0.001  1.00  23.29035  0.02338525  17.87313
  1       0.010  0.25  23.16414  0.03005633  17.80438
  1       0.010  0.50  23.02142  0.03909389  17.73475
  1       0.010  1.00  22.81310  0.05018660  17.63418
  1       0.100  0.25  22.54261  0.06177563  17.53298
  1       0.100  0.50  22.39211  0.06890301  17.50730
  1       0.100  1.00  22.28509  0.07442090  17.48098
  2       0.001  0.25  23.34250  0.02202267  17.93263
  2       0.001  0.50  23.28239  0.02424253  17.86400
  2       0.001  1.00  23.16985  0.02992838  17.79828
  2       0.010  0.25  22.47807  0.07413206  17.44924
  2       0.010  0.50  22.11646  0.09367990  17.24850
  2       0.010  1.00  21.75916  0.10878558  17.06423
  2       0.100  0.25  21.26588  0.13527245  16.72341
  2       0.100  0.50  21.13674  0.14484976  16.58449
  2       0.100  1.00  21.02301  0.15325027  16.46744
  3       0.001  0.25  23.30933  0.02366315  17.88557
  3       0.001  0.50  23.20165  0.02850651  17.80623
  3       0.001  1.00  23.02968  0.03793740  17.72370
  3       0.010  0.25  22.08628  0.09920305  17.16722
  3       0.010  0.50  21.74539  0.11160092  17.00079
  3       0.010  1.00  21.46593  0.12533797  16.83370
  3       0.100  0.25  21.90333  0.13269238  16.65955
  3       0.100  0.50  22.75901  0.11970942  16.95024
  3       0.100  1.00  24.36563  0.10347055  17.45309

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 57 of 224 using max all ranger 2 
Random Forest 

3683 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3314, 3316, 3313, 3315, 3313, 3315, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.41535  0.4763246  11.83020
   2    extratrees  16.98451  0.4580215  12.75475
  13    variance    16.10662  0.4871808  11.17238
  13    extratrees  16.00205  0.5024330  11.38328
  24    variance    16.25606  0.4768891  11.30516
  24    extratrees  15.94689  0.5037995  11.26835

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 58 of 224 using same all ranger 2 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1106, 1106, 1104, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.14353  0.3611897  13.68752
   2    extratrees  18.50735  0.3512080  14.35346
  13    variance    18.13968  0.3548139  13.41820
  13    extratrees  17.89969  0.3782504  13.39156
  24    variance    18.26696  0.3451510  13.58584
  24    extratrees  17.89749  0.3767830  13.33270

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 59 of 224 using max du ranger 2 
Random Forest 

3683 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3315, 3314, 3314, 3314, 3317, 3315, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.94336  0.4432896  12.44758
   2    extratrees  17.65992  0.4104377  13.45202
  10    variance    16.25852  0.4783129  11.37863
  10    extratrees  16.49587  0.4731380  11.93740
  18    variance    16.30248  0.4740977  11.35946
  18    extratrees  16.35878  0.4793616  11.73120

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 60 of 224 using same du ranger 2 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1106, 1105, 1105, 1104, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.46044  0.3395093  14.08158
   2    extratrees  18.89211  0.3234531  14.78362
  10    variance    18.23954  0.3474578  13.56850
  10    extratrees  18.20884  0.3582953  13.78528
  18    variance    18.32742  0.3403610  13.63386
  18    extratrees  18.13473  0.3611717  13.65473

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 61 of 224 using max rms ranger 2 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

3683 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3316, 3313, 3315, 3314, 3314, 3314, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.81020  0.4386939  11.55365
  2     extratrees  16.74868  0.4490053  12.14873
  3     variance    16.92119  0.4321696  11.50880
  3     extratrees  16.66243  0.4510901  11.90501

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 62 of 224 using same rms ranger 2 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    18.41966  0.3341426  13.48985
  2     extratrees  18.18752  0.3560196  13.81144
  3     variance    18.50642  0.3298335  13.46158
  3     extratrees  18.07660  0.3605041  13.58703

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 63 of 224 using max hudgins ranger 2 
Random Forest 

3683 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 3314, 3314, 3315, 3315, 3315, 3315, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.94640  0.3717688  13.53416
   2    extratrees  18.55931  0.3410731  14.39138
   7    variance    17.11211  0.4208045  12.27335
   7    extratrees  17.44717  0.4082392  12.97845
  12    variance    17.03779  0.4241736  12.06953
  12    extratrees  17.15275  0.4257724  12.60892

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 64 of 224 using same hudgins ranger 2 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1104, 1104, 1106, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    19.19090  0.2818973  14.92694
   2    extratrees  19.53549  0.2701924  15.53930
   7    variance    18.80232  0.3059691  14.22158
   7    extratrees  18.94394  0.3017361  14.64597
  12    variance    18.70631  0.3127479  14.04462
  12    extratrees  18.81447  0.3091791  14.43823

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 65 of 224 using max all lm 3 
Linear Regression 

2455 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2209, 2210, 2210, 2210, 2210, 2210, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.66576  0.1490054  16.97234

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 66 of 224 using same all lm 3 
Linear Regression 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1104, 1105, 1106, 1106, 1104, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.36505  0.1599352  16.56668

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 67 of 224 using max du lm 3 
Linear Regression 

2455 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2210, 2211, 2208, 2209, 2209, 2210, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.00056  0.1214878  17.17651

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 68 of 224 using same du lm 3 
Linear Regression 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1105, 1107, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE    
  20.77764  0.1260315  16.8664

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 69 of 224 using max rms lm 3 
Linear Regression 

2455 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2209, 2209, 2210, 2208, 2210, 2210, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.93687  0.04202693  18.20383

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 70 of 224 using same rms lm 3 
Linear Regression 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1104, 1107, 1105, 1104, 1105, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.73472  0.04253537  17.92648

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 71 of 224 using max hudgins lm 3 
Linear Regression 

2455 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2207, 2210, 2211, 2210, 2210, 2210, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  21.2354  0.1023125  17.54298

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 72 of 224 using same hudgins lm 3 
Linear Regression 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1107, 1106, 1105, 1107, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.05918  0.1016204  17.30459

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 73 of 224 using max all knn 3 
k-Nearest Neighbors 

2455 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2208, 2210, 2210, 2210, 2209, 2211, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.16523  0.3593302  12.22948
  7  17.96994  0.3645876  12.39854
  9  17.85193  0.3683862  12.55685

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 74 of 224 using same all knn 3 
k-Nearest Neighbors 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1107, 1104, 1104, 1106, 1104, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.49191  0.3219416  13.07722
  7  18.39399  0.3191359  13.29039
  9  18.39334  0.3152693  13.49083

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 75 of 224 using max du knn 3 
k-Nearest Neighbors 

2455 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2211, 2208, 2211, 2210, 2209, 2208, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.17026  0.3595725  12.22815
  7  17.96533  0.3650783  12.42533
  9  17.84653  0.3687165  12.56157

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 76 of 224 using same du knn 3 
k-Nearest Neighbors 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1106, 1106, 1105, 1103, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.43023  0.3260941  13.03240
  7  18.35180  0.3218507  13.30160
  9  18.45537  0.3115786  13.54747

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 77 of 224 using max rms knn 3 
k-Nearest Neighbors 

2455 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2210, 2210, 2209, 2210, 2208, 2210, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.06818  0.3637647  12.29144
  7  17.91067  0.3670907  12.44576
  9  17.93024  0.3626907  12.65383

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 78 of 224 using same rms knn 3 
k-Nearest Neighbors 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1106, 1105, 1104, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.72459  0.3081513  13.21093
  7  18.46710  0.3172398  13.32741
  9  18.54854  0.3076873  13.55803

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 79 of 224 using max hudgins knn 3 
k-Nearest Neighbors 

2455 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2211, 2208, 2211, 2209, 2210, 2209, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.33215  0.1442308  16.12049
  7  21.02599  0.1466927  16.08524
  9  20.88919  0.1463063  16.16674

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 80 of 224 using same hudgins knn 3 
k-Nearest Neighbors 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1105, 1105, 1107, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.67580  0.1037324  16.76046
  7  21.42014  0.1031830  16.83018
  9  21.21190  0.1059536  16.78848

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 81 of 224 using max all svmPoly 3 
Support Vector Machines with Polynomial Kernel 

2455 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2208, 2209, 2210, 2209, 2210, 2208, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.92833  0.03651694  17.65701
  1       0.001  0.50  22.79498  0.04289434  17.57436
  1       0.001  1.00  22.59287  0.05347771  17.46310
  1       0.010  0.25  22.24896  0.07070717  17.29285
  1       0.010  0.50  21.94965  0.08440163  17.17281
  1       0.010  1.00  21.71871  0.09572834  17.07184
  1       0.100  0.25  21.47734  0.11088583  16.91946
  1       0.100  0.50  21.35897  0.11848643  16.82153
  1       0.100  1.00  21.30708  0.12271129  16.74620
  2       0.001  0.25  22.67700  0.05077238  17.49549
  2       0.001  0.50  22.37312  0.06664497  17.33498
  2       0.001  1.00  21.98378  0.08726557  17.13354
  2       0.010  0.25  21.05061  0.14231572  16.46795
  2       0.010  0.50  20.77556  0.15983246  16.25960
  2       0.010  1.00  20.49275  0.17901770  15.98150
  2       0.100  0.25  20.03413  0.21376840  15.24605
  2       0.100  0.50  19.90388  0.22356307  15.11554
  2       0.100  1.00  19.79690  0.23165943  15.01619
  3       0.001  0.25  22.36263  0.06823313  17.32916
  3       0.001  0.50  21.95809  0.08993869  17.11324
  3       0.001  1.00  21.52212  0.11308005  16.85399
  3       0.010  0.25  20.59423  0.17422239  15.87617
  3       0.010  0.50  20.39010  0.18706059  15.67609
  3       0.010  1.00  20.20754  0.20040973  15.46423
  3       0.100  0.25  20.90317  0.19745030  15.19803
  3       0.100  0.50  21.60818  0.18788399  15.27519
  3       0.100  1.00  22.30382  0.18238777  15.38627

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 82 of 224 using same all svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1105, 1105, 1106, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.77339  0.03597383  17.44571
  1       0.001  0.50  22.68138  0.03983844  17.35514
  1       0.001  1.00  22.59387  0.04562081  17.29402
  1       0.010  0.25  22.33582  0.06042947  17.14746
  1       0.010  0.50  22.05310  0.07374824  17.01771
  1       0.010  1.00  21.78858  0.08615068  16.91024
  1       0.100  0.25  21.43100  0.10600895  16.72614
  1       0.100  0.50  21.22470  0.11730235  16.58287
  1       0.100  1.00  21.08460  0.12638371  16.41205
  2       0.001  0.25  22.66006  0.04327243  17.32978
  2       0.001  0.50  22.52052  0.05180180  17.23346
  2       0.001  1.00  22.26204  0.06523135  17.09690
  2       0.010  0.25  21.26557  0.12397391  16.56005
  2       0.010  0.50  20.90948  0.14087169  16.32102
  2       0.010  1.00  20.55990  0.16245447  16.02141
  2       0.100  0.25  19.98381  0.20555768  15.20229
  2       0.100  0.50  19.91312  0.21124111  15.13153
  2       0.100  1.00  19.86730  0.21755907  15.06329
  3       0.001  0.25  22.54828  0.05234366  17.24056
  3       0.001  0.50  22.29817  0.06542006  17.10860
  3       0.001  1.00  21.92648  0.08345772  16.94584
  3       0.010  0.25  20.66569  0.16081221  15.94121
  3       0.010  0.50  20.39648  0.17742646  15.65204
  3       0.010  1.00  20.20306  0.18880286  15.46427
  3       0.100  0.25  21.30844  0.18756269  15.52957
  3       0.100  0.50  22.55024  0.17715138  15.85287
  3       0.100  1.00  23.98115  0.16833585  16.26837

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 83 of 224 using max du svmPoly 3 
Support Vector Machines with Polynomial Kernel 

2455 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2210, 2210, 2209, 2209, 2209, 2209, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.03816  0.03092324  17.69799
  1       0.001  0.50  22.93858  0.03631801  17.61423
  1       0.001  1.00  22.76336  0.04653156  17.50202
  1       0.010  0.25  22.39455  0.06639547  17.31657
  1       0.010  0.50  22.04668  0.08153463  17.18515
  1       0.010  1.00  21.79171  0.09272174  17.08576
  1       0.100  0.25  21.58349  0.10159385  16.97742
  1       0.100  0.50  21.53740  0.10325670  16.91889
  1       0.100  1.00  21.54919  0.10282234  16.86543
  2       0.001  0.25  22.89179  0.03978905  17.58232
  2       0.001  0.50  22.65851  0.05326508  17.43833
  2       0.001  1.00  22.31174  0.07195607  17.26550
  2       0.010  0.25  21.24550  0.13039730  16.64806
  2       0.010  0.50  20.96055  0.14638512  16.46663
  2       0.010  1.00  20.70211  0.16369432  16.27855
  2       0.100  0.25  20.19505  0.20006329  15.58763
  2       0.100  0.50  20.11923  0.20564944  15.49381
  2       0.100  1.00  20.09430  0.20837810  15.45545
  3       0.001  0.25  22.69847  0.05173860  17.45764
  3       0.001  0.50  22.35021  0.07064992  17.27918
  3       0.001  1.00  21.91268  0.09579833  17.04303
  3       0.010  0.25  20.82357  0.16173567  16.24797
  3       0.010  0.50  20.51951  0.17997643  15.96523
  3       0.010  1.00  20.32094  0.19083480  15.76270
  3       0.100  0.25  20.83629  0.18595112  15.50116
  3       0.100  0.50  21.61297  0.17398697  15.65990
  3       0.100  1.00  22.57016  0.16272581  15.83286

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 84 of 224 using same du svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1104, 1105, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.85508  0.03182678  17.48747
  1       0.001  0.50  22.74641  0.03491162  17.38124
  1       0.001  1.00  22.67334  0.04149928  17.30247
  1       0.010  0.25  22.45238  0.05625599  17.14900
  1       0.010  0.50  22.14316  0.07132358  17.01413
  1       0.010  1.00  21.85579  0.08504984  16.90219
  1       0.100  0.25  21.49171  0.10267314  16.73823
  1       0.100  0.50  21.32067  0.11025847  16.64294
  1       0.100  1.00  21.24549  0.11361051  16.53624
  2       0.001  0.25  22.73599  0.03681567  17.36557
  2       0.001  0.50  22.64430  0.04432706  17.27629
  2       0.001  1.00  22.46253  0.05625157  17.15037
  2       0.010  0.25  21.58122  0.11226102  16.72368
  2       0.010  0.50  21.14684  0.13340718  16.50636
  2       0.010  1.00  20.75634  0.15260639  16.26065
  2       0.100  0.25  20.02579  0.19873081  15.54218
  2       0.100  0.50  19.96195  0.20322944  15.46665
  2       0.100  1.00  19.91988  0.20834025  15.40992
  3       0.001  0.25  22.66348  0.04360543  17.29219
  3       0.001  0.50  22.51020  0.05432238  17.17442
  3       0.001  1.00  22.22479  0.06975748  17.03199
  3       0.010  0.25  21.10831  0.14005641  16.40390
  3       0.010  0.50  20.62265  0.16473291  16.05507
  3       0.010  1.00  20.28956  0.18476668  15.73843
  3       0.100  0.25  21.53211  0.15980398  15.95523
  3       0.100  0.50  22.44531  0.15268815  16.24524
  3       0.100  1.00  24.31970  0.13777404  16.84064

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 85 of 224 using max rms svmPoly 3 
Support Vector Machines with Polynomial Kernel 

2455 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2210, 2210, 2210, 2209, 2211, 2208, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.27240  0.03856558  17.96849
  1       0.001  0.50  23.15688  0.04048435  17.87221
  1       0.001  1.00  22.91521  0.03970281  17.77763
  1       0.010  0.25  22.72497  0.04023283  17.69735
  1       0.010  0.50  22.69569  0.04020298  17.68206
  1       0.010  1.00  22.68155  0.04001664  17.67827
  1       0.100  0.25  22.66548  0.03989492  17.67743
  1       0.100  0.50  22.66032  0.03988509  17.67711
  1       0.100  1.00  22.65710  0.03986326  17.67696
  2       0.001  0.25  23.15612  0.04099477  17.86906
  2       0.001  0.50  22.91050  0.04026208  17.77229
  2       0.001  1.00  22.73106  0.04095354  17.69412
  2       0.010  0.25  22.36871  0.05930193  17.46838
  2       0.010  0.50  22.11522  0.07121902  17.32986
  2       0.010  1.00  21.83814  0.08457497  17.15686
  2       0.100  0.25  21.52568  0.11123087  16.65579
  2       0.100  0.50  21.52417  0.11171718  16.65937
  2       0.100  1.00  21.52020  0.11215309  16.66177
  3       0.001  0.25  23.01412  0.04085544  17.80555
  3       0.001  0.50  22.77713  0.04108109  17.71083
  3       0.001  1.00  22.67543  0.04269162  17.66166
  3       0.010  0.25  21.97839  0.07969208  17.23934
  3       0.010  0.50  21.70874  0.09381459  17.04005
  3       0.010  1.00  21.57521  0.10499563  16.85866
  3       0.100  0.25  20.88504  0.15928086  15.76275
  3       0.100  0.50  20.83805  0.16214270  15.71947
  3       0.100  1.00  20.81164  0.16303487  15.70025

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 86 of 224 using same rms svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1103, 1107, 1106, 1107, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.32731  0.03608430  17.70903
  1       0.001  0.50  23.21067  0.03857781  17.65806
  1       0.001  1.00  22.92888  0.04239548  17.56526
  1       0.010  0.25  22.60506  0.04554726  17.45735
  1       0.010  0.50  22.50129  0.04622920  17.39361
  1       0.010  1.00  22.47520  0.04660282  17.37883
  1       0.100  0.25  22.45132  0.04695858  17.37457
  1       0.100  0.50  22.44677  0.04693117  17.37386
  1       0.100  1.00  22.44437  0.04689027  17.37414
  2       0.001  0.25  23.20854  0.03903785  17.65604
  2       0.001  0.50  22.92465  0.04305442  17.56114
  2       0.001  1.00  22.66311  0.04621335  17.47395
  2       0.010  0.25  22.42263  0.05569365  17.28684
  2       0.010  0.50  22.30878  0.06213540  17.20491
  2       0.010  1.00  22.12383  0.07136798  17.09869
  2       0.100  0.25  21.49171  0.10660276  16.49202
  2       0.100  0.50  21.46602  0.10742610  16.48541
  2       0.100  1.00  21.46118  0.10755570  16.48835
  3       0.001  0.25  23.00172  0.04150957  17.60207
  3       0.001  0.50  22.77182  0.04557567  17.50843
  3       0.001  1.00  22.55222  0.04735470  17.41515
  3       0.010  0.25  22.27880  0.06856577  17.15249
  3       0.010  0.50  22.06635  0.07909302  17.02774
  3       0.010  1.00  21.81165  0.08941984  16.89256
  3       0.100  0.25  20.63419  0.16333618  15.59798
  3       0.100  0.50  20.50312  0.17192779  15.44141
  3       0.100  1.00  20.49319  0.17163413  15.41577

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 87 of 224 using max hudgins svmPoly 3 
Support Vector Machines with Polynomial Kernel 

2455 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2209, 2209, 2210, 2209, 2210, 2209, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.15873  0.02299143  17.84724
  1       0.001  0.50  23.09609  0.02505477  17.75282
  1       0.001  1.00  23.00841  0.03069602  17.68484
  1       0.010  0.25  22.78974  0.04538926  17.55217
  1       0.010  0.50  22.49704  0.05865824  17.42972
  1       0.010  1.00  22.19459  0.07159198  17.31112
  1       0.100  0.25  21.94996  0.08288175  17.20183
  1       0.100  0.50  21.86907  0.08739214  17.14546
  1       0.100  1.00  21.83781  0.08845533  17.10798
  2       0.001  0.25  23.08620  0.02608300  17.74099
  2       0.001  0.50  22.97695  0.03298959  17.66087
  2       0.001  1.00  22.78027  0.04541969  17.54427
  2       0.010  0.25  21.76323  0.09932184  16.97728
  2       0.010  0.50  21.49813  0.11062308  16.82279
  2       0.010  1.00  21.34648  0.11998376  16.71113
  2       0.100  0.25  20.85758  0.15535978  16.21388
  2       0.100  0.50  20.73192  0.16308217  16.10073
  2       0.100  1.00  20.63796  0.16872369  15.99704
  3       0.001  0.25  23.01076  0.03123052  17.67804
  3       0.001  0.50  22.83153  0.04233306  17.57093
  3       0.001  1.00  22.50461  0.06038366  17.40157
  3       0.010  0.25  21.47640  0.11369094  16.76401
  3       0.010  0.50  21.24950  0.12965182  16.58108
  3       0.010  1.00  20.99095  0.14511963  16.36543
  3       0.100  0.25  20.79849  0.16797770  15.72326
  3       0.100  0.50  21.26262  0.16016943  15.81642
  3       0.100  1.00  21.92526  0.15326473  15.94783

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 88 of 224 using same hudgins svmPoly 3 
Support Vector Machines with Polynomial Kernel 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1105, 1106, 1104, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.11754  0.02595227  17.63015
  1       0.001  0.50  22.89986  0.02660529  17.53435
  1       0.001  1.00  22.84131  0.02913618  17.45636
  1       0.010  0.25  22.77146  0.03774528  17.38047
  1       0.010  0.50  22.61908  0.04792977  17.29364
  1       0.010  1.00  22.31469  0.06316257  17.15798
  1       0.100  0.25  21.97464  0.07648415  17.03831
  1       0.100  0.50  21.80209  0.08346098  16.97608
  1       0.100  1.00  21.71463  0.08689306  16.91630
  2       0.001  0.25  22.89233  0.02708737  17.52561
  2       0.001  0.50  22.83218  0.03016014  17.44640
  2       0.001  1.00  22.77880  0.03725779  17.38265
  2       0.010  0.25  22.26671  0.07349939  17.08677
  2       0.010  0.50  21.84938  0.09304465  16.91373
  2       0.010  1.00  21.46626  0.10632891  16.75717
  2       0.100  0.25  20.92535  0.13844824  16.27162
  2       0.100  0.50  20.72632  0.15051661  16.09070
  2       0.100  1.00  20.62943  0.15863158  15.95780
  3       0.001  0.25  22.84595  0.02924809  17.47736
  3       0.001  0.50  22.79353  0.03575277  17.39637
  3       0.001  1.00  22.66727  0.04545753  17.30636
  3       0.010  0.25  21.90798  0.09281279  16.91493
  3       0.010  0.50  21.49659  0.10741987  16.72314
  3       0.010  1.00  21.17618  0.12118053  16.52684
  3       0.100  0.25  21.94527  0.13026073  16.31915
  3       0.100  0.50  22.29795  0.13516075  16.33890
  3       0.100  1.00  22.66946  0.13711119  16.40058

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 89 of 224 using max all ranger 3 
Random Forest 

2455 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2211, 2210, 2209, 2210, 2209, 2209, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.62229  0.4624661  12.06624
   2    extratrees  17.11439  0.4485778  12.87519
  13    variance    16.40146  0.4684840  11.52186
  13    extratrees  16.22496  0.4879465  11.62936
  24    variance    16.56447  0.4571369  11.66979
  24    extratrees  16.18896  0.4881706  11.53019

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 90 of 224 using same all ranger 3 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1105, 1107, 1106, 1104, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.61075  0.3778824  13.14758
   2    extratrees  17.95652  0.3692175  13.74476
  13    variance    17.57083  0.3744354  12.82744
  13    extratrees  17.33239  0.3978813  12.78490
  24    variance    17.72811  0.3628808  12.99198
  24    extratrees  17.33379  0.3958195  12.74624

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 13, splitrule = extratrees
 and min.node.size = 5.


Now processing model 91 of 224 using max du ranger 3 
Random Forest 

2455 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2209, 2210, 2209, 2210, 2211, 2210, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.07321  0.4325826  12.56755
   2    extratrees  17.70392  0.4061797  13.45936
  10    variance    16.52005  0.4608811  11.68482
  10    extratrees  16.68097  0.4595908  12.11009
  18    variance    16.58566  0.4552827  11.69327
  18    extratrees  16.54973  0.4656498  11.92014

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 10, splitrule = variance
 and min.node.size = 5.


Now processing model 92 of 224 using same du ranger 3 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.86882  0.3609192  13.49148
   2    extratrees  18.32404  0.3441461  14.16794
  10    variance    17.63164  0.3706805  12.96482
  10    extratrees  17.65183  0.3768948  13.17036
  18    variance    17.68896  0.3661207  13.00254
  18    extratrees  17.60359  0.3778973  13.06134

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 93 of 224 using max rms ranger 3 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

2455 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2208, 2211, 2210, 2211, 2209, 2210, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    16.80467  0.4370807  11.75439
  2     extratrees  16.78709  0.4469521  12.29370
  3     variance    16.85503  0.4339675  11.68356
  3     extratrees  16.64777  0.4519391  12.01739

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 94 of 224 using same rms ranger 3 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1106, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.72605  0.3630219  12.77210
  2     extratrees  17.54880  0.3817075  13.11165
  3     variance    17.77166  0.3611755  12.68032
  3     extratrees  17.45922  0.3843935  12.87802

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 95 of 224 using max hudgins ranger 3 
Random Forest 

2455 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 2211, 2211, 2209, 2210, 2210, 2208, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.96019  0.3685881  13.52206
   2    extratrees  18.51199  0.3441625  14.33088
   7    variance    17.28585  0.4078906  12.49861
   7    extratrees  17.56247  0.3983985  13.05934
  12    variance    17.25540  0.4081140  12.35726
  12    extratrees  17.33303  0.4114646  12.76616

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 96 of 224 using same hudgins ranger 3 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1105, 1107, 1106, 1103, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.60827  0.3038337  14.28947
   2    extratrees  18.97991  0.2909461  14.88995
   7    variance    18.25074  0.3255624  13.62924
   7    extratrees  18.37273  0.3233941  13.97711
  12    variance    18.24293  0.3261342  13.50918
  12    extratrees  18.21265  0.3333861  13.74983

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 97 of 224 using max all lm 4 
Linear Regression 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1107, 1105, 1106, 1105, 1105, 1105, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  20.69669  0.150225  16.88631

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 98 of 224 using same all lm 4 
Linear Regression 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1107, 1105, 1105, 1105, 1106, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.74036  0.1496411  16.91875

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 99 of 224 using max du lm 4 
Linear Regression 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1105, 1106, 1105, 1103, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.13818  0.1161859  17.19899

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 100 of 224 using same du lm 4 
Linear Regression 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1106, 1105, 1104, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.21525  0.1138076  17.24348

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 101 of 224 using max rms lm 4 
Linear Regression 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1105, 1105, 1106, 1105, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.89696  0.05142386  18.18441

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 102 of 224 using same rms lm 4 
Linear Regression 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1104, 1105, 1105, 1106, 1105, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  22.00151  0.04576477  18.33456

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 103 of 224 using max hudgins lm 4 
Linear Regression 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1105, 1105, 1105, 1106, ... 
Resampling results:

  RMSE      Rsquared    MAE    
  21.33888  0.09810979  17.5621

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 104 of 224 using same hudgins lm 4 
Linear Regression 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1106, 1105, 1106, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.43312  0.0909005  17.68064

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 105 of 224 using max all knn 4 
k-Nearest Neighbors 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1103, 1105, 1106, 1106, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.20929  0.3535042  12.57807
  7  18.19544  0.3474352  13.00981
  9  18.33829  0.3347033  13.36665

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 106 of 224 using same all knn 4 
k-Nearest Neighbors 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.61517  0.3318825  12.84131
  7  18.47298  0.3309883  13.16522
  9  18.48788  0.3253289  13.45642

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 107 of 224 using max du knn 4 
k-Nearest Neighbors 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1106, 1106, 1105, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.21404  0.3537255  12.59733
  7  18.20110  0.3479356  13.06008
  9  18.36776  0.3330815  13.39537

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 108 of 224 using same du knn 4 
k-Nearest Neighbors 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1105, 1104, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.51775  0.3378639  12.81039
  7  18.38325  0.3370180  13.16019
  9  18.39782  0.3322983  13.43583

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 109 of 224 using max rms knn 4 
k-Nearest Neighbors 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1107, 1106, 1106, 1104, 1106, 1104, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.61199  0.3253790  13.05503
  7  18.41884  0.3312650  13.26590
  9  18.54386  0.3192553  13.61847

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 110 of 224 using same rms knn 4 
k-Nearest Neighbors 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1106, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.70117  0.3222110  12.97123
  7  18.47332  0.3295745  13.20083
  9  18.55162  0.3212351  13.48711

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 111 of 224 using max hudgins knn 4 
k-Nearest Neighbors 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1107, 1105, 1107, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.68454  0.1230000  16.63711
  7  21.20993  0.1346156  16.37502
  9  21.07742  0.1350257  16.42679

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 112 of 224 using same hudgins knn 4 
k-Nearest Neighbors 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1104, 1106, 1106, 1105, 1104, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.73273  0.1227542  16.50977
  7  21.48413  0.1195522  16.55536
  9  21.19518  0.1269844  16.46261

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 113 of 224 using max all svmPoly 4 
Support Vector Machines with Polynomial Kernel 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1106, 1105, 1107, 1104, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.97801  0.03996514  17.86273
  1       0.001  0.50  22.79253  0.04291692  17.77674
  1       0.001  1.00  22.62371  0.04903151  17.69938
  1       0.010  0.25  22.31917  0.06400444  17.51691
  1       0.010  0.50  22.05718  0.07525559  17.39176
  1       0.010  1.00  21.80177  0.08740342  17.26064
  1       0.100  0.25  21.52005  0.10322288  17.09397
  1       0.100  0.50  21.37264  0.11448343  16.94663
  1       0.100  1.00  21.24084  0.12518116  16.78396
  2       0.001  0.25  22.72372  0.04727800  17.73640
  2       0.001  0.50  22.47377  0.05747219  17.60464
  2       0.001  1.00  22.14218  0.07514753  17.39722
  2       0.010  0.25  21.02400  0.14440317  16.57061
  2       0.010  0.50  20.80555  0.15705720  16.36492
  2       0.010  1.00  20.55728  0.17307449  16.17631
  2       0.100  0.25  20.16045  0.20362298  15.57966
  2       0.100  0.50  20.05235  0.21269304  15.41176
  2       0.100  1.00  19.98316  0.22023400  15.27499
  3       0.001  0.25  22.46931  0.05963853  17.59900
  3       0.001  0.50  22.13746  0.07763521  17.40048
  3       0.001  1.00  21.73753  0.10114124  17.15885
  3       0.010  0.25  20.61233  0.17234366  16.10814
  3       0.010  0.50  20.38913  0.18654191  15.87006
  3       0.010  1.00  20.20526  0.19897302  15.68166
  3       0.100  0.25  22.72744  0.15717363  16.09652
  3       0.100  0.50  24.01536  0.14452500  16.41305
  3       0.100  1.00  26.17595  0.12751058  16.97587

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 114 of 224 using same all svmPoly 4 
Support Vector Machines with Polynomial Kernel 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1105, 1106, 1106, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.09406  0.03542864  17.94481
  1       0.001  0.50  22.94239  0.03703663  17.87263
  1       0.001  1.00  22.75943  0.04257666  17.80090
  1       0.010  0.25  22.41633  0.05724325  17.61946
  1       0.010  0.50  22.14761  0.07034690  17.47785
  1       0.010  1.00  21.88516  0.08370984  17.33253
  1       0.100  0.25  21.60303  0.09956550  17.14097
  1       0.100  0.50  21.44503  0.11004189  16.97695
  1       0.100  1.00  21.32559  0.11922118  16.82295
  2       0.001  0.25  22.88142  0.04179136  17.82507
  2       0.001  0.50  22.60086  0.05207599  17.69964
  2       0.001  1.00  22.24251  0.06989553  17.49079
  2       0.010  0.25  21.19173  0.13599338  16.71085
  2       0.010  0.50  20.94982  0.14946561  16.46158
  2       0.010  1.00  20.66140  0.16979603  16.20369
  2       0.100  0.25  20.11214  0.20940977  15.43589
  2       0.100  0.50  20.04884  0.21456591  15.32606
  2       0.100  1.00  20.00780  0.21957969  15.24850
  3       0.001  0.25  22.59496  0.05549730  17.68028
  3       0.001  0.50  22.23602  0.07415669  17.47387
  3       0.001  1.00  21.82107  0.09734218  17.22460
  3       0.010  0.25  20.73172  0.16976283  16.11763
  3       0.010  0.50  20.47453  0.18522393  15.87351
  3       0.010  1.00  20.29171  0.19641434  15.69463
  3       0.100  0.25  22.65979  0.16165350  15.97339
  3       0.100  0.50  24.43307  0.14065172  16.41402
  3       0.100  1.00  26.90654  0.12033554  17.07510

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 115 of 224 using max du svmPoly 4 
Support Vector Machines with Polynomial Kernel 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1104, 1107, 1104, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.12053  0.03616044  17.90988
  1       0.001  0.50  22.99373  0.03709162  17.82531
  1       0.001  1.00  22.81981  0.04248182  17.73838
  1       0.010  0.25  22.50217  0.05722615  17.56203
  1       0.010  0.50  22.21433  0.07186598  17.41020
  1       0.010  1.00  21.92438  0.08467714  17.27502
  1       0.100  0.25  21.64752  0.09660907  17.14632
  1       0.100  0.50  21.54860  0.10207600  17.03978
  1       0.100  1.00  21.51250  0.10596576  16.93308
  2       0.001  0.25  22.96838  0.03910700  17.80580
  2       0.001  0.50  22.75359  0.04596617  17.69992
  2       0.001  1.00  22.46767  0.05988794  17.52886
  2       0.010  0.25  21.35147  0.12762925  16.79066
  2       0.010  0.50  20.99641  0.14422245  16.51296
  2       0.010  1.00  20.77787  0.15783976  16.37463
  2       0.100  0.25  20.19613  0.19749561  15.84424
  2       0.100  0.50  20.11367  0.20427209  15.72346
  2       0.100  1.00  20.13055  0.20602990  15.66057
  3       0.001  0.25  22.79994  0.04558043  17.72110
  3       0.001  0.50  22.50968  0.05851577  17.55794
  3       0.001  1.00  22.13614  0.07941953  17.33483
  3       0.010  0.25  20.92252  0.15229759  16.41524
  3       0.010  0.50  20.58929  0.17249075  16.15667
  3       0.010  1.00  20.29280  0.19151126  15.88805
  3       0.100  0.25  22.49103  0.15320379  16.28473
  3       0.100  0.50  23.64032  0.14384650  16.53211
  3       0.100  1.00  25.44512  0.13349831  16.94552

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 0.5.


Now processing model 116 of 224 using same du svmPoly 4 
Support Vector Machines with Polynomial Kernel 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1106, 1106, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.19081  0.03002389  17.99463
  1       0.001  0.50  23.09710  0.03156557  17.91253
  1       0.001  1.00  22.96774  0.03531399  17.84962
  1       0.010  0.25  22.65602  0.04785943  17.69481
  1       0.010  0.50  22.31692  0.06390716  17.51999
  1       0.010  1.00  22.00578  0.08028995  17.36126
  1       0.100  0.25  21.71552  0.09304405  17.19075
  1       0.100  0.50  21.60766  0.09845302  17.06442
  1       0.100  1.00  21.57901  0.10107175  16.96298
  2       0.001  0.25  23.08173  0.03389408  17.89625
  2       0.001  0.50  22.90286  0.03980800  17.80835
  2       0.001  1.00  22.61368  0.05264284  17.65186
  2       0.010  0.25  21.50625  0.12235846  16.94571
  2       0.010  0.50  21.12633  0.13944255  16.66532
  2       0.010  1.00  20.87745  0.15533938  16.47041
  2       0.100  0.25  20.22871  0.19837109  15.73732
  2       0.100  0.50  20.08473  0.20815338  15.57612
  2       0.100  1.00  20.08379  0.20943698  15.52802
  3       0.001  0.25  22.93945  0.04011872  17.81262
  3       0.001  0.50  22.65210  0.05324640  17.66308
  3       0.001  1.00  22.24322  0.07519691  17.43544
  3       0.010  0.25  21.06983  0.14817236  16.53465
  3       0.010  0.50  20.69138  0.17075509  16.20605
  3       0.010  1.00  20.34999  0.19052049  15.89972
  3       0.100  0.25  22.61108  0.15912494  16.17298
  3       0.100  0.50  23.91553  0.14807901  16.42169
  3       0.100  1.00  25.76213  0.13320955  16.88648

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 117 of 224 using max rms svmPoly 4 
Support Vector Machines with Polynomial Kernel 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1107, 1104, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.33530  0.04273021  18.13236
  1       0.001  0.50  23.26353  0.04701998  18.06273
  1       0.001  1.00  23.06122  0.04948375  17.94961
  1       0.010  0.25  22.72695  0.04788182  17.82152
  1       0.010  0.50  22.60762  0.04955481  17.76481
  1       0.010  1.00  22.53324  0.05027484  17.74682
  1       0.100  0.25  22.48453  0.05029369  17.73863
  1       0.100  0.50  22.47736  0.05021432  17.73849
  1       0.100  1.00  22.47260  0.05021130  17.73815
  2       0.001  0.25  23.26229  0.04738176  18.06103
  2       0.001  0.50  23.06045  0.04990692  17.94692
  2       0.001  1.00  22.78249  0.04814163  17.84917
  2       0.010  0.25  22.40949  0.06058551  17.64025
  2       0.010  0.50  22.17149  0.06930998  17.52634
  2       0.010  1.00  21.91911  0.08186751  17.36699
  2       0.100  0.25  21.26171  0.12777381  16.72164
  2       0.100  0.50  21.25609  0.12880065  16.72230
  2       0.100  1.00  21.26291  0.12904268  16.72647
  3       0.001  0.25  23.16708  0.05001844  17.98983
  3       0.001  0.50  22.88557  0.04910098  17.88649
  3       0.001  1.00  22.66228  0.04989830  17.78214
  3       0.010  0.25  22.06780  0.07702725  17.45296
  3       0.010  0.50  21.79433  0.09067065  17.27000
  3       0.010  1.00  21.54442  0.10523975  17.09225
  3       0.100  0.25  20.74872  0.16633065  15.98740
  3       0.100  0.50  20.66960  0.16982301  15.89403
  3       0.100  1.00  20.67698  0.16783459  15.90334

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 0.5.


Now processing model 118 of 224 using same rms svmPoly 4 
Support Vector Machines with Polynomial Kernel 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1106, 1106, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.38567  0.03559659  18.17396
  1       0.001  0.50  23.29996  0.04024681  18.12374
  1       0.001  1.00  23.18704  0.04275764  18.03808
  1       0.010  0.25  22.87019  0.04242174  17.93980
  1       0.010  0.50  22.72053  0.04336016  17.88152
  1       0.010  1.00  22.66564  0.04390177  17.86750
  1       0.100  0.25  22.62363  0.04430627  17.85644
  1       0.100  0.50  22.61355  0.04431878  17.85385
  1       0.100  1.00  22.60902  0.04427084  17.85278
  2       0.001  0.25  23.29802  0.04069643  18.12188
  2       0.001  0.50  23.18615  0.04333110  18.03465
  2       0.001  1.00  22.93852  0.04346722  17.95131
  2       0.010  0.25  22.49886  0.05537724  17.73299
  2       0.010  0.50  22.26165  0.06539435  17.60611
  2       0.010  1.00  22.00171  0.07868019  17.43705
  2       0.100  0.25  21.44754  0.12350670  16.69064
  2       0.100  0.50  21.44157  0.12440812  16.69383
  2       0.100  1.00  21.43352  0.12524071  16.69880
  3       0.001  0.25  23.25780  0.04276534  18.07020
  3       0.001  0.50  23.04049  0.04438210  17.97710
  3       0.001  1.00  22.79166  0.04400563  17.90564
  3       0.010  0.25  22.15484  0.07428930  17.52632
  3       0.010  0.50  21.87766  0.08923659  17.32876
  3       0.010  1.00  21.65356  0.10618405  17.11604
  3       0.100  0.25  20.85458  0.16417078  15.97406
  3       0.100  0.50  20.78954  0.16765057  15.90540
  3       0.100  1.00  20.74616  0.17008434  15.86170

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 119 of 224 using max hudgins svmPoly 4 
Support Vector Machines with Polynomial Kernel 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1104, 1106, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.25323  0.02772527  18.04264
  1       0.001  0.50  23.14297  0.02928665  17.95363
  1       0.001  1.00  23.04530  0.03087926  17.89020
  1       0.010  0.25  22.84102  0.04042309  17.79609
  1       0.010  0.50  22.59431  0.05276986  17.69584
  1       0.010  1.00  22.24093  0.06861506  17.52095
  1       0.100  0.25  21.94654  0.08041362  17.36357
  1       0.100  0.50  21.82740  0.08644681  17.29527
  1       0.100  1.00  21.77149  0.08992949  17.24686
  2       0.001  0.25  23.13874  0.03000050  17.94685
  2       0.001  0.50  23.03046  0.03221431  17.87735
  2       0.001  1.00  22.87018  0.03978619  17.79922
  2       0.010  0.25  21.92852  0.09456674  17.27102
  2       0.010  0.50  21.49120  0.11497519  16.92958
  2       0.010  1.00  21.24063  0.12792416  16.70158
  2       0.100  0.25  20.90177  0.14941926  16.46975
  2       0.100  0.50  20.74081  0.16059739  16.34001
  2       0.100  1.00  20.65314  0.16710849  16.25709
  3       0.001  0.25  23.06543  0.03169167  17.90230
  3       0.001  0.50  22.92731  0.03830715  17.81750
  3       0.001  1.00  22.63466  0.05106226  17.68989
  3       0.010  0.25  21.50598  0.11583485  16.91662
  3       0.010  0.50  21.22846  0.12976925  16.67685
  3       0.010  1.00  20.95487  0.14468845  16.47280
  3       0.100  0.25  21.90913  0.13493814  16.49151
  3       0.100  0.50  22.57843  0.13715636  16.55071
  3       0.100  1.00  23.24660  0.13773713  16.64300

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 120 of 224 using same hudgins svmPoly 4 
Support Vector Machines with Polynomial Kernel 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1105, 1107, 1105, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.30069  0.02371481  18.12318
  1       0.001  0.50  23.23277  0.02576240  18.05005
  1       0.001  1.00  23.16876  0.02801425  17.99614
  1       0.010  0.25  23.01372  0.03559608  17.91570
  1       0.010  0.50  22.76818  0.04637366  17.81466
  1       0.010  1.00  22.39213  0.06088420  17.64938
  1       0.100  0.25  22.09239  0.07235088  17.49190
  1       0.100  0.50  21.99432  0.07758865  17.42961
  1       0.100  1.00  21.88897  0.08325662  17.35090
  2       0.001  0.25  23.22892  0.02669178  18.04280
  2       0.001  0.50  23.15619  0.02959705  17.98335
  2       0.001  1.00  23.02527  0.03601156  17.90751
  2       0.010  0.25  22.06774  0.09058769  17.36419
  2       0.010  0.50  21.61757  0.10822453  17.06311
  2       0.010  1.00  21.37804  0.12014958  16.86047
  2       0.100  0.25  20.92949  0.15049163  16.45793
  2       0.100  0.50  20.80184  0.16001176  16.32015
  2       0.100  1.00  20.69853  0.16749278  16.22397
  3       0.001  0.25  23.17838  0.02959743  17.99229
  3       0.001  0.50  23.06134  0.03483139  17.92069
  3       0.001  1.00  22.82181  0.04610382  17.80621
  3       0.010  0.25  21.65319  0.10884169  17.05780
  3       0.010  0.50  21.37239  0.12467602  16.81079
  3       0.010  1.00  21.01752  0.14249109  16.51069
  3       0.100  0.25  21.50417  0.15204424  16.31561
  3       0.100  0.50  22.27416  0.14908970  16.44738
  3       0.100  1.00  23.17966  0.14598719  16.57189

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 121 of 224 using max all ranger 4 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1106, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.23830  0.4178228  12.66568
   2    extratrees  17.65552  0.4083637  13.46417
  13    variance    17.23574  0.4111533  12.36254
  13    extratrees  16.93605  0.4393096  12.35656
  24    variance    17.41554  0.3983790  12.52848
  24    extratrees  16.92784  0.4377380  12.28587

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 122 of 224 using same all ranger 4 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1104, 1104, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.33658  0.4138437  12.73734
   2    extratrees  17.75342  0.4033325  13.50834
  13    variance    17.33315  0.4066300  12.42810
  13    extratrees  17.04112  0.4342454  12.38221
  24    variance    17.52570  0.3927604  12.59595
  24    extratrees  17.01408  0.4338305  12.28533

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 123 of 224 using max du ranger 4 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1103, 1105, 1106, 1105, 1104, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.58854  0.3948244  13.06680
   2    extratrees  18.13588  0.3732102  13.97104
  10    variance    17.41067  0.3997989  12.54892
  10    extratrees  17.36291  0.4114920  12.80447
  18    variance    17.52770  0.3910082  12.62468
  18    extratrees  17.28034  0.4151197  12.64343

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 124 of 224 using same du ranger 4 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1106, 1106, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.63007  0.3950497  13.11009
   2    extratrees  18.17908  0.3724933  13.97301
  10    variance    17.42651  0.4014052  12.58743
  10    extratrees  17.38579  0.4123213  12.79030
  18    variance    17.53758  0.3928793  12.65844
  18    extratrees  17.28693  0.4171257  12.64387

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 125 of 224 using max rms ranger 4 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1107, 1106, 1103, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.47316  0.3934532  12.45261
  2     extratrees  17.30017  0.4146256  12.88643
  3     variance    17.58842  0.3866440  12.41048
  3     extratrees  17.18524  0.4178586  12.62962

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 126 of 224 using same rms ranger 4 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1105, 1105, 1104, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.58651  0.3877628  12.53263
  2     extratrees  17.41328  0.4080739  12.93779
  3     variance    17.68456  0.3818758  12.48423
  3     extratrees  17.32239  0.4098475  12.68359

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 127 of 224 using max hudgins ranger 4 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1107, 1106, 1105, 1104, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.27276  0.3459535  13.82954
   2    extratrees  18.81620  0.3216075  14.73457
   7    variance    17.91837  0.3639738  13.10693
   7    extratrees  18.06888  0.3625956  13.62228
  12    variance    17.88428  0.3654745  12.95153
  12    extratrees  17.88907  0.3728570  13.35658

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 128 of 224 using same hudgins ranger 4 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1105, 1105, 1105, 1104, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.33973  0.3429733  13.89054
   2    extratrees  18.82512  0.3231542  14.67358
   7    variance    17.96245  0.3628984  13.16863
   7    extratrees  18.07657  0.3639785  13.59455
  12    variance    17.96068  0.3616651  13.03767
  12    extratrees  17.88168  0.3752457  13.32666

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 129 of 224 using max all lm 5 
Linear Regression 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1105, 1104, 1106, 1106, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.88085  0.1549232  17.03893

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 130 of 224 using same all lm 5 
Linear Regression 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1105, 1105, 1105, 1104, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.44831  0.1543551  16.67305

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 131 of 224 using max du lm 5 
Linear Regression 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1107, 1105, 1106, 1105, 1105, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.36509  0.1193327  17.34709

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 132 of 224 using same du lm 5 
Linear Regression 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1105, 1105, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.87493  0.1228845  16.96545

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 133 of 224 using max rms lm 5 
Linear Regression 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1104, 1106, 1105, 1105, 1106, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  22.22487  0.04412012  18.38508

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 134 of 224 using same rms lm 5 
Linear Regression 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1105, 1105, 1106, 1105, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.74051  0.04307897  18.03273

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 135 of 224 using max hudgins lm 5 
Linear Regression 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1104, 1105, 1106, 1106, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.56309  0.09944866  17.71666

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 136 of 224 using same hudgins lm 5 
Linear Regression 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1106, 1104, 1105, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.12092  0.0998676  17.36754

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 137 of 224 using max all knn 5 
k-Nearest Neighbors 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1104, 1107, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.37045  0.3597918  12.50857
  7  18.39714  0.3495609  12.93120
  9  18.33510  0.3503502  13.13473

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 138 of 224 using same all knn 5 
k-Nearest Neighbors 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1103, 1106, 1105, 1106, 1106, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.82448  0.3697937  12.01512
  7  17.75323  0.3655580  12.46053
  9  17.78769  0.3600299  12.75975

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 139 of 224 using max du knn 5 
k-Nearest Neighbors 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.47480  0.3537954  12.60439
  7  18.52654  0.3410580  13.05603
  9  18.46384  0.3424123  13.24560

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 140 of 224 using same du knn 5 
k-Nearest Neighbors 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1106, 1106, 1103, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.69365  0.3787420  11.91560
  7  17.69112  0.3707108  12.40171
  9  17.76980  0.3614919  12.75282

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 141 of 224 using max rms knn 5 
k-Nearest Neighbors 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1106, 1106, 1104, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.82337  0.3302341  12.88122
  7  18.81590  0.3216882  13.18484
  9  18.71855  0.3236927  13.41267

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 142 of 224 using same rms knn 5 
k-Nearest Neighbors 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1105, 1106, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.18548  0.3430983  12.46651
  7  18.13129  0.3387870  12.75923
  9  18.10613  0.3369744  13.00047

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 143 of 224 using max hudgins knn 5 
k-Nearest Neighbors 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1104, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.36327  0.1580134  16.12248
  7  21.08181  0.1588914  16.24355
  9  21.01357  0.1555900  16.40282

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 144 of 224 using same hudgins knn 5 
k-Nearest Neighbors 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1106, 1106, 1105, 1104, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  20.97277  0.1579197  15.73661
  7  20.68595  0.1588954  15.85188
  9  20.50217  0.1615585  15.88644

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 145 of 224 using max all svmPoly 5 
Support Vector Machines with Polynomial Kernel 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1104, 1105, 1106, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.28176  0.03725003  17.96179
  1       0.001  0.50  23.17939  0.04019923  17.87884
  1       0.001  1.00  23.02548  0.04545424  17.80767
  1       0.010  0.25  22.68399  0.05838022  17.64101
  1       0.010  0.50  22.36854  0.07261755  17.46034
  1       0.010  1.00  22.13582  0.08376931  17.33421
  1       0.100  0.25  21.90266  0.09696433  17.18033
  1       0.100  0.50  21.75511  0.10642991  17.08444
  1       0.100  1.00  21.64193  0.11721418  16.95216
  2       0.001  0.25  23.12764  0.04472284  17.83743
  2       0.001  0.50  22.85565  0.05448329  17.71059
  2       0.001  1.00  22.47395  0.07191345  17.49912
  2       0.010  0.25  21.35383  0.13957194  16.67408
  2       0.010  0.50  21.10383  0.15436437  16.45326
  2       0.010  1.00  20.84327  0.17162474  16.22509
  2       0.100  0.25  20.17240  0.22259351  15.38353
  2       0.100  0.50  19.99216  0.23491997  15.21363
  2       0.100  1.00  19.86129  0.24429442  15.10329
  3       0.001  0.25  22.86265  0.05713225  17.69712
  3       0.001  0.50  22.46506  0.07540732  17.48404
  3       0.001  1.00  22.07068  0.09674557  17.23456
  3       0.010  0.25  20.94594  0.16726465  16.19743
  3       0.010  0.50  20.63996  0.18845398  15.87073
  3       0.010  1.00  20.41123  0.20416497  15.64563
  3       0.100  0.25  21.09497  0.19856232  15.54843
  3       0.100  0.50  22.04746  0.17983393  15.90509
  3       0.100  1.00  23.95705  0.15321628  16.48190

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 146 of 224 using same all svmPoly 5 
Support Vector Machines with Polynomial Kernel 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1105, 1105, 1105, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.80603  0.04089016  17.58208
  1       0.001  0.50  22.70740  0.04395472  17.49478
  1       0.001  1.00  22.59988  0.04894613  17.42776
  1       0.010  0.25  22.29333  0.06290823  17.26339
  1       0.010  0.50  21.94507  0.07632652  17.10098
  1       0.010  1.00  21.66765  0.08812020  16.95802
  1       0.100  0.25  21.39613  0.10128587  16.79644
  1       0.100  0.50  21.25244  0.11024393  16.67983
  1       0.100  1.00  21.13424  0.12011441  16.53314
  2       0.001  0.25  22.66271  0.04845452  17.45383
  2       0.001  0.50  22.47471  0.05747567  17.34226
  2       0.001  1.00  22.10517  0.07372384  17.15221
  2       0.010  0.25  20.99744  0.13612879  16.42270
  2       0.010  0.50  20.72905  0.15119646  16.18365
  2       0.010  1.00  20.41485  0.17289419  15.88782
  2       0.100  0.25  19.75571  0.22237756  14.97078
  2       0.100  0.50  19.62216  0.23173185  14.86165
  2       0.100  1.00  19.58341  0.23547503  14.82665
  3       0.001  0.25  22.48066  0.05922775  17.33675
  3       0.001  0.50  22.08859  0.07643861  17.13469
  3       0.001  1.00  21.70080  0.09653903  16.91427
  3       0.010  0.25  20.52937  0.16905991  15.86483
  3       0.010  0.50  20.19913  0.19075571  15.49696
  3       0.010  1.00  19.98484  0.20461037  15.28000
  3       0.100  0.25  20.43541  0.20680874  15.10468
  3       0.100  0.50  21.18213  0.19360743  15.36141
  3       0.100  1.00  23.14227  0.16274243  15.94861

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 147 of 224 using max du svmPoly 5 
Support Vector Machines with Polynomial Kernel 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1104, 1106, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.36760  0.02968411  18.02740
  1       0.001  0.50  23.29149  0.03198869  17.92402
  1       0.001  1.00  23.19478  0.03679396  17.84576
  1       0.010  0.25  22.89944  0.04932468  17.70200
  1       0.010  0.50  22.53230  0.06599169  17.51200
  1       0.010  1.00  22.22002  0.08182580  17.34056
  1       0.100  0.25  21.98610  0.09265433  17.21566
  1       0.100  0.50  21.86924  0.09784992  17.13389
  1       0.100  1.00  21.82212  0.10182332  17.04988
  2       0.001  0.25  23.27173  0.03407586  17.90442
  2       0.001  0.50  23.13534  0.04064552  17.80821
  2       0.001  1.00  22.84970  0.05441421  17.64972
  2       0.010  0.25  21.69122  0.12270619  16.93915
  2       0.010  0.50  21.33519  0.13989769  16.66393
  2       0.010  1.00  21.08349  0.15650495  16.47129
  2       0.100  0.25  20.31682  0.20838600  15.70397
  2       0.100  0.50  20.13315  0.22231245  15.50178
  2       0.100  1.00  20.06194  0.22736006  15.41714
  3       0.001  0.25  23.17252  0.04055073  17.82434
  3       0.001  0.50  22.87559  0.05460007  17.65524
  3       0.001  1.00  22.46320  0.07529151  17.43240
  3       0.010  0.25  21.28699  0.14498538  16.57497
  3       0.010  0.50  20.92948  0.16820398  16.28537
  3       0.010  1.00  20.56881  0.19120290  15.95253
  3       0.100  0.25  21.18540  0.19150110  15.75107
  3       0.100  0.50  21.88907  0.17684464  15.99675
  3       0.100  1.00  23.22186  0.15318654  16.46416

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 148 of 224 using same du svmPoly 5 
Support Vector Machines with Polynomial Kernel 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.86568  0.03347570  17.62629
  1       0.001  0.50  22.77968  0.03508815  17.51998
  1       0.001  1.00  22.71217  0.03985794  17.44822
  1       0.010  0.25  22.46725  0.05372267  17.27980
  1       0.010  0.50  22.11415  0.06871750  17.12433
  1       0.010  1.00  21.74038  0.08459419  16.96196
  1       0.100  0.25  21.46361  0.09625391  16.82165
  1       0.100  0.50  21.33777  0.10193132  16.71645
  1       0.100  1.00  21.30502  0.10448941  16.63561
  2       0.001  0.25  22.76457  0.03705012  17.50125
  2       0.001  0.50  22.66595  0.04352239  17.40841
  2       0.001  1.00  22.42909  0.05685525  17.25922
  2       0.010  0.25  21.33122  0.12265283  16.63182
  2       0.010  0.50  20.90166  0.14151084  16.35567
  2       0.010  1.00  20.58350  0.16056102  16.13648
  2       0.100  0.25  19.81810  0.21631732  15.25833
  2       0.100  0.50  19.68529  0.22590258  15.12192
  2       0.100  1.00  19.66606  0.22804479  15.05928
  3       0.001  0.25  22.68779  0.04324838  17.42649
  3       0.001  0.50  22.46699  0.05565093  17.28110
  3       0.001  1.00  22.08146  0.07602806  17.07612
  3       0.010  0.25  20.83115  0.14913955  16.24455
  3       0.010  0.50  20.41714  0.17569565  15.92640
  3       0.010  1.00  20.00967  0.20177778  15.54522
  3       0.100  0.25  20.38620  0.19710866  15.31563
  3       0.100  0.50  20.85640  0.18896535  15.48799
  3       0.100  1.00  21.88352  0.16890753  15.91489

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 149 of 224 using max rms svmPoly 5 
Support Vector Machines with Polynomial Kernel 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1107, 1104, 1104, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.67699  0.03467264  18.27022
  1       0.001  0.50  23.57159  0.03903748  18.18456
  1       0.001  1.00  23.39065  0.04332697  18.07469
  1       0.010  0.25  23.05971  0.04303195  17.94948
  1       0.010  0.50  22.93180  0.04446550  17.87861
  1       0.010  1.00  22.89734  0.04485062  17.86052
  1       0.100  0.25  22.87669  0.04476396  17.85982
  1       0.100  0.50  22.86820  0.04459541  17.86025
  1       0.100  1.00  22.86295  0.04458726  17.85940
  2       0.001  0.25  23.57054  0.03950545  18.18231
  2       0.001  0.50  23.38940  0.04387342  18.07096
  2       0.001  1.00  23.12065  0.04391954  17.97195
  2       0.010  0.25  22.74726  0.05545901  17.74952
  2       0.010  0.50  22.50403  0.06563645  17.60928
  2       0.010  1.00  22.23114  0.07893770  17.43471
  2       0.100  0.25  21.64509  0.12685883  16.67539
  2       0.100  0.50  21.64782  0.12801561  16.65870
  2       0.100  1.00  21.65076  0.12842067  16.65943
  3       0.001  0.25  23.46677  0.04336950  18.11797
  3       0.001  0.50  23.21649  0.04459449  18.00616
  3       0.001  1.00  22.99203  0.04506653  17.90202
  3       0.010  0.25  22.38823  0.07430427  17.51989
  3       0.010  0.50  22.09263  0.08945260  17.32539
  3       0.010  1.00  21.86606  0.10580499  17.10617
  3       0.100  0.25  21.10039  0.16606991  15.95278
  3       0.100  0.50  21.01569  0.17112270  15.85076
  3       0.100  1.00  20.97653  0.17266460  15.81300

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 150 of 224 using same rms svmPoly 5 
Support Vector Machines with Polynomial Kernel 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.08889  0.03524417  17.84379
  1       0.001  0.50  23.04388  0.04058097  17.77726
  1       0.001  1.00  22.92783  0.04335866  17.69844
  1       0.010  0.25  22.61330  0.04355761  17.57222
  1       0.010  0.50  22.45934  0.04429138  17.51427
  1       0.010  1.00  22.42102  0.04476406  17.49366
  1       0.100  0.25  22.41139  0.04460968  17.49136
  1       0.100  0.50  22.40941  0.04458229  17.49179
  1       0.100  1.00  22.40868  0.04458068  17.49325
  2       0.001  0.25  23.04313  0.04108639  17.77513
  2       0.001  0.50  22.92474  0.04402087  17.69469
  2       0.001  1.00  22.68044  0.04442103  17.59226
  2       0.010  0.25  22.28908  0.05571583  17.37870
  2       0.010  0.50  22.02200  0.06701551  17.23458
  2       0.010  1.00  21.75549  0.08032349  17.07202
  2       0.100  0.25  21.26012  0.12164857  16.35527
  2       0.100  0.50  21.26664  0.12271958  16.34207
  2       0.100  1.00  21.27188  0.12339547  16.33653
  3       0.001  0.25  22.97944  0.04411414  17.72548
  3       0.001  0.50  22.78866  0.04500399  17.62747
  3       0.001  1.00  22.54356  0.04512559  17.53376
  3       0.010  0.25  21.92759  0.07542240  17.15705
  3       0.010  0.50  21.64025  0.09075547  16.96949
  3       0.010  1.00  21.42986  0.10690543  16.75316
  3       0.100  0.25  20.60495  0.16888174  15.58575
  3       0.100  0.50  20.51769  0.17322381  15.50527
  3       0.100  1.00  20.49778  0.17418516  15.49206

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 151 of 224 using max hudgins svmPoly 5 
Support Vector Machines with Polynomial Kernel 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1105, 1104, 1104, 1106, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.47061  0.02434354  18.16475
  1       0.001  0.50  23.40612  0.02489474  18.06822
  1       0.001  1.00  23.34635  0.02716713  17.98610
  1       0.010  0.25  23.21568  0.03503634  17.89296
  1       0.010  0.50  23.01924  0.04497754  17.79804
  1       0.010  1.00  22.63224  0.05900673  17.63447
  1       0.100  0.25  22.26626  0.07373203  17.43367
  1       0.100  0.50  22.16769  0.07993806  17.36050
  1       0.100  1.00  22.12755  0.08248669  17.32090
  2       0.001  0.25  23.40144  0.02554033  18.05964
  2       0.001  0.50  23.33182  0.02839837  17.97145
  2       0.001  1.00  23.22352  0.03510311  17.88877
  2       0.010  0.25  22.26658  0.08658544  17.36906
  2       0.010  0.50  21.82446  0.10673880  17.07674
  2       0.010  1.00  21.54614  0.12245572  16.84416
  2       0.100  0.25  21.13411  0.15135473  16.46684
  2       0.100  0.50  20.98182  0.16315770  16.31428
  2       0.100  1.00  20.84256  0.17297365  16.18667
  3       0.001  0.25  23.35092  0.02779159  17.99326
  3       0.001  0.50  23.25279  0.03366462  17.90467
  3       0.001  1.00  23.03193  0.04520810  17.78108
  3       0.010  0.25  21.84593  0.10738180  17.05060
  3       0.010  0.50  21.55209  0.12451632  16.79368
  3       0.010  1.00  21.30008  0.13760725  16.60272
  3       0.100  0.25  21.33407  0.15984021  16.16527
  3       0.100  0.50  21.69015  0.16246169  16.15894
  3       0.100  1.00  22.33989  0.15613395  16.32205

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 152 of 224 using same hudgins svmPoly 5 
Support Vector Machines with Polynomial Kernel 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1105, 1105, 1104, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.97227  0.02699534  17.77025
  1       0.001  0.50  22.89862  0.02814171  17.67149
  1       0.001  1.00  22.83612  0.03031464  17.58879
  1       0.010  0.25  22.75716  0.03785527  17.51053
  1       0.010  0.50  22.57526  0.04774554  17.41377
  1       0.010  1.00  22.22599  0.06179456  17.25734
  1       0.100  0.25  21.83367  0.07566470  17.08081
  1       0.100  0.50  21.68458  0.08259406  16.99356
  1       0.100  1.00  21.63312  0.08520842  16.96813
  2       0.001  0.25  22.89261  0.02882299  17.66223
  2       0.001  0.50  22.82256  0.03164158  17.57613
  2       0.001  1.00  22.75496  0.03821542  17.50810
  2       0.010  0.25  21.92537  0.08743498  17.04462
  2       0.010  0.50  21.44502  0.10773300  16.75806
  2       0.010  1.00  21.14940  0.12270480  16.52067
  2       0.100  0.25  20.69656  0.15321622  16.10186
  2       0.100  0.50  20.51032  0.16595436  15.93027
  2       0.100  1.00  20.38063  0.17556394  15.79673
  3       0.001  0.25  22.83914  0.03140520  17.59509
  3       0.001  0.50  22.76964  0.03679189  17.52246
  3       0.001  1.00  22.61054  0.04782251  17.41399
  3       0.010  0.25  21.48221  0.10892269  16.71222
  3       0.010  0.50  21.13082  0.12649492  16.46381
  3       0.010  1.00  20.86193  0.13954848  16.25187
  3       0.100  0.25  20.90001  0.15888346  15.89156
  3       0.100  0.50  20.84912  0.17040481  15.71732
  3       0.100  1.00  20.96948  0.17038195  15.73057

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 153 of 224 using max all ranger 5 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1107, 1104, 1106, 1104, 1105, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.22603  0.4343677  12.61273
   2    extratrees  17.67291  0.4249985  13.38391
  13    variance    17.26627  0.4241374  12.37444
  13    extratrees  16.86947  0.4587930  12.19835
  24    variance    17.45704  0.4107673  12.57320
  24    extratrees  16.84090  0.4584383  12.11949

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 154 of 224 using same all ranger 5 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1104, 1105, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.76409  0.4394437  12.21701
   2    extratrees  17.20099  0.4314648  13.02168
  13    variance    16.70140  0.4364775  11.92483
  13    extratrees  16.39676  0.4651347  11.82715
  24    variance    16.93386  0.4196252  12.14491
  24    extratrees  16.35165  0.4656391  11.73798

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 155 of 224 using max du ranger 5 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1106, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.50628  0.4165460  12.97604
   2    extratrees  18.07341  0.3966885  13.83874
  10    variance    17.29289  0.4225445  12.46801
  10    extratrees  17.19471  0.4388359  12.61739
  18    variance    17.43688  0.4119958  12.59001
  18    extratrees  17.07957  0.4439147  12.43288

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 156 of 224 using same du ranger 5 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.03709  0.4215865  12.56722
   2    extratrees  17.60291  0.4026553  13.45876
  10    variance    16.83669  0.4272893  12.06250
  10    extratrees  16.69712  0.4469821  12.20695
  18    variance    16.99884  0.4152545  12.18555
  18    extratrees  16.60395  0.4503150  12.03937

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 157 of 224 using max rms ranger 5 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.63405  0.3974269  12.52398
  2     extratrees  17.48370  0.4148660  12.92945
  3     variance    17.73049  0.3920222  12.48277
  3     extratrees  17.38691  0.4172521  12.68407

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 158 of 224 using same rms ranger 5 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1106, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.16883  0.4022569  12.07303
  2     extratrees  17.03324  0.4191829  12.52951
  3     variance    17.25746  0.3972530  12.02362
  3     extratrees  16.94502  0.4207575  12.27431

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 159 of 224 using max hudgins ranger 5 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1103, 1106, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.30626  0.3598384  13.80841
   2    extratrees  18.85341  0.3383966  14.64634
   7    variance    17.90493  0.3805342  13.11588
   7    extratrees  18.00687  0.3848531  13.50563
  12    variance    17.96278  0.3751843  13.06605
  12    extratrees  17.82102  0.3948180  13.25169

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 160 of 224 using same hudgins ranger 5 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.84746  0.3634340  13.41762
   2    extratrees  18.38736  0.3420964  14.29518
   7    variance    17.42233  0.3875901  12.70993
   7    extratrees  17.52225  0.3909477  13.11433
  12    variance    17.45066  0.3845272  12.62545
  12    extratrees  17.32648  0.4018088  12.82945

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 161 of 224 using max all lm 6 
Linear Regression 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1107, 1105, 1104, 1104, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.40219  0.1721334  16.58944

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 162 of 224 using same all lm 6 
Linear Regression 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1105, 1106, 1105, 1107, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.73005  0.1530879  16.95705

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 163 of 224 using max du lm 6 
Linear Regression 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1105, 1105, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.85743  0.1377784  16.92874

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 164 of 224 using same du lm 6 
Linear Regression 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1104, 1105, 1105, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  21.1103  0.1262541  17.17155

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 165 of 224 using max rms lm 6 
Linear Regression 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1107, 1105, 1105, ... 
Resampling results:

  RMSE      Rsquared    MAE     
  21.85164  0.05364807  18.09395

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 166 of 224 using same rms lm 6 
Linear Regression 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1104, 1106, 1106, 1105, 1105, ... 
Resampling results:

  RMSE     Rsquared   MAE     
  21.9801  0.0508817  18.17215

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 167 of 224 using max hudgins lm 6 
Linear Regression 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1106, 1106, 1104, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.09591  0.1178005  17.34481

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 168 of 224 using same hudgins lm 6 
Linear Regression 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1105, 1106, 1106, ... 
Resampling results:

  RMSE     Rsquared   MAE    
  21.3071  0.1052928  17.5009

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 169 of 224 using max all knn 6 
k-Nearest Neighbors 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1107, 1104, 1105, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.99877  0.3711442  12.15072
  7  17.92207  0.3683619  12.50432
  9  17.99758  0.3594763  12.80842

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 170 of 224 using same all knn 6 
k-Nearest Neighbors 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1106, 1104, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.24784  0.3596728  12.53519
  7  18.33769  0.3467483  12.91099
  9  18.32179  0.3422565  13.12552

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 171 of 224 using max du knn 6 
k-Nearest Neighbors 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1105, 1106, 1106, 1104, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.99301  0.3714823  12.12454
  7  17.94716  0.3664483  12.48738
  9  17.97131  0.3606002  12.77031

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 172 of 224 using same du knn 6 
k-Nearest Neighbors 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1105, 1107, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.22255  0.3596628  12.50962
  7  18.29324  0.3480889  12.87967
  9  18.27755  0.3439894  13.11478

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 173 of 224 using max rms knn 6 
k-Nearest Neighbors 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1106, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.87860  0.3748103  12.19322
  7  17.93834  0.3647469  12.58888
  9  18.07063  0.3534102  12.89834

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 174 of 224 using same rms knn 6 
k-Nearest Neighbors 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.15166  0.3615672  12.55862
  7  18.19709  0.3521909  12.95013
  9  18.30238  0.3420437  13.29951

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 175 of 224 using max hudgins knn 6 
k-Nearest Neighbors 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1106, 1106, 1106, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.20609  0.1581038  15.90100
  7  20.80502  0.1662879  15.89387
  9  20.71156  0.1626561  16.02314

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 176 of 224 using same hudgins knn 6 
k-Nearest Neighbors 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.30911  0.1517679  16.08187
  7  20.90984  0.1598426  15.99687
  9  20.85912  0.1542429  16.18110

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 177 of 224 using max all svmPoly 6 
Support Vector Machines with Polynomial Kernel 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1105, 1106, 1104, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  22.90176  0.04417926  17.69293
  1       0.001  0.50  22.70135  0.04769489  17.60247
  1       0.001  1.00  22.51842  0.05515060  17.50108
  1       0.010  0.25  22.23161  0.06917290  17.32920
  1       0.010  0.50  21.99612  0.08212929  17.16169
  1       0.010  1.00  21.75206  0.09448492  17.02412
  1       0.100  0.25  21.43023  0.11111277  16.82176
  1       0.100  0.50  21.26557  0.12282693  16.69280
  1       0.100  1.00  21.13546  0.13458228  16.49571
  2       0.001  0.25  22.62951  0.05254987  17.55795
  2       0.001  0.50  22.36885  0.06480442  17.40276
  2       0.001  1.00  22.04014  0.08185884  17.20196
  2       0.010  0.25  20.98886  0.14848935  16.41373
  2       0.010  0.50  20.71266  0.16450377  16.16243
  2       0.010  1.00  20.41836  0.18492751  15.89035
  2       0.100  0.25  19.75603  0.23662378  14.89165
  2       0.100  0.50  19.60422  0.24641789  14.75680
  2       0.100  1.00  19.48002  0.25482524  14.64144
  3       0.001  0.25  22.38751  0.06577620  17.41588
  3       0.001  0.50  22.02774  0.08433832  17.19659
  3       0.001  1.00  21.66070  0.10666962  16.92554
  3       0.010  0.25  20.54075  0.17923245  15.87616
  3       0.010  0.50  20.27476  0.19820613  15.54121
  3       0.010  1.00  20.03255  0.21503288  15.25771
  3       0.100  0.25  20.61079  0.21545902  15.01016
  3       0.100  0.50  20.92667  0.22023846  14.99233
  3       0.100  1.00  22.45929  0.19947768  15.49722

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 178 of 224 using same all svmPoly 6 
Support Vector Machines with Polynomial Kernel 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1104, 1106, 1106, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.00013  0.04162583  17.73424
  1       0.001  0.50  22.82612  0.04504684  17.64762
  1       0.001  1.00  22.68011  0.05145708  17.55182
  1       0.010  0.25  22.43021  0.06293564  17.41391
  1       0.010  0.50  22.16087  0.07520248  17.26405
  1       0.010  1.00  21.96063  0.08666245  17.12908
  1       0.100  0.25  21.72581  0.09991575  16.97535
  1       0.100  0.50  21.56636  0.11018410  16.87068
  1       0.100  1.00  21.46344  0.12027971  16.73184
  2       0.001  0.25  22.76311  0.04904966  17.60522
  2       0.001  0.50  22.52255  0.06031207  17.45244
  2       0.001  1.00  22.24928  0.07327004  17.29962
  2       0.010  0.25  21.29360  0.12896369  16.60177
  2       0.010  0.50  21.05220  0.14302293  16.37245
  2       0.010  1.00  20.79468  0.16125053  16.13011
  2       0.100  0.25  20.12227  0.21449935  15.26853
  2       0.100  0.50  19.98080  0.22364382  15.14949
  2       0.100  1.00  19.89598  0.22993999  15.08396
  3       0.001  0.25  22.53588  0.06027391  17.46880
  3       0.001  0.50  22.21816  0.07580578  17.28707
  3       0.001  1.00  21.91881  0.09325746  17.07263
  3       0.010  0.25  20.89344  0.15786008  16.09844
  3       0.010  0.50  20.62166  0.17717691  15.81543
  3       0.010  1.00  20.38062  0.19359820  15.57492
  3       0.100  0.25  21.09235  0.19018530  15.65857
  3       0.100  0.50  21.87886  0.18184817  15.89531
  3       0.100  1.00  23.39860  0.16625398  16.31819

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 179 of 224 using max du svmPoly 6 
Support Vector Machines with Polynomial Kernel 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1106, 1104, 1105, 1104, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.04163  0.03793801  17.76516
  1       0.001  0.50  22.89234  0.04078620  17.64591
  1       0.001  1.00  22.73657  0.04676773  17.55593
  1       0.010  0.25  22.42365  0.06254361  17.37779
  1       0.010  0.50  22.14296  0.07794682  17.20964
  1       0.010  1.00  21.85842  0.09295600  17.03771
  1       0.100  0.25  21.53599  0.10637736  16.86879
  1       0.100  0.50  21.37790  0.11368269  16.75131
  1       0.100  1.00  21.35239  0.11713534  16.62265
  2       0.001  0.25  22.86714  0.04271232  17.62827
  2       0.001  0.50  22.67262  0.05104190  17.51345
  2       0.001  1.00  22.37805  0.06627085  17.34122
  2       0.010  0.25  21.32700  0.12911294  16.64388
  2       0.010  0.50  20.95455  0.15004237  16.38340
  2       0.010  1.00  20.66583  0.16908612  16.15136
  2       0.100  0.25  19.94445  0.21975566  15.31614
  2       0.100  0.50  19.82643  0.22841555  15.18348
  2       0.100  1.00  19.77151  0.23261873  15.11528
  3       0.001  0.25  22.70315  0.04999105  17.53780
  3       0.001  0.50  22.42658  0.06496020  17.36033
  3       0.001  1.00  22.04313  0.08677729  17.12720
  3       0.010  0.25  20.88735  0.15626523  16.26902
  3       0.010  0.50  20.54533  0.17881959  15.97983
  3       0.010  1.00  20.19549  0.20040683  15.62677
  3       0.100  0.25  20.58696  0.19909541  15.51197
  3       0.100  0.50  20.97425  0.19497249  15.58300
  3       0.100  1.00  21.54223  0.19017986  15.71908

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 180 of 224 using same du svmPoly 6 
Support Vector Machines with Polynomial Kernel 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1106, 1106, 1104, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.12689  0.03651305  17.80272
  1       0.001  0.50  22.99375  0.03929793  17.68613
  1       0.001  1.00  22.84163  0.04522612  17.58503
  1       0.010  0.25  22.60446  0.05840073  17.43452
  1       0.010  0.50  22.32848  0.07257487  17.27833
  1       0.010  1.00  22.01462  0.08731972  17.12443
  1       0.100  0.25  21.77969  0.09813463  16.99065
  1       0.100  0.50  21.65515  0.10345694  16.91968
  1       0.100  1.00  21.60784  0.10666001  16.83914
  2       0.001  0.25  22.97311  0.04088454  17.66911
  2       0.001  0.50  22.78020  0.04907584  17.54285
  2       0.001  1.00  22.54497  0.06217498  17.39911
  2       0.010  0.25  21.57830  0.11653076  16.81323
  2       0.010  0.50  21.23660  0.13379484  16.55254
  2       0.010  1.00  20.97421  0.14904058  16.34537
  2       0.100  0.25  20.26099  0.20140346  15.59576
  2       0.100  0.50  20.11707  0.21145759  15.45756
  2       0.100  1.00  20.11531  0.21202689  15.44965
  3       0.001  0.25  22.82512  0.04754251  17.57819
  3       0.001  0.50  22.57189  0.06099511  17.41398
  3       0.001  1.00  22.25408  0.07873228  17.22664
  3       0.010  0.25  21.21069  0.13684079  16.46289
  3       0.010  0.50  20.89430  0.15704555  16.21625
  3       0.010  1.00  20.54064  0.18021620  15.90531
  3       0.100  0.25  21.11813  0.17810040  15.96481
  3       0.100  0.50  21.50023  0.17644622  16.03171
  3       0.100  1.00  22.34396  0.16717391  16.30830

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 181 of 224 using max rms svmPoly 6 
Support Vector Machines with Polynomial Kernel 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1105, 1105, 1104, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.29203  0.04570712  18.05576
  1       0.001  0.50  23.23451  0.04945896  17.96606
  1       0.001  1.00  23.01543  0.05156054  17.83476
  1       0.010  0.25  22.63397  0.05144448  17.67329
  1       0.010  0.50  22.53305  0.05274496  17.60218
  1       0.010  1.00  22.49186  0.05277138  17.59423
  1       0.100  0.25  22.46034  0.05245817  17.59715
  1       0.100  0.50  22.44882  0.05243789  17.59922
  1       0.100  1.00  22.44588  0.05244478  17.60052
  2       0.001  0.25  23.23352  0.04991018  17.96379
  2       0.001  0.50  23.01297  0.05205591  17.83083
  2       0.001  1.00  22.69347  0.05180340  17.70342
  2       0.010  0.25  22.32090  0.06418392  17.47727
  2       0.010  0.50  22.09152  0.07370339  17.34709
  2       0.010  1.00  21.87397  0.08548554  17.18472
  2       0.100  0.25  21.35371  0.12771868  16.46400
  2       0.100  0.50  21.33485  0.13034225  16.45347
  2       0.100  1.00  21.32956  0.13120475  16.45587
  3       0.001  0.25  23.12709  0.05259701  17.88436
  3       0.001  0.50  22.80729  0.05284856  17.74333
  3       0.001  1.00  22.56935  0.05344506  17.62848
  3       0.010  0.25  22.00001  0.08109833  17.26463
  3       0.010  0.50  21.74456  0.09460298  17.07910
  3       0.010  1.00  21.48679  0.11130803  16.83498
  3       0.100  0.25  20.57954  0.18407971  15.56911
  3       0.100  0.50  20.48156  0.18778725  15.44912
  3       0.100  1.00  20.43372  0.18915059  15.40433

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 182 of 224 using same rms svmPoly 6 
Support Vector Machines with Polynomial Kernel 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1106, 1105, 1104, 1107, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.36524  0.04292619  18.08333
  1       0.001  0.50  23.30005  0.04604945  18.00801
  1       0.001  1.00  23.13412  0.04734002  17.89279
  1       0.010  0.25  22.76172  0.04625408  17.75261
  1       0.010  0.50  22.64772  0.04786519  17.69147
  1       0.010  1.00  22.60689  0.04779236  17.68178
  1       0.100  0.25  22.57484  0.04780799  17.68168
  1       0.100  0.50  22.56279  0.04779466  17.68249
  1       0.100  1.00  22.55682  0.04780089  17.68281
  2       0.001  0.25  23.29816  0.04647150  18.00580
  2       0.001  0.50  23.13305  0.04779214  17.88935
  2       0.001  1.00  22.83930  0.04646267  17.77824
  2       0.010  0.25  22.43353  0.05921149  17.55540
  2       0.010  0.50  22.22242  0.06797527  17.43274
  2       0.010  1.00  22.02459  0.07841092  17.27989
  2       0.100  0.25  21.66765  0.11228765  16.67135
  2       0.100  0.50  21.66834  0.11397837  16.65354
  2       0.100  1.00  21.67247  0.11450716  16.65095
  3       0.001  0.25  23.22317  0.04813963  17.93688
  3       0.001  0.50  22.94733  0.04765825  17.80805
  3       0.001  1.00  22.69101  0.04851158  17.70849
  3       0.010  0.25  22.14380  0.07448806  17.35538
  3       0.010  0.50  21.93024  0.08533126  17.19474
  3       0.010  1.00  21.74309  0.09756850  17.00854
  3       0.100  0.25  20.95967  0.16366788  15.83094
  3       0.100  0.50  20.86329  0.16838459  15.72829
  3       0.100  1.00  20.80186  0.17087938  15.67724

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 183 of 224 using max hudgins svmPoly 6 
Support Vector Machines with Polynomial Kernel 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1104, 1105, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.18660  0.02786247  17.93843
  1       0.001  0.50  23.05406  0.02837333  17.80589
  1       0.001  1.00  22.94584  0.03190265  17.71036
  1       0.010  0.25  22.71381  0.04199761  17.60424
  1       0.010  0.50  22.50455  0.05487275  17.48610
  1       0.010  1.00  22.18802  0.06968394  17.33670
  1       0.100  0.25  21.86845  0.08571456  17.13186
  1       0.100  0.50  21.75155  0.09248421  17.04960
  1       0.100  1.00  21.68285  0.09639581  16.98184
  2       0.001  0.25  23.04919  0.02908279  17.79734
  2       0.001  0.50  22.92922  0.03322057  17.69703
  2       0.001  1.00  22.72373  0.04128502  17.60818
  2       0.010  0.25  21.81695  0.09475785  17.09494
  2       0.010  0.50  21.43653  0.11558945  16.76707
  2       0.010  1.00  21.20052  0.13006885  16.59139
  2       0.100  0.25  20.75406  0.16247822  16.07986
  2       0.100  0.50  20.64904  0.17085590  15.97597
  2       0.100  1.00  20.55939  0.17766128  15.89043
  3       0.001  0.25  22.97275  0.03189819  17.72762
  3       0.001  0.50  22.78054  0.03908874  17.63116
  3       0.001  1.00  22.52391  0.05328760  17.49262
  3       0.010  0.25  21.44058  0.11736518  16.74106
  3       0.010  0.50  21.17814  0.13320695  16.52980
  3       0.010  1.00  20.92219  0.14736354  16.30802
  3       0.100  0.25  20.83197  0.17267798  15.87086
  3       0.100  0.50  20.98278  0.17558170  15.84172
  3       0.100  1.00  21.49575  0.16949118  15.97718

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 184 of 224 using same hudgins svmPoly 6 
Support Vector Machines with Polynomial Kernel 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1106, 1104, 1105, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.24554  0.03109376  17.97030
  1       0.001  0.50  23.13572  0.03144302  17.84354
  1       0.001  1.00  23.04627  0.03441208  17.75477
  1       0.010  0.25  22.87045  0.04210076  17.65842
  1       0.010  0.50  22.68589  0.05276021  17.55392
  1       0.010  1.00  22.41413  0.06503952  17.41574
  1       0.100  0.25  22.07282  0.07997489  17.22917
  1       0.100  0.50  21.95203  0.08713429  17.12729
  1       0.100  1.00  21.88932  0.09046923  17.07935
  2       0.001  0.25  23.13183  0.03192700  17.83406
  2       0.001  0.50  23.03182  0.03562265  17.74257
  2       0.001  1.00  22.87469  0.04183274  17.65628
  2       0.010  0.25  22.11380  0.08493231  17.22846
  2       0.010  0.50  21.72637  0.10416096  16.93680
  2       0.010  1.00  21.47361  0.11795462  16.71411
  2       0.100  0.25  21.03049  0.14764440  16.30056
  2       0.100  0.50  20.90159  0.15774807  16.19496
  2       0.100  1.00  20.78328  0.16684540  16.09645
  3       0.001  0.25  23.05568  0.03516189  17.75606
  3       0.001  0.50  22.92058  0.04031713  17.67679
  3       0.001  1.00  22.70064  0.05201038  17.54788
  3       0.010  0.25  21.71267  0.10653913  16.85888
  3       0.010  0.50  21.47953  0.11825109  16.65969
  3       0.010  1.00  21.25920  0.12916876  16.50217
  3       0.100  0.25  21.23245  0.15158978  16.33438
  3       0.100  0.50  21.46999  0.15725578  16.34265
  3       0.100  1.00  21.98467  0.15654150  16.44886

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 185 of 224 using max all ranger 6 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1107, 1106, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.74922  0.4533007  12.12164
   2    extratrees  17.13465  0.4475759  12.90253
  13    variance    16.68230  0.4501216  11.79850
  13    extratrees  16.35954  0.4797930  11.74127
  24    variance    16.84385  0.4389477  11.98637
  24    extratrees  16.35015  0.4782097  11.67860

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 186 of 224 using same all ranger 6 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1106, 1106, 1106, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.94202  0.4450532  12.36594
   2    extratrees  17.41515  0.4345848  13.18089
  13    variance    16.86992  0.4417233  12.04205
  13    extratrees  16.60922  0.4676721  12.03685
  24    variance    17.05509  0.4286956  12.23790
  24    extratrees  16.60029  0.4654424  11.96723

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 187 of 224 using max du ranger 6 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1104, 1105, 1105, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.04152  0.4335451  12.52063
   2    extratrees  17.56751  0.4171891  13.36305
  10    variance    16.77817  0.4427523  11.95776
  10    extratrees  16.67619  0.4599148  12.15183
  18    variance    16.84921  0.4374756  12.00375
  18    extratrees  16.57437  0.4640742  11.98428

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 188 of 224 using same du ranger 6 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1105, 1107, 1104, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.31353  0.4192994  12.77828
   2    extratrees  17.90981  0.3982501  13.65960
  10    variance    17.05912  0.4281886  12.25057
  10    extratrees  17.03603  0.4396844  12.48980
  18    variance    17.16759  0.4200337  12.33850
  18    extratrees  16.93710  0.4435378  12.33614

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 189 of 224 using max rms ranger 6 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1105, 1105, 1105, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.03253  0.4229619  12.02366
  2     extratrees  16.89546  0.4402981  12.38699
  3     variance    17.12143  0.4179210  11.96204
  3     extratrees  16.81996  0.4405917  12.14498

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 190 of 224 using same rms ranger 6 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1106, 1106, 1105, 1104, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.25670  0.4125135  12.26773
  2     extratrees  17.13166  0.4312560  12.68782
  3     variance    17.35797  0.4065365  12.20391
  3     extratrees  17.03485  0.4324389  12.45046

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 191 of 224 using max hudgins ranger 6 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1105, 1105, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.88945  0.3728022  13.37296
   2    extratrees  18.34612  0.3591842  14.17782
   7    variance    17.48345  0.3948068  12.65421
   7    extratrees  17.50985  0.4041834  13.05382
  12    variance    17.46255  0.3950831  12.52926
  12    extratrees  17.35405  0.4114812  12.81759

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = extratrees
 and min.node.size = 5.


Now processing model 192 of 224 using same hudgins ranger 6 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1104, 1106, 1104, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.11635  0.3616993  13.59665
   2    extratrees  18.64933  0.3424657  14.43476
   7    variance    17.67454  0.3860997  12.89570
   7    extratrees  17.82725  0.3866688  13.33790
  12    variance    17.62240  0.3886824  12.76589
  12    extratrees  17.66255  0.3946961  13.09947

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 193 of 224 using max all lm 7 
Linear Regression 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1105, 1105, 1106, 1105, 1106, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  20.94903  0.1417027  17.13063

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 194 of 224 using same all lm 7 
Linear Regression 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1105, 1106, 1105, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  20.86717  0.147796  17.08571

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 195 of 224 using max du lm 7 
Linear Regression 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1105, 1106, 1107, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.28776  0.1138901  17.35523

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 196 of 224 using same du lm 7 
Linear Regression 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1104, 1105, 1104, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE    
  21.33105  0.1131658  17.3496

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 197 of 224 using max rms lm 7 
Linear Regression 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1106, 1105, 1106, 1105, 1106, ... 
Resampling results:

  RMSE     Rsquared    MAE     
  22.0889  0.04039453  18.43787

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 198 of 224 using same rms lm 7 
Linear Regression 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1107, 1104, 1106, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE   
  22.15878  0.0377736  18.422

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 199 of 224 using max hudgins lm 7 
Linear Regression 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1106, 1105, 1105, 1105, ... 
Resampling results:

  RMSE      Rsquared   MAE     
  21.35649  0.1022292  17.70732

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 200 of 224 using same hudgins lm 7 
Linear Regression 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1105, 1106, 1107, ... 
Resampling results:

  RMSE      Rsquared  MAE     
  21.38604  0.103924  17.64551

Tuning parameter 'intercept' was held constant at a value of TRUE


Now processing model 201 of 224 using max all knn 7 
k-Nearest Neighbors 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1104, 1106, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.91856  0.3763709  12.19444
  7  17.95372  0.3666848  12.73951
  9  17.98315  0.3621545  13.00440

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 202 of 224 using same all knn 7 
k-Nearest Neighbors 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1105, 1105, 1104, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.35908  0.3516744  12.51895
  7  18.33093  0.3444517  12.95085
  9  18.38708  0.3378350  13.24264

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.


Now processing model 203 of 224 using max du knn 7 
k-Nearest Neighbors 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1105, 1105, 1105, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  17.80760  0.3839459  12.11212
  7  17.89881  0.3714735  12.68685
  9  17.97891  0.3631079  12.99679

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 204 of 224 using same du knn 7 
k-Nearest Neighbors 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1106, 1105, 1106, 1104, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.39542  0.3480095  12.50905
  7  18.37206  0.3418383  12.96033
  9  18.37141  0.3389409  13.24089

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 205 of 224 using max rms knn 7 
k-Nearest Neighbors 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1105, 1104, 1105, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.40925  0.3458689  12.65255
  7  18.43524  0.3366809  13.04095
  9  18.42149  0.3341677  13.31064

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 206 of 224 using same rms knn 7 
k-Nearest Neighbors 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1104, 1106, 1105, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  18.44135  0.3438085  12.69280
  7  18.55213  0.3291822  13.17972
  9  18.64759  0.3194795  13.50904

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.


Now processing model 207 of 224 using max hudgins knn 7 
k-Nearest Neighbors 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1104, 1106, 1105, 1106, 1106, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.09236  0.1665263  15.95147
  7  20.78914  0.1681839  15.96068
  9  20.68946  0.1675014  15.99788

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 208 of 224 using same hudgins knn 7 
k-Nearest Neighbors 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1104, 1105, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE     
  5  21.20978  0.1592946  16.06848
  7  20.91099  0.1629898  16.04070
  9  20.77863  0.1643475  16.09280

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.


Now processing model 209 of 224 using max all svmPoly 7 
Support Vector Machines with Polynomial Kernel 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1107, 1105, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.21823  0.03321818  17.89998
  1       0.001  0.50  23.07749  0.03541688  17.81799
  1       0.001  1.00  22.97417  0.04062957  17.72660
  1       0.010  0.25  22.78665  0.05087004  17.61375
  1       0.010  0.50  22.50024  0.06334944  17.48795
  1       0.010  1.00  22.19925  0.07578698  17.37105
  1       0.100  0.25  21.94704  0.09074780  17.22789
  1       0.100  0.50  21.75590  0.10352630  17.05882
  1       0.100  1.00  21.67107  0.10985726  16.86976
  2       0.001  0.25  23.02226  0.04003053  17.75686
  2       0.001  0.50  22.84599  0.04948571  17.62147
  2       0.001  1.00  22.54517  0.06525171  17.44921
  2       0.010  0.25  21.46822  0.13128230  16.60458
  2       0.010  0.50  21.14857  0.15137926  16.36806
  2       0.010  1.00  20.77178  0.17567184  16.05656
  2       0.100  0.25  19.96024  0.23230850  15.13059
  2       0.100  0.50  19.74920  0.24530370  14.99791
  2       0.100  1.00  19.57432  0.25471295  14.89276
  3       0.001  0.25  22.84933  0.05127211  17.61918
  3       0.001  0.50  22.50306  0.06934978  17.42476
  3       0.001  1.00  22.11264  0.09213040  17.16992
  3       0.010  0.25  20.84047  0.17547291  15.96077
  3       0.010  0.50  20.56580  0.19096471  15.67295
  3       0.010  1.00  20.33813  0.20431203  15.48215
  3       0.100  0.25  21.49111  0.18330373  15.77436
  3       0.100  0.50  23.12192  0.16348542  16.12268
  3       0.100  1.00  25.75757  0.13925094  16.70395

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 210 of 224 using same all svmPoly 7 
Support Vector Machines with Polynomial Kernel 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1106, 1105, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.26414  0.03458866  17.83764
  1       0.001  0.50  23.15572  0.03745335  17.75984
  1       0.001  1.00  23.05222  0.04298625  17.67782
  1       0.010  0.25  22.87309  0.05413270  17.57521
  1       0.010  0.50  22.60100  0.06714902  17.45929
  1       0.010  1.00  22.26337  0.08056796  17.33453
  1       0.100  0.25  21.96011  0.09563773  17.19050
  1       0.100  0.50  21.80386  0.10642263  17.04879
  1       0.100  1.00  21.68982  0.11383613  16.86384
  2       0.001  0.25  23.11774  0.04213746  17.71555
  2       0.001  0.50  22.94976  0.05099705  17.58774
  2       0.001  1.00  22.69388  0.06550372  17.45163
  2       0.010  0.25  21.66669  0.12177032  16.75842
  2       0.010  0.50  21.32039  0.14031165  16.50551
  2       0.010  1.00  20.97242  0.16124732  16.22365
  2       0.100  0.25  20.18905  0.21518783  15.27234
  2       0.100  0.50  19.97880  0.22804377  15.10290
  2       0.100  1.00  19.87795  0.23503280  15.04319
  3       0.001  0.25  22.96660  0.05262355  17.60001
  3       0.001  0.50  22.66352  0.06761356  17.44141
  3       0.001  1.00  22.24731  0.08966685  17.22283
  3       0.010  0.25  21.06641  0.16003201  16.12398
  3       0.010  0.50  20.73962  0.18000120  15.80637
  3       0.010  1.00  20.46691  0.19642082  15.57293
  3       0.100  0.25  21.60314  0.18173275  15.76016
  3       0.100  0.50  23.18902  0.16456365  16.08885
  3       0.100  1.00  25.93373  0.14201879  16.65916

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 211 of 224 using max du svmPoly 7 
Support Vector Machines with Polynomial Kernel 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1104, 1106, 1106, 1106, 1103, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.26707  0.02940600  17.95397
  1       0.001  0.50  23.17594  0.03178410  17.85261
  1       0.001  1.00  23.06673  0.03643373  17.75163
  1       0.010  0.25  22.90186  0.04824648  17.61988
  1       0.010  0.50  22.63757  0.06068456  17.50466
  1       0.010  1.00  22.27318  0.07657867  17.36692
  1       0.100  0.25  22.01912  0.08965487  17.26136
  1       0.100  0.50  21.85143  0.09813906  17.13832
  1       0.100  1.00  21.79476  0.10098770  16.99588
  2       0.001  0.25  23.15332  0.03374969  17.82873
  2       0.001  0.50  23.01012  0.04084330  17.69740
  2       0.001  1.00  22.83819  0.05297772  17.56281
  2       0.010  0.25  21.79669  0.11936973  16.85050
  2       0.010  0.50  21.40169  0.13508636  16.66030
  2       0.010  1.00  20.99860  0.15719386  16.40901
  2       0.100  0.25  20.21145  0.20968177  15.57361
  2       0.100  0.50  20.02162  0.22153320  15.44635
  2       0.100  1.00  19.94142  0.22692273  15.38686
  3       0.001  0.25  23.03289  0.04046468  17.71560
  3       0.001  0.50  22.84474  0.05308326  17.56640
  3       0.001  1.00  22.50800  0.07301540  17.37535
  3       0.010  0.25  21.25635  0.14714835  16.48226
  3       0.010  0.50  20.78701  0.17579449  16.11956
  3       0.010  1.00  20.44682  0.19590353  15.79202
  3       0.100  0.25  20.93466  0.18145963  15.90542
  3       0.100  0.50  21.85041  0.16672606  16.19394
  3       0.100  1.00  23.33268  0.14991142  16.53971

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 212 of 224 using same du svmPoly 7 
Support Vector Machines with Polynomial Kernel 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1104, 1105, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.31686  0.02842155  17.89756
  1       0.001  0.50  23.24050  0.03139007  17.80376
  1       0.001  1.00  23.13576  0.03661703  17.70989
  1       0.010  0.25  22.97861  0.04862689  17.58645
  1       0.010  0.50  22.74334  0.06184748  17.48181
  1       0.010  1.00  22.36463  0.07846050  17.32812
  1       0.100  0.25  22.04343  0.09098145  17.22505
  1       0.100  0.50  21.87565  0.09870481  17.12258
  1       0.100  1.00  21.83216  0.10194155  17.00433
  2       0.001  0.25  23.22295  0.03374789  17.77960
  2       0.001  0.50  23.08986  0.04051259  17.66868
  2       0.001  1.00  22.93385  0.05200317  17.54817
  2       0.010  0.25  21.95623  0.10935825  16.96527
  2       0.010  0.50  21.57568  0.12852558  16.76085
  2       0.010  1.00  21.20466  0.14763975  16.52369
  2       0.100  0.25  20.32372  0.20263170  15.69462
  2       0.100  0.50  20.12897  0.21478337  15.51702
  2       0.100  1.00  20.08516  0.21831469  15.47083
  3       0.001  0.25  23.12594  0.04002608  17.69656
  3       0.001  0.50  22.95427  0.05106064  17.56333
  3       0.001  1.00  22.66207  0.06904532  17.40158
  3       0.010  0.25  21.50160  0.13575762  16.59039
  3       0.010  0.50  21.03487  0.16077657  16.27244
  3       0.010  1.00  20.64682  0.18492562  15.94507
  3       0.100  0.25  21.03757  0.17754166  15.84335
  3       0.100  0.50  21.63118  0.16976855  16.05458
  3       0.100  1.00  23.49202  0.14647755  16.57205

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 213 of 224 using max rms svmPoly 7 
Support Vector Machines with Polynomial Kernel 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1104, 1106, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.50459  0.03165294  18.15629
  1       0.001  0.50  23.39212  0.03493496  18.10350
  1       0.001  1.00  23.33760  0.03787944  18.01743
  1       0.010  0.25  23.05076  0.03878783  17.92549
  1       0.010  0.50  22.90352  0.03912677  17.87921
  1       0.010  1.00  22.84367  0.03970527  17.85769
  1       0.100  0.25  22.82118  0.04014537  17.84829
  1       0.100  0.50  22.81824  0.04018732  17.84771
  1       0.100  1.00  22.81645  0.04019183  17.84722
  2       0.001  0.25  23.39104  0.03541648  18.10144
  2       0.001  0.50  23.33468  0.03854061  18.01318
  2       0.001  1.00  23.10729  0.03964019  17.93577
  2       0.010  0.25  22.74845  0.05151533  17.72586
  2       0.010  0.50  22.49199  0.06148996  17.59181
  2       0.010  1.00  22.21392  0.07607361  17.41825
  2       0.100  0.25  21.79791  0.11171769  16.59352
  2       0.100  0.50  21.81767  0.11229266  16.58636
  2       0.100  1.00  21.82506  0.11254848  16.58722
  3       0.001  0.25  23.35600  0.03863421  18.04462
  3       0.001  0.50  23.21187  0.03985925  17.95886
  3       0.001  1.00  22.98930  0.04044867  17.89444
  3       0.010  0.25  22.39821  0.07209013  17.49985
  3       0.010  0.50  22.12385  0.08803697  17.29152
  3       0.010  1.00  21.90772  0.10259662  16.99262
  3       0.100  0.25  20.87639  0.17313859  15.72130
  3       0.100  0.50  20.76454  0.17919916  15.55878
  3       0.100  1.00  20.73431  0.18096839  15.50394

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 214 of 224 using same rms svmPoly 7 
Support Vector Machines with Polynomial Kernel 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1106, 1105, 1106, 1105, 1104, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.68419  0.03345922  18.07709
  1       0.001  0.50  23.44149  0.03412692  18.03345
  1       0.001  1.00  23.38118  0.03722578  17.93987
  1       0.010  0.25  23.13360  0.03799111  17.85072
  1       0.010  0.50  23.01911  0.03772299  17.81249
  1       0.010  1.00  22.96689  0.03810513  17.79478
  1       0.100  0.25  22.93531  0.03828032  17.78681
  1       0.100  0.50  22.92633  0.03839884  17.78441
  1       0.100  1.00  22.92347  0.03838806  17.78446
  2       0.001  0.25  23.43990  0.03457969  18.03129
  2       0.001  0.50  23.37811  0.03788267  17.93553
  2       0.001  1.00  23.17750  0.03896781  17.86008
  2       0.010  0.25  22.90351  0.04889063  17.66801
  2       0.010  0.50  22.72241  0.05670435  17.57412
  2       0.010  1.00  22.43104  0.06884588  17.44166
  2       0.100  0.25  21.99086  0.09949949  16.72281
  2       0.100  0.50  21.99420  0.10016378  16.71668
  2       0.100  1.00  22.01165  0.10047010  16.72225
  3       0.001  0.25  23.39625  0.03774362  17.97089
  3       0.001  0.50  23.25561  0.03935101  17.88191
  3       0.001  1.00  23.08502  0.03935731  17.82176
  3       0.010  0.25  22.64265  0.06537119  17.49152
  3       0.010  0.50  22.32673  0.07937803  17.33289
  3       0.010  1.00  22.13020  0.09101380  17.09606
  3       0.100  0.25  21.14912  0.15805377  15.83127
  3       0.100  0.50  21.03425  0.16477207  15.69132
  3       0.100  1.00  20.99295  0.16626309  15.64380

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.1 and C = 1.


Now processing model 215 of 224 using max hudgins svmPoly 7 
Support Vector Machines with Polynomial Kernel 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1105, 1104, 1106, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.35539  0.02342562  18.08829
  1       0.001  0.50  23.31501  0.02429487  18.00983
  1       0.001  1.00  23.24777  0.02682414  17.92243
  1       0.010  0.25  23.11654  0.03280190  17.82333
  1       0.010  0.50  22.98020  0.04134604  17.73856
  1       0.010  1.00  22.75814  0.05199921  17.63779
  1       0.100  0.25  22.35763  0.06905997  17.47198
  1       0.100  0.50  22.18676  0.07776981  17.35789
  1       0.100  1.00  22.06447  0.08471313  17.21150
  2       0.001  0.25  23.30861  0.02511540  18.00087
  2       0.001  0.50  23.22906  0.02795816  17.90546
  2       0.001  1.00  23.10201  0.03334162  17.80952
  2       0.010  0.25  22.31376  0.08460409  17.28416
  2       0.010  0.50  21.89621  0.10477118  17.01373
  2       0.010  1.00  21.58901  0.11947963  16.80266
  2       0.100  0.25  21.07298  0.15304357  16.25139
  2       0.100  0.50  20.95504  0.16051062  16.17876
  2       0.100  1.00  20.85499  0.16651786  16.13994
  3       0.001  0.25  23.25873  0.02761748  17.92896
  3       0.001  0.50  23.12832  0.03250404  17.82442
  3       0.001  1.00  22.96913  0.04231860  17.70991
  3       0.010  0.25  21.85974  0.11055683  16.92379
  3       0.010  0.50  21.55724  0.12387975  16.73549
  3       0.010  1.00  21.27211  0.14057847  16.51145
  3       0.100  0.25  21.12078  0.15676320  16.07295
  3       0.100  0.50  21.39511  0.15358417  16.25039
  3       0.100  1.00  22.04992  0.14413856  16.50173

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 216 of 224 using same hudgins svmPoly 7 
Support Vector Machines with Polynomial Kernel 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1107, 1105, 1104, 1105, ... 
Resampling results across tuning parameters:

  degree  scale  C     RMSE      Rsquared    MAE     
  1       0.001  0.25  23.41927  0.02195340  18.01555
  1       0.001  0.50  23.34691  0.02323856  17.93267
  1       0.001  1.00  23.28445  0.02593633  17.84473
  1       0.010  0.25  23.17057  0.03286967  17.76132
  1       0.010  0.50  23.01613  0.04236331  17.67839
  1       0.010  1.00  22.79073  0.05509582  17.57569
  1       0.100  0.25  22.41493  0.07207202  17.43039
  1       0.100  0.50  22.19964  0.08184039  17.32522
  1       0.100  1.00  22.06611  0.08802735  17.19808
  2       0.001  0.25  23.34195  0.02396068  17.92401
  2       0.001  0.50  23.26788  0.02725469  17.82875
  2       0.001  1.00  23.16212  0.03331171  17.75268
  2       0.010  0.25  22.44485  0.07968880  17.33007
  2       0.010  0.50  22.04068  0.09812659  17.09787
  2       0.010  1.00  21.74610  0.11204709  16.88923
  2       0.100  0.25  21.27235  0.14066905  16.45212
  2       0.100  0.50  21.12819  0.14972221  16.36322
  2       0.100  1.00  20.99111  0.15917086  16.24452
  3       0.001  0.25  23.29406  0.02635504  17.85389
  3       0.001  0.50  23.18414  0.03236432  17.76580
  3       0.001  1.00  23.03327  0.04257355  17.65750
  3       0.010  0.25  22.06716  0.09879369  17.04169
  3       0.010  0.50  21.74015  0.11445784  16.81733
  3       0.010  1.00  21.48872  0.12947074  16.61438
  3       0.100  0.25  21.13714  0.15537328  16.01933
  3       0.100  0.50  21.25562  0.15683793  16.06077
  3       0.100  1.00  21.69640  0.14934319  16.23504

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 217 of 224 using max all ranger 7 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1107, 1105, 1106, 1104, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.94863  0.4434810  12.34280
   2    extratrees  17.45380  0.4302647  13.20042
  13    variance    16.84052  0.4418372  11.97596
  13    extratrees  16.67785  0.4617144  12.06610
  24    variance    17.03149  0.4285050  12.17570
  24    extratrees  16.64502  0.4616492  11.97462

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 218 of 224 using same all ranger 7 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.10862  0.4363440  12.45521
   2    extratrees  17.62000  0.4235436  13.31760
  13    variance    17.00170  0.4337343  12.08125
  13    extratrees  16.82570  0.4553453  12.12651
  24    variance    17.18945  0.4204392  12.27855
  24    extratrees  16.80701  0.4537144  12.04277

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 219 of 224 using max du ranger 7 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1105, 1105, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.30251  0.4208027  12.75149
   2    extratrees  17.86159  0.4023420  13.63417
  10    variance    17.01974  0.4310976  12.16118
  10    extratrees  17.03252  0.4409546  12.46802
  18    variance    17.11895  0.4238910  12.23748
  18    extratrees  16.93177  0.4450301  12.30428

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 220 of 224 using same du ranger 7 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1106, 1104, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.40361  0.4186341  12.81509
   2    extratrees  17.95257  0.4029118  13.70092
  10    variance    17.07716  0.4309257  12.20197
  10    extratrees  17.08755  0.4421316  12.47716
  18    variance    17.16859  0.4236112  12.25741
  18    extratrees  16.99959  0.4446384  12.31820

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 221 of 224 using max rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.69315  0.3829105  12.49765
  2     extratrees  17.46817  0.4041229  12.85827
  3     variance    17.77545  0.3792957  12.41394
  3     extratrees  17.42702  0.4030734  12.64665

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 222 of 224 using same rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1106, 1105, 1105, 1104, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.81354  0.3769832  12.58254
  2     extratrees  17.62253  0.3972047  12.99314
  3     variance    17.86343  0.3750491  12.50227
  3     extratrees  17.55412  0.3976585  12.75448

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 223 of 224 using max hudgins ranger 7 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1104, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.09035  0.3621451  13.60435
   2    extratrees  18.59763  0.3445786  14.43283
   7    variance    17.63513  0.3879510  12.84332
   7    extratrees  17.78890  0.3874488  13.32802
  12    variance    17.57570  0.3915130  12.73314
  12    extratrees  17.61442  0.3966990  13.08586

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 224 of 224 using same hudgins ranger 7 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.30041  0.3499782  13.75012
   2    extratrees  18.77493  0.3374224  14.58044
   7    variance    17.85667  0.3741025  13.00223
   7    extratrees  17.98499  0.3780692  13.47151
  12    variance    17.80907  0.3764582  12.87039
  12    extratrees  17.80990  0.3864832  13.22495

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.
 MAE     
  1       0.001  0.25  23.41927  0.02195340  18.01555
  1       0.001  0.50  23.34691  0.02323856  17.93267
  1       0.001  1.00  23.28445  0.02593633  17.84473
  1       0.010  0.25  23.17057  0.03286967  17.76132
  1       0.010  0.50  23.01613  0.04236331  17.67839
  1       0.010  1.00  22.79073  0.05509582  17.57569
  1       0.100  0.25  22.41493  0.07207202  17.43039
  1       0.100  0.50  22.19964  0.08184039  17.32522
  1       0.100  1.00  22.06611  0.08802735  17.19808
  2       0.001  0.25  23.34195  0.02396068  17.92401
  2       0.001  0.50  23.26788  0.02725469  17.82875
  2       0.001  1.00  23.16212  0.03331171  17.75268
  2       0.010  0.25  22.44485  0.07968880  17.33007
  2       0.010  0.50  22.04068  0.09812659  17.09787
  2       0.010  1.00  21.74610  0.11204709  16.88923
  2       0.100  0.25  21.27235  0.14066905  16.45212
  2       0.100  0.50  21.12819  0.14972221  16.36322
  2       0.100  1.00  20.99111  0.15917086  16.24452
  3       0.001  0.25  23.29406  0.02635504  17.85389
  3       0.001  0.50  23.18414  0.03236432  17.76580
  3       0.001  1.00  23.03327  0.04257355  17.65750
  3       0.010  0.25  22.06716  0.09879369  17.04169
  3       0.010  0.50  21.74015  0.11445784  16.81733
  3       0.010  1.00  21.48872  0.12947074  16.61438
  3       0.100  0.25  21.13714  0.15537328  16.01933
  3       0.100  0.50  21.25562  0.15683793  16.06077
  3       0.100  1.00  21.69640  0.14934319  16.23504

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.1 and C = 1.


Now processing model 217 of 224 using max all ranger 7 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1107, 1105, 1106, 1104, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    16.94863  0.4434810  12.34280
   2    extratrees  17.45380  0.4302647  13.20042
  13    variance    16.84052  0.4418372  11.97596
  13    extratrees  16.67785  0.4617144  12.06610
  24    variance    17.03149  0.4285050  12.17570
  24    extratrees  16.64502  0.4616492  11.97462

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 218 of 224 using same all ranger 7 
Random Forest 

1228 samples
  24 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.10862  0.4363440  12.45521
   2    extratrees  17.62000  0.4235436  13.31760
  13    variance    17.00170  0.4337343  12.08125
  13    extratrees  16.82570  0.4553453  12.12651
  24    variance    17.18945  0.4204392  12.27855
  24    extratrees  16.80701  0.4537144  12.04277

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 24, splitrule = extratrees
 and min.node.size = 5.


Now processing model 219 of 224 using max du ranger 7 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1104, 1105, 1105, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.30251  0.4208027  12.75149
   2    extratrees  17.86159  0.4023420  13.63417
  10    variance    17.01974  0.4310976  12.16118
  10    extratrees  17.03252  0.4409546  12.46802
  18    variance    17.11895  0.4238910  12.23748
  18    extratrees  16.93177  0.4450301  12.30428

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 220 of 224 using same du ranger 7 
Random Forest 

1228 samples
  18 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1105, 1106, 1104, 1106, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    17.40361  0.4186341  12.81509
   2    extratrees  17.95257  0.4029118  13.70092
  10    variance    17.07716  0.4309257  12.20197
  10    extratrees  17.08755  0.4421316  12.47716
  18    variance    17.16859  0.4236112  12.25741
  18    extratrees  16.99959  0.4446384  12.31820

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 18, splitrule = extratrees
 and min.node.size = 5.


Now processing model 221 of 224 using max rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1106, 1105, 1105, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.69315  0.3829105  12.49765
  2     extratrees  17.46817  0.4041229  12.85827
  3     variance    17.77545  0.3792957  12.41394
  3     extratrees  17.42702  0.4030734  12.64665

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 222 of 224 using same rms ranger 7 
note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .

Random Forest 

1228 samples
   3 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1106, 1106, 1105, 1105, 1104, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
  2     variance    17.81354  0.3769832  12.58254
  2     extratrees  17.62253  0.3972047  12.99314
  3     variance    17.86343  0.3750491  12.50227
  3     extratrees  17.55412  0.3976585  12.75448

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 3, splitrule = extratrees
 and min.node.size = 5.


Now processing model 223 of 224 using max hudgins ranger 7 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1104, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.09035  0.3621451  13.60435
   2    extratrees  18.59763  0.3445786  14.43283
   7    variance    17.63513  0.3879510  12.84332
   7    extratrees  17.78890  0.3874488  13.32802
  12    variance    17.57570  0.3915130  12.73314
  12    extratrees  17.61442  0.3966990  13.08586

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.


Now processing model 224 of 224 using same hudgins ranger 7 
Random Forest 

1228 samples
  12 predictors

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1105, 1105, 1105, 1105, 1106, 1105, ... 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE      Rsquared   MAE     
   2    variance    18.30041  0.3499782  13.75012
   2    extratrees  18.77493  0.3374224  14.58044
   7    variance    17.85667  0.3741025  13.00223
   7    extratrees  17.98499  0.3780692  13.47151
  12    variance    17.80907  0.3764582  12.87039
  12    extratrees  17.80990  0.3864832  13.22495

Tuning parameter 'min.node.size' was held constant at a value of 5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 12, splitrule = variance
 and min.node.size = 5.
Time difference of 10.32553 hours

Enter a frame number, or 0 to exit   

1: source("plot_shallow_emg_grid_search_learning.R")
2: withVisible(eval(ei, envir))
3: eval(ei, envir)
4: eval(ei, envir)
5: plot_shallow_emg_grid_search_learning.R#33: print(p1)
6: print.ggplot(p1)
7: grid.newpage()

No suitable frames for recover()
      X  method datalength featureset seconds ntraindata ntestdata        cor
1     2      lm        max        all       1       8593       955 0.28873124
2   225      lm       same        all       1       1228       136 0.41570316
3     3      lm        max         du       1       8593       955 0.27149646
4     4      lm       same         du       1       1228       136 0.34354788
5     5      lm        max        rms       1       8593       955 0.15199968
6     6      lm       same        rms       1       1228       136 0.24319939
7     7      lm        max    hudgins       1       8593       955 0.26142117
8     8      lm       same    hudgins       1       1228       136 0.31691208
9     9     knn        max        all       1       8593       955 0.59581716
10   10     knn       same        all       1       1228       136 0.47788347
11   11     knn        max         du       1       8593       955 0.59584731
12   12     knn       same         du       1       1228       136 0.47788347
13   13     knn        max        rms       1       8593       955 0.59565818
14   14     knn       same        rms       1       1228       136 0.60119484
15   15     knn        max    hudgins       1       8593       955 0.35920325
16   16     knn       same    hudgins       1       1228       136 0.25576908
17   17 svmPoly        max        all       1       8593       955 0.03274488
18   18 svmPoly       same        all       1       1228       136 0.43102902
19   19 svmPoly        max         du       1       8593       955 0.10665844
20   20 svmPoly       same         du       1       1228       136 0.38991453
21   21 svmPoly        max        rms       1       8593       955 0.20086977
22   22 svmPoly       same        rms       1       1228       136 0.41242504
23   23 svmPoly        max    hudgins       1       8593       955 0.13380963
24   24 svmPoly       same    hudgins       1       1228       136 0.44810145
25   25  ranger        max        all       1       8593       955 0.68910077
26   26  ranger       same        all       1       1228       136 0.59246981
27   27  ranger        max         du       1       8593       955 0.66866902
28   28  ranger       same         du       1       1228       136 0.56991247
29   29  ranger        max        rms       1       8593       955 0.64764537
30   30  ranger       same        rms       1       1228       136 0.58925304
31   31  ranger        max    hudgins       1       8593       955 0.63801454
32   32  ranger       same    hudgins       1       1228       136 0.52890274
33   33      lm        max        all       2       3683       409 0.38744731
34   34      lm       same        all       2       1228       136 0.42971451
35   35      lm        max         du       2       3683       409 0.35491122
36   36      lm       same         du       2       1228       136 0.39283182
37   37      lm        max        rms       2       3683       409 0.13880935
38   38      lm       same        rms       2       1228       136 0.32627065
39   39      lm        max    hudgins       2       3683       409 0.33682096
40   40      lm       same    hudgins       2       1228       136 0.36566278
41   41     knn        max        all       2       3683       409 0.59426312
42   42     knn       same        all       2       1228       136 0.59123737
43   43     knn        max         du       2       3683       409 0.59420910
44   44     knn       same         du       2       1228       136 0.59123737
45   45     knn        max        rms       2       3683       409 0.60494443
46   46     knn       same        rms       2       1228       136 0.52047737
47   47     knn        max    hudgins       2       3683       409 0.33109355
48   48     knn       same    hudgins       2       1228       136 0.32447632
49   49 svmPoly        max        all       2       3683       409 0.47539083
50   50 svmPoly       same        all       2       1228       136 0.43221449
51   51 svmPoly        max         du       2       3683       409 0.45794745
52   52 svmPoly       same         du       2       1228       136 0.42997733
53   53 svmPoly        max        rms       2       3683       409 0.42168424
54   54 svmPoly       same        rms       2       1228       136 0.31890939
55   55 svmPoly        max    hudgins       2       3683       409 0.43946761
56   56 svmPoly       same    hudgins       2       1228       136 0.33508773
57   57  ranger        max        all       2       3683       409 0.69842268
58   58  ranger       same        all       2       1228       136 0.68833098
59   59  ranger        max         du       2       3683       409 0.68781074
60   60  ranger       same         du       2       1228       136 0.65401484
61   61  ranger        max        rms       2       3683       409 0.63601959
62   62  ranger       same        rms       2       1228       136 0.67512414
63   63  ranger        max    hudgins       2       3683       409 0.65747116
64   64  ranger       same    hudgins       2       1228       136 0.58915696
65   65      lm        max        all       3       2455       273 0.38079232
66   66      lm       same        all       3       1228       136 0.25480743
67   67      lm        max         du       3       2455       273 0.30926466
68   68      lm       same         du       3       1228       136 0.21864324
69   69      lm        max        rms       3       2455       273 0.21984420
70   70      lm       same        rms       3       1228       136 0.27510372
71   71      lm        max    hudgins       3       2455       273 0.31319337
72   72      lm       same    hudgins       3       1228       136 0.25190905
73   73     knn        max        all       3       2455       273 0.64485718
74   74     knn       same        all       3       1228       136 0.56193046
75   75     knn        max         du       3       2455       273 0.64506571
76   76     knn       same         du       3       1228       136 0.53736699
77   77     knn        max        rms       3       2455       273 0.65110071
78   78     knn       same        rms       3       1228       136 0.55150081
79   79     knn        max    hudgins       3       2455       273 0.42544249
80   80     knn       same    hudgins       3       1228       136 0.25647083
81   81 svmPoly        max        all       3       2455       273 0.59101810
82   82 svmPoly       same        all       3       1228       136 0.33868692
83   83 svmPoly        max         du       3       2455       273 0.55838229
84   84 svmPoly       same         du       3       1228       136 0.35247256
85   85 svmPoly        max        rms       3       2455       273 0.48912533
86   86 svmPoly       same        rms       3       1228       136 0.33658948
87   87 svmPoly        max    hudgins       3       2455       273 0.49738410
88   88 svmPoly       same    hudgins       3       1228       136 0.31070438
89   89  ranger        max        all       3       2455       273 0.72404686
90   90  ranger       same        all       3       1228       136 0.52846813
91   91  ranger        max         du       3       2455       273 0.71720732
92   92  ranger       same         du       3       1228       136 0.51304720
93   93  ranger        max        rms       3       2455       273 0.70850206
94   94  ranger       same        rms       3       1228       136 0.54356236
95   95  ranger        max    hudgins       3       2455       273 0.68340243
96   96  ranger       same    hudgins       3       1228       136 0.46748269
97   97      lm        max        all       4       1228       136 0.36559888
98   98      lm       same        all       4       1228       136 0.39906862
99   99      lm        max         du       4       1228       136 0.27844897
100 100      lm       same         du       4       1228       136 0.33921267
101 101      lm        max        rms       4       1228       136 0.06513404
102 102      lm       same        rms       4       1228       136 0.19712036
103 103      lm        max    hudgins       4       1228       136 0.25542026
104 104      lm       same    hudgins       4       1228       136 0.34800653
105 105     knn        max        all       4       1228       136 0.54158748
106 106     knn       same        all       4       1228       136 0.64382210
107 107     knn        max         du       4       1228       136 0.54158748
108 108     knn       same         du       4       1228       136 0.64382210
109 109     knn        max        rms       4       1228       136 0.56570035
110 110     knn       same        rms       4       1228       136 0.61013803
111 111     knn        max    hudgins       4       1228       136 0.33167796
112 112     knn       same    hudgins       4       1228       136 0.43584764
113 113 svmPoly        max        all       4       1228       136 0.47295202
114 114 svmPoly       same        all       4       1228       136 0.51337277
115 115 svmPoly        max         du       4       1228       136 0.46799089
116 116 svmPoly       same         du       4       1228       136 0.50378072
117 117 svmPoly        max        rms       4       1228       136 0.43522239
118 118 svmPoly       same        rms       4       1228       136 0.46049567
119 119 svmPoly        max    hudgins       4       1228       136 0.42305007
120 120 svmPoly       same    hudgins       4       1228       136 0.45331267
121 121  ranger        max        all       4       1228       136 0.64518205
122 122  ranger       same        all       4       1228       136 0.65613323
123 123  ranger        max         du       4       1228       136 0.63174418
124 124  ranger       same         du       4       1228       136 0.63763401
125 125  ranger        max        rms       4       1228       136 0.63804186
126 126  ranger       same        rms       4       1228       136 0.66112492
127 127  ranger        max    hudgins       4       1228       136 0.57259318
128 128  ranger       same    hudgins       4       1228       136 0.61121330
129 129      lm        max        all       5       1228       136 0.45789344
130 130      lm       same        all       5       1228       136 0.43800601
131 131      lm        max         du       5       1228       136 0.40467218
132 132      lm       same         du       5       1228       136 0.36803275
133 133      lm        max        rms       5       1228       136 0.22564332
134 134      lm       same        rms       5       1228       136 0.20413049
135 135      lm        max    hudgins       5       1228       136 0.37281143
136 136      lm       same    hudgins       5       1228       136 0.36186300
137 137     knn        max        all       5       1228       136 0.64558900
138 138     knn       same        all       5       1228       136 0.54114442
139 139     knn        max         du       5       1228       136 0.64558900
140 140     knn       same         du       5       1228       136 0.54114442
141 141     knn        max        rms       5       1228       136 0.60055460
142 142     knn       same        rms       5       1228       136 0.52020921
143 143     knn        max    hudgins       5       1228       136 0.37852160
144 144     knn       same    hudgins       5       1228       136 0.33384785
145 145 svmPoly        max        all       5       1228       136 0.50058831
146 146 svmPoly       same        all       5       1228       136 0.45333159
147 147 svmPoly        max         du       5       1228       136 0.49323673
148 148 svmPoly       same         du       5       1228       136 0.43185524
149 149 svmPoly        max        rms       5       1228       136 0.44392272
150 150 svmPoly       same        rms       5       1228       136 0.38661087
151 151 svmPoly        max    hudgins       5       1228       136 0.46752587
152 152 svmPoly       same    hudgins       5       1228       136 0.36456320
153 153  ranger        max        all       5       1228       136 0.70675524
154 154  ranger       same        all       5       1228       136 0.66143452
155 155  ranger        max         du       5       1228       136 0.69528375
156 156  ranger       same         du       5       1228       136 0.65829052
157 157  ranger        max        rms       5       1228       136 0.68888771
158 158  ranger       same        rms       5       1228       136 0.65014627
159 159  ranger        max    hudgins       5       1228       136 0.64935097
160 160  ranger       same    hudgins       5       1228       136 0.62924093
161 161      lm        max        all       6       1228       136 0.27179745
162 162      lm       same        all       6       1228       136 0.46123215
163 163      lm        max         du       6       1228       136 0.27219596
164 164      lm       same         du       6       1228       136 0.39887172
165 165      lm        max        rms       6       1228       136 0.08699112
166 166      lm       same        rms       6       1228       136 0.12517308
167 167      lm        max    hudgins       6       1228       136 0.25891778
168 168      lm       same    hudgins       6       1228       136 0.35214341
169 169     knn        max        all       6       1228       136 0.50224711
170 170     knn       same        all       6       1228       136 0.63518613
171 171     knn        max         du       6       1228       136 0.50224711
172 172     knn       same         du       6       1228       136 0.63518613
173 173     knn        max        rms       6       1228       136 0.48935847
174 174     knn       same        rms       6       1228       136 0.57799294
175 175     knn        max    hudgins       6       1228       136 0.40663254
176 176     knn       same    hudgins       6       1228       136 0.43501318
177 177 svmPoly        max        all       6       1228       136 0.37374256
178 178 svmPoly       same        all       6       1228       136 0.60412913
179 179 svmPoly        max         du       6       1228       136 0.36951438
180 180 svmPoly       same         du       6       1228       136 0.55975993
181 181 svmPoly        max        rms       6       1228       136 0.31592332
182 182 svmPoly       same        rms       6       1228       136 0.47192508
183 183 svmPoly        max    hudgins       6       1228       136 0.34761259
184 184 svmPoly       same    hudgins       6       1228       136 0.45890855
185 185  ranger        max        all       6       1228       136 0.64999365
186 186  ranger       same        all       6       1228       136 0.71206249
187 187  ranger        max         du       6       1228       136 0.63934787
188 188  ranger       same         du       6       1228       136 0.71968968
189 189  ranger        max        rms       6       1228       136 0.58444452
190 190  ranger       same        rms       6       1228       136 0.69667290
191 191  ranger        max    hudgins       6       1228       136 0.56364016
192 192  ranger       same    hudgins       6       1228       136 0.63482512
193 193      lm        max        all       7       1228       136 0.48937626
194 194      lm       same        all       7       1228       136 0.47811179
195 195      lm        max         du       7       1228       136 0.44339757
196 196      lm       same         du       7       1228       136 0.45141348
197 197      lm        max        rms       7       1228       136 0.36719482
198 198      lm       same        rms       7       1228       136 0.42029992
199 199      lm        max    hudgins       7       1228       136 0.44950171
200 200      lm       same    hudgins       7       1228       136 0.44053956
201 201     knn        max        all       7       1228       136 0.62038902
202 202     knn       same        all       7       1228       136 0.71914965
203 203     knn        max         du       7       1228       136 0.62038902
204 204     knn       same         du       7       1228       136 0.72356205
205 205     knn        max        rms       7       1228       136 0.68253233
206 206     knn       same        rms       7       1228       136 0.69733604
207 207     knn        max    hudgins       7       1228       136 0.44802627
208 208     knn       same    hudgins       7       1228       136 0.56218638
209 209 svmPoly        max        all       7       1228       136 0.47723395
210 210 svmPoly       same        all       7       1228       136 0.56886290
211 211 svmPoly        max         du       7       1228       136 0.47311205
212 212 svmPoly       same         du       7       1228       136 0.55974029
213 213 svmPoly        max        rms       7       1228       136 0.28302309
214 214 svmPoly       same        rms       7       1228       136 0.50258100
215 215 svmPoly        max    hudgins       7       1228       136 0.37514676
216 216 svmPoly       same    hudgins       7       1228       136 0.52748113
217 217  ranger        max        all       7       1228       136 0.71433020
218 218  ranger       same        all       7       1228       136 0.76456577
219 219  ranger        max         du       7       1228       136 0.70665739
220 220  ranger       same         du       7       1228       136 0.75889001
221 221  ranger        max        rms       7       1228       136 0.70990633
222 222  ranger       same        rms       7       1228       136 0.76658039
223 223  ranger        max    hudgins       7       1228       136 0.64563344
224 224  ranger       same    hudgins       7       1228       136 0.71465299
          corll      corul     rmse       dv
1    0.22949730 0.34583430 21.10960 21.10960
2    0.26598283 0.54584006 18.25128 18.25128
3    0.21170517 0.32926303 21.14937 21.14937
4    0.18596970 0.48389682 18.86205 18.86205
5    0.08942446 0.21337967 21.62297 21.62297
6    0.07806226 0.39534736 19.52054 19.52054
7    0.20132239 0.31955912 21.14822 21.14822
8    0.15695208 0.46066974 19.05523 19.05523
9    0.55329255 0.63524424 17.56545 17.56545
10   0.33663031 0.59810286 17.69666 17.69666
11   0.55332498 0.63527212 17.56492 17.56492
12   0.33663031 0.59810286 17.69666 17.69666
13   0.55312153 0.63509722 17.61392 17.61392
14   0.48160033 0.69880778 16.03395 16.03395
15   0.30266251 0.41322461 20.58294 20.58294
16   0.09137007 0.40659622 19.74280 19.74280
17  -0.03075650 0.09598299 95.66926 95.66926
18   0.28324754 0.55881623 18.53880 18.53880
19   0.04351537 0.16895277 42.42092 42.42092
20   0.23714689 0.52386341 18.80454 18.80454
21   0.13920614 0.26098166 27.69202 27.69202
22   0.26230242 0.54305640 18.51520 18.51520
23   0.07097461 0.19558688 51.09174 51.09174
24   0.30259324 0.57319790 18.18428 18.18428
25   0.65426433 0.72101904 15.87591 15.87591
26   0.47112304 0.69180726 16.27961 16.27961
27   0.63204192 0.70231523 16.23996 16.23996
28   0.44419328 0.67362147 16.58235 16.58235
29   0.60923844 0.68302095 16.63325 16.63325
30   0.46726886 0.68922157 16.23669 16.23669
31   0.59881344 0.67416581 16.81036 16.81036
32   0.39580945 0.64023444 17.14991 17.14991
33   0.30182070 0.46687305 20.81242 20.81242
34   0.28176298 0.55770570 19.64923 19.64923
35   0.26713885 0.43684334 21.10552 21.10552
36   0.24039545 0.52635846 20.03884 20.03884
37   0.04241448 0.23264377 22.43651 22.43651
38   0.16711613 0.46885314 20.84654 20.84654
39   0.24795339 0.42006727 21.26745 21.26745
40   0.21027284 0.50303247 20.35241 20.35241
41   0.52770543 0.65356820 18.33693 18.33693
42   0.46964585 0.69081692 17.50882 17.50882
43   0.52764517 0.65352036 18.33840 18.33840
44   0.46964585 0.69081692 17.50882 17.50882
45   0.53963295 0.66301827 18.15409 18.15409
46   0.38595975 0.63332266 19.02277 19.02277
47   0.24189373 0.41474405 21.63880 21.63880
48   0.16516475 0.46728603 20.77395 20.77395
49   0.39671218 0.54713548 19.94255 19.94255
50   0.28458699 0.55981735 19.66384 19.66384
51   0.37775604 0.53131983 20.13214 20.13214
52   0.28205975 0.55792778 19.65875 19.65875
53   0.33856192 0.49827601 20.60909 20.60909
54   0.15911845 0.46241830 20.74179 20.74179
55   0.35774663 0.51450846 20.32439 20.32439
56   0.17672314 0.47654072 20.58157 20.58157
57   0.64514837 0.74493888 16.27927 16.27927
58   0.58814581 0.76771025 16.26968 16.26968
59   0.63306679 0.73570905 16.47101 16.47101
60   0.54576661 0.74079227 16.89098 16.89098
61   0.57448345 0.69040651 17.45600 17.45600
62   0.57177045 0.75738370 16.35198 16.35198
63   0.59867207 0.70922244 17.09392 17.09392
64   0.46715382 0.68914431 17.69001 17.69001
65   0.27448366 0.47790499 21.15597 21.15597
66   0.09034988 0.40573724 24.16560 24.16560
67   0.19781014 0.41282491 21.78341 21.78341
68   0.05223278 0.37323911 24.60727 24.60727
69   0.10383715 0.32994987 22.33265 22.33265
70   0.11195538 0.42381037 23.92618 23.92618
71   0.20198639 0.41642714 21.73045 21.73045
72   0.08727711 0.40314666 24.14759 24.14759
73   0.56975824 0.70927532 17.52075 17.52075
74   0.43471817 0.66715613 20.56620 20.56620
75   0.56999934 0.70945270 17.51667 17.51667
76   0.40573554 0.64716002 20.95090 20.95090
77   0.57698249 0.71458286 17.44610 17.44610
78   0.42237987 0.65868426 20.66857 20.66857
79   0.32304132 0.51799698 20.78941 20.78941
80   0.09211477 0.40722289 24.42715 24.42715
81   0.50794006 0.66320231 18.57971 18.57971
82   0.18065348 0.47967263 24.06540 24.06540
83   0.47087956 0.63500521 19.07993 19.07993
84   0.19575431 0.49163537 23.90711 23.90711
85   0.39324286 0.57448360 20.20759 20.20759
86   0.17836245 0.47784794 24.73902 24.73902
87   0.40242965 0.58175004 19.98682 19.98682
88   0.15022873 0.45522804 24.68226 24.68226
89   0.66225503 0.77605666 15.90057 15.90057
90   0.39530062 0.63987835 20.96098 20.96098
91   0.65419116 0.77033447 16.00749 16.00749
92   0.37729887 0.62721232 21.19160 21.19160
93   0.64394828 0.76303896 16.23233 16.23233
94   0.41302062 0.65221770 20.72259 20.72259
95   0.61454424 0.74192593 16.74140 16.74140
96   0.32470145 0.58943161 21.84031 21.84031
97   0.21020234 0.50297737 21.32598 21.32598
98   0.24735195 0.53168485 20.71629 20.71629
99   0.11553128 0.42677787 22.08124 22.08124
100  0.18122802 0.48012983 21.22864 21.22864
101 -0.10434288 0.23093491 23.14717 23.14717
102  0.02977549 0.35371629 22.11260 22.11260
103  0.09099998 0.40628467 22.13518 22.13518
104  0.19085406 0.48776564 21.14618 21.14618
105  0.41069656 0.65060653 19.27736 19.27736
106  0.53328471 0.73274292 17.26981 17.26981
107  0.41069656 0.65060653 19.27736 19.27736
108  0.53328471 0.73274292 17.26981 17.26981
109  0.43918974 0.67021168 18.83784 18.83784
110  0.49237517 0.70596397 17.94365 17.94365
111  0.17300430 0.47357032 21.94468 21.94468
112  0.28869563 0.56288319 20.31709 20.31709
113  0.33096867 0.59399495 20.17879 20.17879
114  0.37767788 0.62748036 19.36677 19.36677
115  0.32528326 0.58985595 20.16159 20.16159
116  0.36653068 0.61957211 19.49670 19.49670
117  0.28798817 0.56235583 20.46867 20.46867
118  0.31671331 0.58359061 20.27861 20.27861
119  0.27424722 0.55206837 20.68636 20.68636
120  0.30852233 0.57757248 20.20454 20.20454
121  0.53494731 0.73381833 17.50979 17.50979
122  0.54836681 0.74246207 17.05981 17.05981
123  0.51855629 0.72317250 17.77480 17.77480
124  0.52573026 0.72784393 17.43462 17.43462
125  0.52622762 0.72816710 17.58059 17.58059
126  0.55450205 0.74639248 16.97703 16.97703
127  0.44738175 0.67578926 18.71182 18.71182
128  0.49367311 0.70682307 17.93432 17.93432
129  0.31374344 0.58141195 18.12900 18.12900
130  0.29113905 0.56470289 22.27427 22.27427
131  0.25361549 0.53646150 18.62302 18.62302
132  0.21288875 0.50507520 23.03723 23.03723
133  0.05957322 0.37955932 19.82386 19.82386
134  0.03707145 0.36009000 24.27218 24.27218
135  0.21817013 0.50918941 18.92644 18.92644
136  0.20608336 0.49975413 23.16020 23.16020
137  0.53544499 0.73414005 15.61117 15.61117
138  0.41017540 0.65024493 20.80923 20.80923
139  0.53544499 0.73414005 15.61117 15.61117
140  0.41017540 0.65024493 20.80923 20.80923
141  0.48083034 0.69829471 16.44732 16.44732
142  0.38564677 0.63310239 21.05395 21.05395
143  0.22449287 0.51409741 19.37114 19.37114
144  0.17537036 0.47546098 23.55821 23.55821
145  0.36282942 0.61693489 17.99932 17.99932
146  0.30854388 0.57758835 22.18668 22.18668
147  0.35432253 0.61085183 17.99908 17.99908
148  0.28418102 0.55951400 22.41931 22.41931
149  0.29784699 0.56968488 18.82572 18.82572
150  0.23347220 0.52103514 23.27587 23.27587
151  0.32475087 0.58946766 18.27915 18.27915
152  0.20905990 0.50208419 23.26986 23.26986
153  0.61112839 0.78204778 14.44727 14.44727
154  0.55488296 0.74663606 18.78322 18.78322
155  0.59679978 0.77313016 14.69471 14.69471
156  0.55101688 0.74416143 18.89009 18.89009
157  0.58883792 0.76814466 14.76985 14.76985
158  0.54102354 0.73774014 18.93092 18.93092
159  0.54004933 0.73711224 15.50312 15.50312
160  0.51551209 0.72118455 19.45975 19.45975
161  0.10842529 0.42087430 22.31745 22.31745
162  0.31755436 0.58420689 19.69556 19.69556
163  0.10885056 0.42122835 22.21227 22.21227
164  0.24713208 0.53151684 20.32029 20.32029
165 -0.08255058 0.25163914 23.00942 23.00942
166 -0.04408879 0.28744919 22.01196 22.01196
167  0.09471291 0.40940692 22.22516 22.22516
168  0.19539289 0.49135036 20.75518 20.75518
169  0.36475209 0.61830554 19.95194 19.95194
170  0.52274675 0.72590345 17.36774 17.36774
171  0.36475209 0.61830554 19.95194 19.95194
172  0.52274675 0.72590345 17.36774 17.36774
173  0.34984403 0.60763716 20.39186 20.39186
174  0.45381399 0.68015044 18.55537 18.55537
175  0.25580974 0.53813058 21.02972 21.02972
176  0.28775148 0.56217934 20.01817 20.01817
177  0.21920026 0.50999034 21.42289 21.42289
178  0.48513160 0.70115790 17.69728 17.69728
179  0.21452527 0.50635148 21.43759 21.43759
180  0.43214649 0.66539528 18.37979 18.37979
181  0.15588019 0.45980371 21.86621 21.86621
182  0.32979096 0.59313872 19.65105 19.65105
183  0.19042220 0.48742404 21.65825 21.65825
184  0.31490163 0.58226204 19.77080 19.77080
185  0.54083657 0.73761965 17.85327 17.85327
186  0.61777875 0.78616309 15.93178 15.93178
187  0.52782079 0.72920168 18.11258 18.11258
188  0.62735984 0.79206582 15.87335 15.87335
189  0.46151617 0.68535166 18.75950 18.75950
190  0.59853159 0.77421169 16.19515 16.19515
191  0.43674532 0.66854230 19.16201 19.16201
192  0.52230697 0.72561714 17.24953 17.24953
193  0.34986457 0.60765192 19.44892 19.44892
194  0.33689268 0.59829290 19.15921 19.15921
195  0.29725103 0.56924307 19.95117 19.95117
196  0.30636021 0.57597902 19.46972 19.46972
197  0.21196361 0.50435315 20.93470 20.93470
198  0.27115106 0.54973863 20.31604 20.31604
199  0.30418529 0.57437405 19.96497 19.96497
200  0.29400964 0.56683732 19.62220 19.62220
201  0.50477024 0.71414265 17.52226 17.52226
202  0.62668055 0.79164833 15.14133 15.14133
203  0.50477024 0.71414265 17.52226 17.52226
204  0.63223493 0.79505750 15.09719 15.09719
205  0.58094585 0.76318131 16.26234 16.26234
206  0.59935862 0.77472782 15.68486 15.68486
207  0.30250778 0.57313473 20.06316 20.06316
208  0.43502152 0.66736366 18.01713 18.01713
209  0.33588404 0.59756218 19.80865 19.80865
210  0.44294576 0.67277223 17.99453 17.99453
211  0.33115224 0.59412836 19.79347 19.79347
212  0.43212322 0.66537934 18.09373 18.09373
213  0.12042764 0.43083032 22.36953 22.36953
214  0.36513922 0.61858134 19.16352 19.16352
215  0.22075441 0.51119774 20.99746 20.99746
216  0.39414536 0.63906948 18.73051 18.73051
217  0.62062448 0.78791950 15.74957 15.74957
218  0.68430386 0.82652375 14.47252 14.47252
219  0.61100591 0.78197185 15.92143 15.92143
220  0.67704731 0.82219114 14.65263 14.65263
221  0.61507530 0.78449197 16.01835 16.01835
222  0.68688341 0.82805986 14.37648 14.37648
223  0.53549934 0.73417518 16.96944 16.96944
224  0.62102974 0.78816941 15.34715 15.34715
 [1] "X"          "method"     "datalength" "featureset" "seconds"   
 [6] "ntraindata" "ntestdata"  "cor"        "corll"      "corul"     
[11] "rmse"       "dv"        
[1] 0.7665804
[1] 0.5876455

778 0.57313473 20.06316 20.06316
208  0.43502152 0.66736366 18.01713 18.01713
209  0.33588404 0.59756218 19.80865 19.80865
210  0.44294576 0.67277223 17.99453 17.99453
211  0.33115224 0.59412836 19.79347 19.79347
212  0.43212322 0.66537934 18.09373 18.09373
213  0.12042764 0.43083032 22.36953 22.36953
214  0.36513922 0.61858134 19.16352 19.16352
215  0.22075441 0.51119774 20.99746 20.99746
216  0.39414536 0.63906948 18.73051 18.73051
217  0.62062448 0.78791950 15.74957 15.74957
218  0.68430386 0.82652375 14.47252 14.47252
219  0.61100591 0.78197185 15.92143 15.92143
220  0.67704731 0.82219114 14.65263 14.65263
221  0.61507530 0.78449197 16.01835 16.01835
222  0.68688341 0.82805986 14.37648 14.37648
223  0.53549934 0.73417518 16.96944 16.96944
224  0.62102974 0.78816941 15.34715 15.34715
 [1] "X"          "method"     "datalength" "featureset" "seconds"   
 [6] "ntraindata" "ntestdata"  "cor"        "corll"      "corul"     
[11] "rmse"       "dv"        
[1] 0.7665804
[1] 0.5876455

